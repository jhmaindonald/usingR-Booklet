[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Using R for Data Analysis and Graphics",
    "section": "",
    "text": "Preface\nThis document was designed, when it first appeared in 2000, to cover elementary aspects of the R language syntax and semantics, and to demonstrate by example a limited selection of R graphics and data analysis abilities. This 2023 fourth revision should now replace, for who find it still useful, earlier versions. Attention has been primarily on corrections and clarifications, with very limited attention to new features.\nPrimarily, the focus is on using examples that readers can follow and work through. It demonstrates the use of R for a range of data manipulation, graphical presentation, and statistical analysis tasks. Examples in Chapters 3 and later come with what is often a minimum of explanatory comment.\nThose who want to explore the Hadley Wickham tidyverse R package collection, using its functions for data manipulation and graphics in place of the limited range of base R functions that get attention in this present document, can find very extensive resources online.1\n\nThe history of R\nR implements a dialect of the S language that was developed at AT&T Bell Laboratories by Rick Becker, John Chambers and Allan Wilks. The citation for John Chambers’ 1998 Association for Computing Machinery Software award stated that S has “forever altered how people analyze, visualize and manipulate data.” The R project enlarges on the ideas and insights that generated the S language.\nThe initial version of R was developed by Ross Ihaka and Robert Gentleman, at that time at the University of Auckland. Development of R is now overseen by a core team, widely drawn from different institutions worldwide.\nSource code is available for users to adapt and/or improve, or take across to other systems. Exposing code to the critical scrutiny of expert users has proved an effective way to identify bugs and other inadequacies, and to elicit ideas for enhancement. New minor releases appear four or five times a year.\n\n\nObtaining R\nVersions of R are available, at no cost, for Microsoft Windows, for Linux, for Unix, and for Macintosh OS X. It is available through the Comprehensive R Archive Network (CRAN). Standard R installations come with base and recommended packages. As the name suggests the base packages provide a base on which other R packages are built. Most users will want to supplement base and recommended packages with packages that target their own specific requirements.\n\n\nA language and an environment\nThe R language environment is designed to facilitate the development of new scientific computational tools. The packages give access to up-to-date methodology from leading statistical and other researchers.\nR is a functional language. There is a language core that uses standard forms of algebraic notation, allowing the calculations such as 2+3, or 3^11. Beyond this, most computation is handled using functions. The action of quitting from an R session uses the function call q(), either directly or invoked by a click on a menu item that is provided for this purpose.\nIt is often possible and desirable to operate on objects — vectors, arrays, lists and so on – as a whole. This largely avoids the need for explicit loops, leading to clearer code. Section 1.8 has an example.\nWith the very large address spaces now possible, and as a result of continuing improvements in the efficiency of R’s coding and memory management, R’s routines can readily process data sets that by historical standards seem large — e.g., on a Unix machine with 2GB of memory, a regression with 500,000 cases and 100 variables is feasible. With very large datasets, the main issue is often manipulation of data, and systems that are specifically designed for such manipulation may be preferable.\nData structure is, typically, an even more important issue for large data sets than for small data sets. Consider that repeated smaller analyses with subsets of the total data may give insight that is not available from a single global analysis.\n\n\nThe use of these notes\nThe notes are designed so that users can run the examples in the script files (ch1.R, ch2.R, etc.) using the notes as commentary.\nReaders of these notes may find it helpful to have available for reference the document: “An Introduction to R”, written by W N Venables, D M Smith and the R Development Core Team, and available from <https://cran.r-project.org/doc/manuals/r-release/R-intro.pdf Books that provide a more extended commentary include Maindonald and Braun (2010) .\nPoints to note are:\n\nUsers who want a point and click interface should investigate the R Commander (Rcmdr package) interface.\nWhile R is as reliable as any statistical software that is available, and exposed to higher standards of scrutiny than most other systems, there are traps that call for special care. Some of the model fitting routines are leading edge, with a limited tradition of experience of the limitations and pitfalls. Whatever the statistical system, and especially when there is some element of complication, check each step with care.\n\n\nThe R community is widely drawn, from application area specialists as well as statistical specialists. It is a community that is sensitive to the potential for misuse of statistical techniques and suspicious of what might seem mindless use. Expect scepticism of the use of models that are not susceptible to some minimal form of data-based validation.\nThe skills needed for the computing are not on their own enough. Neither R nor any other statistical system will give the statistical expertise needed to use sophisticated abilities, or to know when naïve methods are inadequate. A butcher’s meat-cleaving skills are unlikely to be adequate for effective animal surgery. Experimentation with the use of R is however, more than with most systems, likely to be an educational experience.\n\nHurrah for the R development team!\n\n\nWeb Pages and Email Lists\nFor a variety of official and contributed documentation, for copies of various versions of R, and for other information, go to http://cran.r-project.org and find the nearest CRAN (Comprehensive R Archive Network) mirror site.\nThere is no official support for R. The r-help email list gives access to an informal support network that can be highly effective. Details of the R-help list, and of other lists that serve the R community, are available from the web site for the R project at https://www.R-project.org/. A search on Stack Overflow can often yield information that is very helpful.\nBinary installations of R are available from CRAN sites, for Windows, for MacOS, and for four different flavors of Linux. These come with all the base and recommended packages. Other packages must be installed.\nInstallation instructions appropriate to the operating system can be found on CRAN sites. Copy down the relevant setup file, click on its icon to start installation, and follow instructions.\nFor running the examples in the first chapter, ensure that the DAAG package is installed. The following command line instruction can be used:\n\ninstall.packages(\"DAAG\")\n\n\n\n\n\nMaindonald, John, and John Braun. 2010. Data Analysis and Graphics Using r: An Example-Based Approach. Cambridge University Press.\n\n\n\n\n\n\n\nNotably, check https://www.tidyverse.org/↩︎"
  },
  {
    "objectID": "basics.html#sec-typing",
    "href": "basics.html#sec-typing",
    "title": "1  An overview of the R system",
    "section": "1.1 Learn by typing at the command line",
    "text": "1.1 Learn by typing at the command line\nThis document mostly assumes that users will type commands into the command window, at the command line prompt. The command line prompt, i.e. the >, is an invitation to start typing in commands. For example, type 2+2 and press the Enter key. Here is what appears on the screen:\n\n2+2\n[1] 4\n\nHere the result is 4. The [1] says, a little strangely, “first requested element will follow”. Here, there is just one element. The > indicates that R is ready for another command.\nFor later reference, note that the exit or quit command is\n\nq()\n\nAn alternative to the use of q() is to click on the File menu and then on Exit. There will be a message asking whether to save the workspace image. Clicking Yes (the safe option) will save all the objects that remain in the workspace — any that remain from the start of the session and any that have been added since. RStudio users can click on one of the options that are available under the Session menu header.\n\nSome notational details\nAs noted earlier, the command line prompt is\n>\nR commands (expressions) are typed following this prompt.\nThere is also a continuation prompt, used when, following a carriage return, the command is still not complete. By default, the continuation prompt is\n+\nIn these notes, we often continue commands over more than one line, but omit the + that will appear on the commands window if the command is typed in as we show it.\nFor the names of R objects or commands, case is significant. Thus Austpop is different from austpop. For file names when using Windows, however, the Microsoft Windows conventions apply, and case does not distinguish file names. On Unix and Mac systems, letters that have a different case are treated as different.\nAnything that follows a # on the command line is taken as comment.\nNote: Recall that, in order to quit from the R session we could type q(). This is because q is a function. Typing q on its own, without the parentheses, displays the text of the function on the screen. Try it!\n\n\nBase, recommended, and other packages\n\n## Base packages\nnames(which(installed.packages()[ ,\"Priority\"] == \"base\", ))\n [1] \"base\"      \"compiler\"  \"datasets\"  \"graphics\"  \"grDevices\" \"grid\"     \n [7] \"methods\"   \"parallel\"  \"splines\"   \"stats\"     \"stats4\"    \"tcltk\"    \n[13] \"tools\"     \"utils\"    \n## Recommended packages\nnames(which(available.packages(repos = \n  c(CRAN = \"https://cran.r-project.org\"))[ ,\"Priority\"] == \"recommended\", ))\n [1] \"boot\"       \"class\"      \"cluster\"    \"codetools\"  \"foreign\"   \n [6] \"KernSmooth\" \"lattice\"    \"MASS\"       \"Matrix\"     \"mgcv\"      \n[11] \"nlme\"       \"nnet\"       \"rpart\"      \"spatial\"    \"survival\"  \n\nPackages that do not come with the initial distribution must be downloaded and installed separately. A number of packages are by default attached at startup. Names of packages (additional to base) that are by default attached at startup can be checked thus:\n\ngetOption('defaultPackages')\n[1] \"datasets\"  \"utils\"     \"grDevices\" \"graphics\"  \"stats\"     \"methods\"  \n\nTo see which packages have been attached at any point in a session, type:\n\nsearch()\n\nThese may, if earlier session was saved upon quitting and restored on startup for the current session, include packages that were attached in the earlier session.\nIt pays to have a separate working directory for each major project. RStudio makes it straightforward, both to set up a new project in a new directory, and to move between projects.\n\n\nSome ways to use R\n\nR may be used as a calculator.\nR evaluates and prints out the result of any expression that one types in at the command line in the console window. Expressions are typed following the prompt (>) on the screen. The result, if any, appears on subsequent lines.\n\n2+2\n[1] 4\nsqrt(10)                 # Type ?sqrt to see help for `sqrt`\n[1] 3.162278\n2*3*4*5\n[1] 120\n1000*(1+0.045)^5 - 1000  # Interest on $1000, compounded annually \n[1] 246.1819\n                         # at 4.5% p.a. for five years\npi  # R knows about pi\n[1] 3.141593\n2*pi*6378 #Circumference of Earth at Equator, in km; radius is 6378 km\n[1] 40074.16\ndeg <- c(30,60,90)  \n  # Save the numeric vector `c(30,60,90)` with the name `deg`\nsin(deg*pi/180)          # Convert angles to radians, then take sin()\n[1] 0.5000000 0.8660254 1.0000000\nsin(c(30,60,90)*pi/180)  # The result is the same\n[1] 0.5000000 0.8660254 1.0000000\n\nObserve that c() is a function that joins vectors together\n\n\n\n1.1.1 R makes it easy to create plots and other forms of data summary\nAs a relatively simple example, where all columns are numeric, consider the data frame austpop that holds population figures (in thousands) for Australian states and territories, and total population, at various times since 1917. Assuming that the DAAG package has been installed, this can be accessed as DAAG::austpop. The data are:\n\naustpop <- DAAG::austpop\nhead(austpop)\n  year  NSW  Vic  Qld   SA  WA Tas NT ACT  Aust\n1 1917 1904 1409  683  440 306 193  5   3  4941\n2 1927 2402 1727  873  565 392 211  4   8  6182\n3 1937 2693 1853  993  589 457 233  6  11  6836\n4 1947 2985 2055 1106  646 502 257 11  17  7579\n5 1957 3625 2656 1413  873 688 326 21  38  9640\n6 1967 4295 3274 1700 1110 879 375 62 103 11799\n\nFigure 1.1 uses the function plot() to show a plot of the Australian Capital Territory (ACT) population between 1917 and 1997.\nFigure 1.1 uses the function plot() to show a plot of the Australian Capital Territory (ACT) population between 1917 and 1997.\n\n\n\n\n\n\n\n\nFigure 1.1: Australian Capital Territory (ACT) population between 1917 and 1997\n\n\n\n\nCode is:\n\nplot(ACT ~ year, data=austpop, pch=16) \n\nThe option pch=16 sets the plotting character to a solid black dot. This plot can be improved greatly. We can specify more informative axis labels, change size of the text and of the plotting symbol, and so on.\n\nR provides a huge range of abilities for working with data\nBetween the base, recommended, and huge range of contributed packages, R offers wide-ranging abilities for data manipulation, for data summary, for graphical display, for fitting models, for simulation, and for a variety of other computations.\nData frames are a standard way to store data. A dataframe is a list of columns, all of the same length. Columns can be numeric, or character, or logical (values are TRUE or FALSE), or factor, or dates. As a first example, consider the data frame hills. This has three columns (variables), with the names distance, climb, and time. Typing summary(hills) gives summary information on these variables. There is one column for each variable, thus:\n\nhills <- DAAG::hills  # Copy the dataframe `hills`, from the \n                      # DAAG package, into the  workspace.\nsummary(hills)\n      dist            climb           time       \n Min.   : 2.000   Min.   : 300   Min.   :0.2658  \n 1st Qu.: 4.500   1st Qu.: 725   1st Qu.:0.4667  \n Median : 6.000   Median :1000   Median :0.6625  \n Mean   : 7.526   Mean   :1815   Mean   :0.9646  \n 3rd Qu.: 8.000   3rd Qu.:2200   3rd Qu.:1.1438  \n Max.   :28.000   Max.   :7500   Max.   :3.4103  \n\nWe may for example require information on ranges of variables. Thus the range of distances (first column) is from 2 miles to 28 miles, while the range of times (third column) is from 15.95 (minutes) to 204.6 minutes.\nA helpful graphical summary for the hills data frame is the scatterplot matrix, shown in Figure 1.2.\n\n\n\n\npairs(DAAG::hills)\n\n\n\n\nFigure 1.2: Scatterplot matrix for the Scottish hill race data\n\n\n\n\nTo reproduce the plot, type\n\npairs(DAAG::hills)\n\nCorrelation calculations are a form of data summary. The correlation matrix for the hills data is:\n\noptions(digits=3)\ncor(DAAG::hills)\n       dist climb  time\ndist  1.000 0.652 0.920\nclimb 0.652 1.000 0.805\ntime  0.920 0.805 1.000\n\nThere is a case for taking logarithms of data values, and then calculating correlations. This can all be done in one step, thus:\n\ncor(log(DAAG::hills))\n      dist climb  time\ndist  1.00 0.700 0.890\nclimb 0.70 1.000 0.724\ntime  0.89 0.724 1.000\n\nR was not clever enough to relabel distance as log(distance), climb as log(climb), and time as log(time). Notice that the correlations between time and distance, and between time and climb, have reduced. Why has this happened?\nIn the straight line regression calculations now demonstrated, the variable names are the names of columns in the data frame DAAG::elasticband. The formula that is supplied to the lm() (linear model) command asks for the regression of distance traveled by the elastic band (distance) on the amount by which it is stretched (stretch).\n\nelasticband <- DAAG::elasticband\nelastic.lm <- lm(distance~stretch,data=elasticband)\nlm(distance ~stretch, data=elasticband)\n\nCall:\nlm(formula = distance ~ stretch, data = elasticband)\n\nCoefficients:\n(Intercept)      stretch  \n     -63.57         4.55  \n\nMore complete information is available by typing\n\nsummary(elastic.lm)\n\n\n\n\nFigure 1.3 plots the data and adds the regression line:\n\n\n\n\n\nFigure 1.3: Elastic band distance versus stretch, with regression line added\n\n\n\n\n\n## Code\nplot(distance ~ stretch,data=elasticband, pch=16)\nabline(elastic.lm)  # Add regression line to graph\n\n\n\nR is an interactive programming language\nWe calculate the Fahrenheit temperatures that correspond to Celsius temperatures 25, 26, …, 30:\n\ncelsius <- 25:30\nfahrenheit <- 9/5*celsius+32\nconversion <- data.frame(Celsius=celsius, Fahrenheit=fahrenheit)\nprint(conversion)\n  Celsius Fahrenheit\n1      25       77.0\n2      26       78.8\n3      27       80.6\n4      28       82.4\n5      29       84.2\n6      30       86.0\n\n\n\nExtensive help is available from the command line\nTo get a help on an R object, use help() or ?, thus\nThus, to get help on the R function plot(), type:\n\n?plot\n\nThe two search functions help.search() and apropos() can help in finding what one wants. Examples of their use are:\n\nhelp.search(\"matrix\")\n  ## This lists all functions whose help pages have a title or alias in\n  ## which the text string “matrix” appears.\napropos(\"matrix\")\n  ## This lists all function names that include the text “matrix”\n\nThe function help.start() opens a browser window that gives access to the full range of documentation for syntax, packages and functions.\nExperimentation often helps clarify the precise action of an R function."
  },
  {
    "objectID": "basics.html#vectors",
    "href": "basics.html#vectors",
    "title": "1  An overview of the R system",
    "section": "1.2 Vectors",
    "text": "1.2 Vectors\nExamples of vectors are\n\nc(2,3,5,2,7,1)\n3:10   # The numbers 3, 4, .., 10\nc(TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE)\nc(”Canberra”,”Sydney”,”Newcastle”,”Darwin”)\n\nVectors may have mode logical, numeric or character . The first two vectors above are numeric, the third is logical (i.e. a vector with elements of mode logical), and the fourth is a string vector (i.e. a vector with elements of mode character). The missing value symbol, which is NA, can be included as an element of a vector.\n\nJoining (concatenating) vectors\nThe c in c(2, 3, 5, 7, 1) above is an acronym for “concatenate”, i.e. the meaning is: “Join these numbers together in to a vector. Existing vectors may be included among the elements that are to be concatenated. In the following we form vectors x and y, which we then concatenate to form a vector z:\n\nx <- c(2,3,5,2,7,1)\nx\n[1] 2 3 5 2 7 1\ny <- c(10,15,12)\ny\n[1] 10 15 12\nz <- c(x, y)\nz\n[1]  2  3  5  2  7  1 10 15 12\n\nThe concatenate function c() may also be used to join lists.\n\n\nSubsets of Vectors\nThere are two common ways to extract subsets of vectors.\n1. Specify the numbers of the elements that are to be extracted, e.g.\n\nx <- c(3,11,8,15,12)  # Assign to x the values 3, 11, 8, 15, 12\nx[c(2,4)]   # Extract elements (rows) 2 and 4\n[1] 11 15\n\nOne can use negative numbers to omit elements:\n\nx <- c(3,11,8,15,12)\nx[-c(2,3)]\n[1]  3 15 12\n\n\nSpecify a vector of logical values. The elements that are extracted are those for which the logical value is T. Thus suppose we want to extract values of x that are greater than 10.\n\n\nx>10       # This generates a vector of logical (T or F)\n[1] FALSE  TRUE FALSE  TRUE  TRUE\nx[x>10]\n[1] 11 15 12\n\nArithmetic relations that may be used in the extraction of subsets of vectors are <, <=, >, >=, ==, and !=. The first four compare magnitudes, == tests for equality, and != tests for inequality.\nVectors can have named elements, in which case elements can be extracted by name. For example:\n\nheight <- c(Andreas=178, John=185, Jeff=183)\nheight[c(\"John\",\"Jeff\")]\nJohn Jeff \n 185  183 \n\n\n\nPatterned Data\nUse 5:15 to generate the numbers 5, 6, . . ., 15. Entering 15:5 will generate the sequence in the reverse order. To repeat the sequence (2, 3, 5) four times over, enter rep(c(2,3,5), 4) thus:\n\nrep(c(2,3,5),4)\n [1] 2 3 5 2 3 5 2 3 5 2 3 5\n\nIf instead one wants four 2s, then four 3s, then four 5s, enter\n\nrep(c(2,3,5),c(4,4,4))    # An alternative is rep(c(2,3,5), each=4)\n [1] 2 2 2 2 3 3 3 3 5 5 5 5\n\nNote further that, in place of c(4,4,4) we could write rep(4,3).\nIn addition to the above, note that the function rep() has an argument length.out, meaning “keep on repeating the sequence until the length is length.out.”"
  },
  {
    "objectID": "basics.html#lists-with-dataframes-as-an-important-special-case",
    "href": "basics.html#lists-with-dataframes-as-an-important-special-case",
    "title": "1  An overview of the R system",
    "section": "1.3 Lists, with dataframes as an important special case",
    "text": "1.3 Lists, with dataframes as an important special case\nLists collect together, under a single name, what can be an arbitrary set of R objects. These might be vectors of several different modes and lengths, scalars, dates, matrices or more general arrays, or functions, etc.\n\nDataframes are lists!\nA data frame is a list of variables, all of equal length. Variables can be vectors (integer, or numeric, or character, or logical, or complex). Among other possibilities, they can also be factors, or date objects. For the moment, attention will be limited to dataframes where the columns are integer, or numeric, or character, or logical.\nJust as with any other list, subscripting extracts a list. Thus Cars93.summary[4] is a data frame with a single column, which is the fourth column vector of Cars93.summary. Use Cars93.summary[[4]] or Cars93.summary[,4] to extract the column vector.\nR packages include a wide variety of datasets, mostly in the form of dataframes. Data frames have a central role in the way that R is set up to process data, fit models, and display graphs.\n\nOperations with dataframes\nAmong the datasets in the DAAG package is Cars93.summary, created from information in the Cars93 data set in the Venables and Ripley MASS package. Here it is:\n\nCars93.summary <- DAAG::Cars93.summary \nCars93.summary\n        Min.passengers Max.passengers No.of.cars abbrev\nCompact              4              6         16      C\nLarge                6              6         11      L\nMidsize              4              6         22      M\nSmall                4              5         21     Sm\nSporty               2              4         14     Sp\nVan                  7              8          9      V\n\nNotice that the final column has the mode character. Different columns can have different modes – including numeric, character, and logical (values TRUE and FALSE).\nThe data frame has row labels (access with row.names(Cars93.summary)) Compact, Large, . . . The column names (access with names(Cars93.summary)) are Min.passengers (i.e. the minimum number of passengers for cars in this category), Max.passengers, No.of.cars., and abbrev. The first three columns have mode numeric, and the fourth has mode character. Columns can be vectors of any mode. The column abbrev could equally well be stored as a factor – more on that in due course.\n\n\nAccessing parts of data frames\nA data frame that has only the first four rows and omits the third column can be extracted thus:\n\ncars <- Cars93.summary[1:4, c(1,2,4)]\ncars <- Cars93.summary[1:4, -3]   # Alternative --- specify what to omit\n\nAny of the following will pick out the fourth column of the data frame Cars93.summary, then storing it in the vector type.\n\ntype <- Cars93.summary$abbrev\ntype <- Cars93.summary[,4]\ntype <- Cars93.summary[,\"abbrev\"]\ntype <- Cars93.summary[[4]]    # Take the object that is stored\n                               # in the fourth list element.\n\n\n\n1.3.0.1 *Merging Data Frames – a simple example {-}\nThe data frame MASS::Cars93 holds extensive information on data from 93 cars on sale in the USA in 1993. The data frame DAAG::Cars93.summary has as row names the distinct values of the factor Type. The final column, with the name abbrev, holds two character abbreviations of each of the car type names, suitable for use in plotting.\n\nCars93.summary <- DAAG::Cars93.summary\nCars93.summary\n        Min.passengers Max.passengers No.of.cars abbrev\nCompact              4              6         16      C\nLarge                6              6         11      L\nMidsize              4              6         22      M\nSmall                4              5         21     Sm\nSporty               2              4         14     Sp\nVan                  7              8          9      V\n\nWe proceed thus to add a column that has the abbreviations to the data frame.\n\nCars93 <- MASS::Cars93\nnew.Cars93 <- merge(x=Cars93,y=Cars93.summary[,4,drop=F],\n            by.x=\"Type\", by.y=\"row.names\") \n\nNotice that the row names of Cars93.summary are treated as a column of the data frame, with name row.names. The effect is to create a data frame that has the abbreviations in the additional column with name abbrev. If there had been rows with missing values of Type, these would have been omitted from the new data frame. This can be avoided by ensuring that Type has NA as one of its levels, in both data frames.\n\n\n\nLists more generally\n\nOutput from fitting a model is a list\nAs an illustration consider the list object that R creates as output from an lm() linear model fit.\n\nelastic.lm <- lm(distance~stretch, data=DAAG::elasticband)\n\nThe object elastic.lm is a list that brings together several different kinds of objects. The names are:\n\nnames(elastic.lm)\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\nThe first list element is:\n\nelastic.lm$coefficients\n(Intercept)     stretch \n     -63.57        4.55 \n\nAlternative ways to extract this first list element are:\n\nelastic.lm[[\"coefficients\"]]\nelastic.lm[[1]]\n\nWe can alternatively ask for the sublist whose only element is the vector elastic.lm$coefficients. For this, specify\n\nelastic.lm[\"coefficients\"]\n$coefficients\n(Intercept)     stretch \n     -63.57        4.55 \n# elastic.lm[1] gives the same result\n\nNotice that the information is in this case preceded by $coefficients, meaning “list element with name coefficients”. The result is a list, now with just the first element of elastic.lm.\nThe second list element is a vector of length 7:\n\nprint(elastic.lm$residuals, digits=3)\n      1       2       3       4       5       6       7 \n  2.107  -0.321  18.000   1.893 -27.786  13.321  -7.214 \n\nThe tenth list element documents the function call:\n\nelastic.lm$call\nlm(formula = distance ~ stretch, data = DAAG::elasticband)\nmode(elastic.lm$call)\n[1] \"call\""
  },
  {
    "objectID": "basics.html#factors-dates-nas-and-more",
    "href": "basics.html#factors-dates-nas-and-more",
    "title": "1  An overview of the R system",
    "section": "1.4 Factors, dates, NAs, and more",
    "text": "1.4 Factors, dates, NAs, and more\n\nFactors and ordered factors\nA factor is stored internally as a numeric vector with values 1, 2, 3, k, where k is the number of levels. An attributes table gives the ‘level’ for each integer value . Factors provide a compact way to store character strings. They are crucial in the representation of categorical effects in model and graphics formulae. The class attribute of a factor has, not surprisingly, the value \"factor\".\nConsider a survey that has data on 691 females and 692 males. If the first 691 are females and the next 692 males, we can create a vector of strings that that holds the values thus:\n\ngender <- c(rep(\"female\",691), rep(\"male\",692))\n\n(The usage is that rep(“female”, 691) creates 691 copies of the character string “female”, and similarly for the creation of 692 copies of “male”.)\nWe can change the vector to a factor, by entering:\n\ngender <- factor(gender)\n\nInternally the factor gender is stored as 691 1’s, followed by 692 2’s. It has stored with it the vector:\n\nlevels(gender)\n[1] \"female\" \"male\"  \n\nIn most cases where the context seems to demand a character string, the 1 is translated into \"female\" and the 2 into \"male\". The values \"female\" and \"male\" are the levels of the factor. By default, the levels are in alphanumeric order, so that \"female\" precedes \"male\". Hence:\n\nlevels(gender)  # Assumes gender is a factor, created as above\n[1] \"female\" \"male\"  \n\nThe order of the levels in a factor determines the order in which the levels appear in graphs that use this information, and in tables. To cause \"male\" to come before \"female\", use\n\ngender <- relevel(gender, ref=\"male\")\n\nAn alternative is\n\ngender <- factor(gender, levels=c(\"male\", \"female\"))\n\nThis last syntax is available both when the factor is first created, or later when one wishes to change the order of levels in an existing factor. Incorrect spelling of the level names will generate an error message. Try\n\ngender <- factor(c(rep(“female”,691), rep(“male”,692)))\ntable(gender)\ngender <- factor(gender, levels=c(“male”, “female”))\ntable(gender)\ngender <- factor(gender, levels=c(“Male”, “female”))  \n                # Erroneous - \"male\" rows now hold missing values\ntable(gender)\nrm(gender)      # Remove gender  \n\nThe following adds site names to the possum dataframe:\n\npossum <- DAAG::possum\npossumsites <- DAAG::possumsites\npossum$sitenam <- rownames(possumsites)[DAAG::possum$site]\nwith(possum, table(sitenam))\nsitenam\nAllyn River    Bellbird    Bulburin   Byrangery Cambarville   Conondale \n          7          13          18          13          33          13 \nWhian Whian \n          7 \nsitefac <- factor(possum$sitenam)\ntable(sitefac)\nsitefac\nAllyn River    Bellbird    Bulburin   Byrangery Cambarville   Conondale \n          7          13          18          13          33          13 \nWhian Whian \n          7 \n\nIt is the integer values that are stored. Along with the vector of integer values are stored a list of attributes, which holds the level names and the class vector:\n\nmode(sitefac)        \n[1] \"numeric\"\n  # The attributes list is for an object of this class\nattributes(sitefac)  \n$levels\n[1] \"Allyn River\" \"Bellbird\"    \"Bulburin\"    \"Byrangery\"   \"Cambarville\"\n[6] \"Conondale\"   \"Whian Whian\"\n\n$class\n[1] \"factor\"\n  # This controls the interpretation of `sitefac` a factor\n\nPrinting the contents of the column with the name sitefac gives the names, not the integer values. As in most operations with factors, R does the translation invisibly. There are though annoying exceptions that can make the use of factors tricky. To get back the site names as a character vector and tabulate the result, specify\n\ntable(as.character(possum$sitefac))\n< table of extent 0 >\n\nTo get the integer values and tabulate the result, specify\n\ntable(unclass(possum$sitefac))\n< table of extent 0 >\n\nWe might prefer the names to appear in order of latitude, from North to South. We can change the order of the level names to reflect this desired order:\n\nordnam <- rownames(possumsites)[order(possumsites$Latitude)]\npossum$sitefac <- factor(possum$sitenam, levels=ordnam)\nwith(possum, table(sitefac))\nsitefac\n   Bellbird Cambarville Allyn River Whian Whian   Byrangery   Conondale \n         13          33           7           7          13          13 \n   Bulburin \n         18 \n\nFactors have the potential to cause a few surprises. Points to note are:\n* When a vector of character strings becomes a column of a data frame, R by default turns it into a factor. Enclose the vector of character strings in the wrapper function I() if it is to remain character. With tibbles, this is not an issue. * There are some contexts in which factors become numeric vectors. To be sure of getting the vector of text strings, specify e.g. as.character(possum$sitefac).\n\nOrdered Factors\nActually, it is their levels that are ordered. To create an ordered factor, or to turn a factor into an ordered factor, use the function ordered(). The levels of an ordered factor are assumed to specify positions on an ordinal scale. Try\n\nstress.level <- rep(c(\"low\",\"medium\",\"high\"),2)\nordf.stress <- ordered(stress.level, \n                       levels=c(\"low\",\"medium\",\"high\"))\nordf.stress\n[1] low    medium high   low    medium high  \nLevels: low < medium < high\nordf.stress<\"medium\"\n[1]  TRUE FALSE FALSE  TRUE FALSE FALSE\nordf.stress>=\"medium\"\n[1] FALSE  TRUE  TRUE FALSE  TRUE  TRUE\n\nOrdered factors inherit the attributes of factors, and have a further ordering attribute. Asking for the class of an object returns details both of the class of the object, and of any classes from which it inherits.\nThus:\n\nclass(ordf.stress)\n[1] \"ordered\" \"factor\" \nattributes(ordf.stress)\n$levels\n[1] \"low\"    \"medium\" \"high\"  \n\n$class\n[1] \"ordered\" \"factor\" \n\n\n\nInclusion of character string vectors in data frames\nWhen data are input using read.table(), or when the data.frame() function is used to create data frames, vectors of character strings are by default turned into factors. The parameter setting stringsAsFactors=TRUE, available both with read.table() and with data.frame(), will if needed ensure that character strings are input without such conversion. For read.table(), an alternative is as.is=TRUE. When input uses functions in the readr package, character strings are left unchanged.\n\n\n\nDates\nSee ?Dates, ?as.Date and ?format.Date for information on functions in base R for working with dates. Use as.Date() to convert text strings into dates. The default is that the year comes first, then the month, and then the day of the month, thus:\n\n# Electricity Billing Dates\ndd <- as.Date(c(\"2003/08/24\",\"2003/11/23\",\"2004/02/22\",\"2004/05/23\"))\ndiff(dd)\nTime differences in days\n[1] 91 91 91\n\nUse format() to set or change the way that a date is formatted. The following are a selection of the symbols used:\n%d: day, as number\n%a: abbreviated weekday name (%A: unabbreviated)\n%m: month (00-12)\n%b: month abbreviated name (%B: unabbreviated)\n%y: final two digits of year (%Y: all four digits)\nThe default format is %Y-%m-%d. The function as.Date() takea vector of character strings that has  an appropriate format, and converts it into a dates object. By default,  dates are stored using January 1 1970 as origin. This becomes apparent  whenas.integer()` is used to convert a date into an integer value. Examples are:\n\nas.Date(\"1/1/1960\", format=\"%d/%m/%Y\")\n[1] \"1960-01-01\"\nas.Date(\"1:12:1960\",format=\"%d:%m:%Y\")\n[1] \"1960-12-01\"\nas.Date(\"1960-12-1\")-as.Date(\"1960-1-1\")\nTime difference of 335 days\nas.Date(\"31/12/1960\",\"%d/%m/%Y\")\n[1] \"1960-12-31\"\nas.integer(as.Date(\"1/1/1970\",\"%d/%m/%Y\"))\n[1] 0\nas.integer(as.Date(\"1/1/2000\",\"%d/%m/%Y\"))\n[1] 10957\n\nThe function format() allows control of the formatting of dates when they are printed. See ?format.Date.\n\ndec1 <- as.Date(\"2004-12-1\")\nformat(dec1, format=\"%b %d %Y\")\n[1] \"Dec 01 2004\"\nformat(dec1, format=\"%a %b %d %Y\")\n[1] \"Wed Dec 01 2004\"\n\nAs with factors, the underlying storage mode is a numeric vector.\n\nmode(dd)\n[1] \"numeric\"\njulian(dd)          \n[1] 12288 12379 12470 12561\nattr(,\"origin\")\n[1] \"1970-01-01\"\n  # Makes it clear that date is days since '1970-01-01'\njulian(as.Date('1970-01-01'))                    \n[1] 0\nattr(,\"origin\")\n[1] \"1970-01-01\"\n\n\n\nNAs (missing Values), NaN (not a number) and Inf\nIn R, the missing value symbol is NA. Any arithmetic operation or relation that involves NA generates an NA. This applies also to the relations <, <=, >, >=, ==, !=. The first four compare magnitudes, == tests for equality, and != tests for inequality. Users who do not carefully consider implications for expressions that include NAs may be puzzled by the results. Specifically, note that x==NA generates NA.\nBe sure to use is.na(x) to test which values of x are NA. As x==NA gives a vector of NAs, this gives no information about x.\nFor example\n\nx <- c(1,6,2,NA)\nis.na(x)            # TRUE for when NA appears, and otherwise FALSE\n[1] FALSE FALSE FALSE  TRUE\nx==NA     # All elements are set to NA\n[1] NA NA NA NA\nNA==NA\n[1] NA\n\n\nThe Use of NA in variable subscripts\nAny arithmetic operation or relation that involves NA generates an NA. Set\n\ny <- c(1, NA, 3, 0, NA)\n\nBe warned that y[y==NA] <- 0 leaves y unchanged. The reason is that all elements of y==NA evaluate to NA. This does not select an element of y, and there is no assignment. To replace all NAs by 0, use\n\ny[is.na(y)] <- 0\n\nThe following, where the subscript vector on both sides has one or more missing values, generates an error message:\n\nx <- c(1,6,2,NA)\ny <- 11:15\ny[x>2] <- x[x>2]\nError in y[x > 2] <- x[x > 2]: NAs are not allowed in subscripted assignments\n\nUse !is.na(x) to limit the selection, on both sides, to those elements of x that are not NAs.\n\n\nInf and NaN\nThe following are allowed:\n\nc(-1/0, 1/0, 0/0, 1/Inf)\n[1] -Inf  Inf  NaN    0\n\nIt is up to the user to ensure that allowing such calculations to proceed leads to results that make sense."
  },
  {
    "objectID": "basics.html#matrices-and-arrays",
    "href": "basics.html#matrices-and-arrays",
    "title": "1  An overview of the R system",
    "section": "1.5 Matrices and arrays",
    "text": "1.5 Matrices and arrays\nAll elements of a matrix have the same mode, i.e. all numeric, or all character. Thus a matrix is a more restricted structure than a data frame. One reason for numeric matrices is that they allow a variety of mathematical operations that are not available for data frames. Matrices are likely to be important for those users who wish to implement new regression and multivariate methods. The matrix construct generalizes to array, which may have more than two dimensions. Matrices are stored columnwise, in a single vector. Thus consider:\n\nxx <- matrix(1:6,ncol=3)  # Equivalently, enter matrix(1:6,nrow=2)\nxx\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\nIf xx is any matrix, the assignment\n\nx <- as.vector(xx)\n\nplaces columns of xx, in order, into one long vector x. In the example just given, we get back the elements 1, 2, . . . , 6. Matrices have the attribute “dimension”. Thus\n\ndim(xx)\n[1] 2 3\n\nThus, a matrix is a vector (numeric or character or logical) whose dimension attribute has length 2.\nNow set\n\nx34 <- matrix(1:12,ncol=6)\nx34\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]    1    3    5    7    9   11\n[2,]    2    4    6    8   10   12\n\nExamples of the extraction of columns or rows or submatrices are:\n\nx34 <- matrix(1:12, nrow=3)\nx34[2:3,c(1,4)]  # Extract rows 2 & 3 & columns 1 & 4\nx34[2,]          # Extract the second row\nx34[-2,]         # Extract all rows except the second\nx34[-2,-3]       # Omit row 2 & column 3\n\nThe dimnames() function assigns and/or extracts matrix row and column names. The is a list, in which the first list element is the vector of row names, and the second list element is the vector of column names. This generalizes in the obvious way for use with arrays, which we now discuss.\n\nArrays\nThe generalization from a matrix (2 dimensions) to allow more than 2 dimensions gives an array. A matrix is a 2-dimensional array. Consider a numeric vector of length 24. So that we can easily keep track of the elements, we will make them 1, 2, .., 24. Thus\n\nx <- 1:24\ndim(x) <- c(2,12)\n# `x` is then a 2 x 12 matrix.  \nx\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\n[1,]    1    3    5    7    9   11   13   15   17    19    21    23\n[2,]    2    4    6    8   10   12   14   16   18    20    22    24\n\nNow try\n\ndim(x) <-c(3,4,2)\nx\n, , 1\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\n, , 2\n\n     [,1] [,2] [,3] [,4]\n[1,]   13   16   19   22\n[2,]   14   17   20   23\n[3,]   15   18   21   24\n\n\nConversion of numeric dataframes into matrices\nThere are various manipulations that are available for matrices, but not for data frames. Use as.matrix() to handle any conversion that may be necessary. Or, as with apply(), the conversion may happen automatically."
  },
  {
    "objectID": "basics.html#sec-entry",
    "href": "basics.html#sec-entry",
    "title": "1  An overview of the R system",
    "section": "1.6 Data entry and editing",
    "text": "1.6 Data entry and editing\n\nEntry of Data at the Command Line\nThe following data gives, for each amount by which an elastic band is stretched over the end of a ruler, the distance that the band moved when released:\n  stretch   46  54  48   50  44  42  52  \n  distance 148 182 173  166 109 141 166  \nThe function data.frame() can be used to input these (or other) data directly at the command line. We will give the data frame the name elasticband:\n\nelasticband <- data.frame(stretch=c(46,54,48,50,44,42,52),\n                          distance=c(148,182,173,166,109,141,166))\n\n\n\nEntry and/or editing of data in an editor window\nTo edit the data frame elasticband in a spreadsheet-like format, type\n\nelasticband <- edit(elasticband)\n\n\n\nOptions for read.table()\nUnder RStudio, input of data from files is very conveniently handled by selecting \\(\\underline{File}\\) | \\(\\underline{Import \\space Dataset}\\) from the menu. Functions in the tidyverse collection greatly extend the range of data structure types that can be readily input into R. For this purpose, it is convenient to work from the RStudio menu, where the default is to read data into the data frame variant that has the name tibble.\nThe base R function read.table() takes, optionally various parameters additional to the file name that holds the data. Specify header=TRUE if there is an initial row of header names. The default is header=FALSE. In addition users can specify the separator character or characters. Command alternatives to the default use of a space are sep=\",\" and sep=\"\\\\t\". This last choice makes tabs separators. Similarly, users can control over the choice of missing value character or characters, which by default is NA. If the missing value character is a period (“.”), specify na.strings=\".\".\nThere are several variants of read.table() that differ only in having different default parameter settings. Note in particular read.csv(), which has settings that are suitable for comma delimited (csv) files that have been generated from Excel spreadsheets.\nIf read.table() detects that lines in the input file have different numbers of fields, data input will fail, with an error message that draws attention to the discrepancy. It is then often useful to use the function count.fields() to report the number of fields that were identified on each separate line of the file."
  },
  {
    "objectID": "basics.html#r-objects-the-workspace-and-attached-packages",
    "href": "basics.html#r-objects-the-workspace-and-attached-packages",
    "title": "1  An overview of the R system",
    "section": "1.7 R objects, the workspace, and attached packages",
    "text": "1.7 R objects, the workspace, and attached packages\nTo attach the MASS package, type:\n\nlibrary(\"MASS\")\n\nAttaching package: 'MASS'\nThe following object is masked _by_ '.GlobalEnv':\n\n    hills\n\nIt will then be attached at position 2 on the ‘search list’, which is the list of databases (as they are termed) that R searches for datasets or functions whose source has not been specifically identified. Position 1 is reserved for workspace objects that have been created or copied in by the user.\nFunctions and datasets are specific types of R objects. Notice that the first name on the search list is .GlobalEnv, which refers to the workspace. The following code ‘loads’ the dataframe cabbages, from the MASS package, into the workspace:\n\ncabbages <- MASS::cabbages\n\nIf the MASS package has earlier been attached, there will then be two places on the search list where it can be found. The name cabbages, appearing on its own, would be taken to refer to the version in the workspace, not to that in the attached package MASS.\n\nThe function with()\nThe function with() attaches the data frame (or, it can be a list) that is given as its first argument, within a specially created environment for the duration of the calculation(s) that are specified by its second argument. The environment here is neither the workspace nor an attached database. See ?environment for details of what, in general, constitutes an environment.\nFor example:\n\nav <- with(trees, mean(Height))\n  # The assignment places the result in the worksapce.\n\nThe environment is then the first place searched, looking for a column with the name Height.\n\nwith(list(x=1:3, y=5:8), mean(y))\n[1] 6.5\n\n\n\nSaving the workspace\nAll R entities, including functions and data structures, exist as objects. They can all be operated on as data. Type in ls() to see the names of all objects in your workspace. An alternative to ls() is objects(). In both cases there is provision to specify a particular pattern, e.g. starting with the letter p.\nTyping the name of an object causes the printing of its contents. Try typing q, mean, etc. In a long session, it makes sense to save the contents of the working directory from time to time. It is also possible to save individual objects, or collections of objects into a named image file. Some possibilities are:\n\nsave.image()           # Save contents of workspace, into the file .RData\nsave.image(file=\"archive.RData\")      # Save into the file archive.RData\nsave(celsius, fahrenheit, file=\"tempscales.RData\")\n\nImportant: On quitting, R offers the option of saving the workspace image, by default in the file .RData in the working directory. This allows the retention, for use in the next session in the same workspace, any objects that were created in the current session. Careful housekeeping may be needed to distinguish between objects that are to be kept and objects that will not be used again. Before typing q() to quit, use rm() to remove objects that are no longer required. Saving the workspace image will then save everything remains. The workspace image will be automatically loaded upon starting another session in that directory.\nThe function save() can be used to save a specific set of R objects into a named image file.\n\n\nFunctions and datasets – one at a time, or per database?\nThe dataset possum from the DAAG can be accessed as DAAG::possum, provided DAAG is installed on the computer. This has the advantage of leaving no room for ambiguity over the source of the dataset.\nDatasets can alternatively be made available, using the functions attach() or load(), on a per database basis. For this purpose, a database is an R package, or an image file that holds some or all of the datasets that have been saved from an R session. Or it can be a list or data frame, allowing the list elements or data frame columns to be referred to directly, without reference to the list or data frame. The following first saves the dataframes DAAG::possum and DAAG::cuckoos to the image file misc.RData in the working directory (enter getwd() to check where that is), and then attaches misc.RData, making both these datasets available from a database that is placed at position 2 on the search list:\n\npossum <- DAAG::possum; cuckoos <- DAAG::cuckoos\nsave(possum, cuckoos, file=\"misc.Rdata\")\nrm(possum, cuckoos)  # Remove from the workspace\n# Use `attach()` to make these available again.\nattach(\"misc.RData\")\n\nBe aware that if an object of the same name happens to be present in the workspace, that will be taken instead. The function load() can alternatively be used to load the objects saved in misc.RData into the workspace, which might be a safer way to proceed.\nIndividual dataframes can also be attached or loaded.\n\npossum <- DAAG::possum   # Load `possum` into the workspace\nattach(DAAG::possum)     # Add DAAG::possum to the search list\n  # Its columns can then be directly referenced by name \n\nDatabases that are attached in the course of a session are by default added at position 2 on the search list. Set the argument pos to a value greater than 2 in order to add at a later position.\nImage files, from the working directory or (with the path specified) from another directory, can be attached, thus making objects in the file available on request. For example\n\nattach(\"tempscales.RData\")\nls(pos=2)   # Check the contents of the file that has been attached\n\nThe parameter pos gives the position on the search list.\n\nDatasets in R packages\nType in data() to get a list of data sets (mostly data frames) associated with all packages that are in the current search path. To get information on the data sets that are included in the datasets package, specify\n\ndata(package=\"datasets\")\n\nand similarly for any other package. In most packages, data from an attached package are automatically available. Use of e.g., data(airquality) to attach the data set airquality (datasets package) is unnecessary. The out-of-the-box Windows and other binary distributions include a number of commonly required packages, including datasets. Other packages must be explicitly installed.\nThe base package, and several other packages (including datasets), are automatically attached at the beginning of the session. To attach any other installed package, use the library() command."
  },
  {
    "objectID": "basics.html#sec-loop",
    "href": "basics.html#sec-loop",
    "title": "1  An overview of the R system",
    "section": "1.8 Functions in R",
    "text": "1.8 Functions in R\nWe give two simple examples of R functions.\n\nAn Approximate Miles to Kilometers Conversion\n\nmiles.to.km <- function(miles)miles*8/5\n\nThe return value is the value of the final (and in this instance only) expression that appears in the function body . Use the function thus\n\nmiles.to.km(175)    # Approximate distance from Canberra to Sydney, in miles\n[1] 280\n\nThe function will do the conversion for several distances all at once. To convert a vector of the three distances 100, 200 and 300 miles to distances in kilometers, specify:\n\nmiles.to.km(c(100,200,300))\n[1] 160 320 480\n\n\n\nA function that returns the mean and standard deviation of a set of numbers\n\nmean.and.sd <- function(x=1:10){\n  av <- mean(x)\n  sd <- sqrt(var(x))\n  c(mean=av, SD=sd)\n}\n\nNotice that a default argument is supplied. Now invoke the function:\n\nmean.and.sd()       # Uses default argument\nmean   SD \n5.50 3.03 \nmean.and.sd(hills$climb)\nmean   SD \n1815 1619 \n\n\n\nLooping – the for() function\nA simple example of a for loop is\n\nfor (i in 1:10) print(i)\n\nHere is another example:\n\n# Celsius to Fahrenheit\nfor (celsius in 25:30)\n     print(c(celsius, 9/5*celsius + 32))\n[1] 25 77\n[1] 26.0 78.8\n[1] 27.0 80.6\n[1] 28.0 82.4\n[1] 29.0 84.2\n[1] 30 86\n\nA better way to formulate the calculation is:\n\ncelsius <- 25:30\nprint(9/5*celsius+32)\n[1] 77.0 78.8 80.6 82.4 84.2 86.0\n\nSkilled R users have limited recourse to loops. There are often, as in this and earlier examples, better alternatives.\n\n\nFunction syntax and semantics\nA function is created using an assignment. On the right hand side, the parameters appear within round brackets. A default can, optionally, be provided. In the example above the default was x = 1:10, so that users can run the function without specifying a parameter, just to see what it does. Following the closing “)” the function body appears. Except where the function body consists of just one statement, this is enclosed between curly braces ({ }). The return value usually appears on the final line of the function body. In the example above, this was the vector consisting of the two named elements mean and sd.\n\nAn example of a user function\nThe data set mtcars in the datasets package has data on 63 cars, as given in the 1974 Motor Trend US magazine.\n\nwith(mtcars, plot(hp, mpg))\n## Alternatively:\nwith(mtcars, plot(mpg ~ hp))   # mpg ~ hp is a graphics formula\n\nHere is a function that makes it possible to plot the figures for any pair of columns of mtcars:\n\nplot.mtcars <- function(form=mpg~hp, data=mtcars){\n    plot(form, data=data)\n    vars <- all.vars(form)   # Extract the variable names\n    mtext(side=3, line=1.5, \n     paste(\"Plot of\", vars[1], \"versus\", vars[2]))\n}\n\nObserve that the function body is enclosed in braces ({ }). Figure 1.4 shows the graph produced by plot.mtcars(). Parameter settings were left at their defaults.\n\n\n\n\nplot.mtcars()\n\n\n\n\nFigure 1.4: Plot of miles per gallon versus horsepower, for cars in the mtcars data frame\n\n\n\n\n\n\n\nCommon Useful Functions\n\nprint()     # Prints a single R object\ncat()       # Prints multiple objects, one after the other\nlength()    # Number of elements in a vector or of a list\nmean()\nmedian()\nrange()\nunique()    # Gives the vector of distinct values\ndiff()      # Replace a vector by the vector of first differences\n              # N. B. diff(x) has one less element than x\nsort()      # Sort elements into order, but omitting NAs\norder()     # x[order(x)] orders elements of x, with NAs last\ncumsum()\ncumprod()\nrev()       # reverse the order of vector elements\n\nThe functions mean(), median(), range(), and a number of other functions, allow the argument na.rm=T; i.e. remove NAs, then proceed with the calculation. By default, sort() omits any NAs. The function order() places NAs last. Hence:\n\nx <- c(1, 20,  2, NA, 22)\norder(x)\n[1] 1 3 2 5 4\nx[order(x)]\n[1]  1  2 20 22 NA\nsort(x)\n[1]  1  2 20 22\n\n\nString Functions\nsubstring(<vector of text strings>, <first position>, <last position>)\nnchar(<vector of text strings>)       \n              ## Returns vector of number of characters in each element.\n\n\n\nThe functions sapply(), lapply(), apply(), and tapply()\nThe functions are called as follows:\nlapply(<list>, <function>) \n               ## N. B. A dataframe is a list.  Output is a list.\nsapply(<list>, <function>)              \n               ## As lapply(), but simplify (e.g. to a vector\n               ## or matrix), if possible.\napply(<array>, <dimension>, <function>)\nBoth sapply() (‘s’=‘simplify’) and lapply() (‘l’=‘list’) can be used with lists or with vectors, as well as with dataframes.\nThe function lapply() works in the same way as sapply(), but generates a list, and does not attempt to bring the list elements together into a common structure.\nto the rows or columns of a matrix, can also be used with matrices. It can also be used with arrays of more than two dimensions.\nThe functions sapply() and lapply() take as arguments data frame, and the function that is to be applied. The following applies the function is.factor() to all columns of the supplied data frame rainforest.\n\nsapply(DAAG::rainforest, is.factor)\n    dbh    wood    bark    root  rootsk  branch species \n  FALSE   FALSE   FALSE   FALSE   FALSE   FALSE    TRUE \nsapply(DAAG::rainforest[,-7], range)   # The final column (7) is a factor\n     dbh wood bark root rootsk branch\n[1,]   4   NA   NA   NA     NA     NA\n[2,]  56   NA   NA   NA     NA     NA\n\nOne can specify na.rm=TRUE as a third argument to sapply().\nThis argument is then automatically passed to the function that is specified in the second argument position. For example:\n\nsapply(DAAG::rainforest[,-7], range, na.rm=TRUE)\n     dbh wood bark root rootsk branch\n[1,]   4    3    8    2    0.3      4\n[2,]  56 1530  105  135   24.0    120\n\nThe following code returns that number of missing values in each column of the data frame airquality.\n\nsapply(airquality, function(x)sum(is.na(x)))\n  Ozone Solar.R    Wind    Temp   Month     Day \n     37       7       0       0       0       0 \n\nThe function apply() can be used on data frames as well as matrices and arrays. An example that calculates means for each column is:\n\napply(airquality,2,mean)   # All elements must be numeric!\n  Ozone Solar.R    Wind    Temp   Month     Day \n     NA      NA    9.96   77.88    6.99   15.80 \napply(airquality,2,mean,na.rm=TRUE)\n  Ozone Solar.R    Wind    Temp   Month     Day \n  42.13  185.93    9.96   77.88    6.99   15.80 \n\nThe use of apply(airquality,1,mean) will give means for each row.\nThese are not, for these data, useful information!\n\n1.8.0.1 *An example that uses sapply() with a vector of text strings {-}\nWe will work with the column Make in the dataset MASS::Cars93. To find the position at which the first space appears in the information on make of car, we might do the following:\n\nCars93 <- MASS::Cars93\ncar.brandnames <- sapply(strsplit(as.character(Cars93$Make), \" \", fixed=TRUE),\n                         function(x)x[1])\ncar.brandnames[1:5]\n[1] \"Acura\" \"Acura\" \"Audi\"  \"Audi\"  \"BMW\"  \n\n\n\nUsing aggregate() and tapply()\nThe arguments are in each case a variable, a list of factors, and a function that operates on a vector to return a single value. For each combination of factor levels, the function is applied to corresponding values of the variable.\nThe function aggregate() returns a data frame. For example:\n\ncabbages <- MASS::cabbages\nstr(cabbages)\n'data.frame':   60 obs. of  4 variables:\n $ Cult  : Factor w/ 2 levels \"c39\",\"c52\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Date  : Factor w/ 3 levels \"d16\",\"d20\",\"d21\": 1 1 1 1 1 1 1 1 1 1 ...\n $ HeadWt: num  2.5 2.2 3.1 4.3 2.5 4.3 3.8 4.3 1.7 3.1 ...\n $ VitC  : int  51 55 45 42 53 50 50 52 56 49 ...\nwith(cabbages, aggregate(HeadWt, by=list(Cult=Cult, Date=Date), FUN=mean))\n  Cult Date    x\n1  c39  d16 3.18\n2  c52  d16 2.26\n3  c39  d20 2.80\n4  c52  d20 3.11\n5  c39  d21 2.74\n6  c52  d21 1.47\n\nThe syntax for tapply() is similar, except that the name of the second argument is INDEX rather than by. The output is an array with as many dimensions as there are factors. Where there are no data values for a particular combination of factor levels, NA is returned.\n\n\nFunctions for Confidence Intervals and Tests\nTwo of the simpler functions are t-test (allows both a one-sample and a two-sample test) and chisq.test() for testing for no association between rows and columns in a two way table. (This assumes counts enter independently into the cells of the table. The test is invalid if there is clustering in the data.)\nUse the help pages to get more complete information.\n\n\nMatching and Ordering\nmatch(<vec1>, <vec2>)  ## For each element of <vec1>, returns the \n                       ## position of the first occurrence in <vec2>\norder(<vector>)        ## Returns the vector of subscripts giving\n                       ## the order in which elements must be taken\n                       ## so that <vector> will be sorted.\nrank(<vector>)         ## Returns the ranks of the successive elements.\nNumeric vectors will be sorted in numerical order. Character vectors will be sorted in alphanumeric order. The operator %in% can be used to pick out subsets of data. For example:\n\nx <- rep(1:5,rep(3,5))\nx\n [1] 1 1 1 2 2 2 3 3 3 4 4 4 5 5 5\ntwo4 <- x %in% c(2,4)\ntwo4\n [1] FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE  TRUE  TRUE\n[13] FALSE FALSE FALSE\n# Now pick out the 2s and the 4s\nx[two4]\n[1] 2 2 2 4 4 4\n\n\n\nCompare Working Directory Data Sets with a Reference Set\nAt the beginning of a new session, we might store the names of the objects in the working directory in the vector dsetnames, thus:\n\ndsetnames <- objects()\n\nNow suppose that we have a function additions(), defined thus:\n\nadditions <- function(objnames = dsetnames)\n{\n        newnames <- objects(pos=1)\n        existing <- as.logical(match(newnames, objnames, nomatch = 0))\n        newnames[!existing]\n}\n\nAt some later point in the session, we can enter\n\nadditions(dsetnames)\n\nto get the names of objects that have been added since the start of the session.\n\n\nA Simulation Example\nWe would like to know how well such a student might do by random guessing, on a multiple choice test consisting of 15 questions each with five alternatives. If the choice is truly random, with the same probability for all 5 possibilities, we can simulate the correctness of the student for each question by using sample(1:5, size=1), taking 1 as ‘correct’ and any other number as ‘wrong’. For 15 questions we can do sample(1:5, replace=T, size=15), and repeat this perhaps 1000 times to get an idea of the distribution of number of right answers. Or we can calculate the distribution as dbinom(x, size=15, prob=0.2), with x running from 0 to 15.\n\nresp1000 <- matrix(sample(1:5, replace=T, size=15000),nrow=1000)\n  # Each row counts as one set of responses\ncorrect <- apply(resp1000, 1, function(x)sum(x==1))\n## Look at distribution of number of correct answers.\ntable(correct)/1000\ncorrect\n    0     1     2     3     4     5     6     7     8 \n0.044 0.118 0.223 0.258 0.178 0.116 0.048 0.012 0.003 \n## Now use `dbinom()` to obtain expected numbers\nsetNames(round(dbinom(0:15, size=15, prob=0.2), 3), 0:15)[1:11]\n    0     1     2     3     4     5     6     7     8     9    10 \n0.035 0.132 0.231 0.250 0.188 0.103 0.043 0.014 0.003 0.001 0.000 \n  # For x>0, the probability is no more than 0.0005\nround(pbinom(8, size=15, prob=0.2, lower.tail=F),4)  ## 9 or more\n[1] 8e-04\n\n\n\nPoisson Random Numbers\nOne can think of the Poisson distribution as the distribution of the total for occurrences of rare events. For example, an accident at an intersection on any one day should be a rare event. The total number of accidents over the course of a year may well follow a distribution that is close to Poisson. However, the total number of people injured is unlikely to follow a Poisson distribution. Why?\nThe function using rpois() generates Poisson random numbers. Suppose for example that traffic accidents occur at an intersection with a Poisson distribution that has a mean rate of 3.7 per year. To simulate the annual number of accidents for a 10-year period, we can specify rpois(10,3.7). We pursue the Poisson distribution in an exercise below.\n\n\nFunctions that assist with data management\nWhere data, labeling etc must be pulled together from a number of sources, and especially where you may want to retrace your steps some months later, take the same care over structuring data as over structuring code. Thus if there is a factorial structure to the data files, choose file names that reflect it. You can then generate the file names automatically, using paste() to glue the separate portions of the name together.\nLists are a useful mechanism for grouping together all data and labeling information that one may wish to bring together in a single set of computations. Use as the name of the list a unique and meaningful identification code. Consider whether you should include objects as list items, or whether identification by name is preferable. Bear in mind, also, the use of switch(), with the identification code used to determine what switch() should pick out, to pull out specific information and data that is required for a particular run. Concentrate in one function the task of pulling together data and labeling information, perhaps with some subsequent manipulation, from a number of separate files. This structures the code, and makes the function a source of documentation for the data.\n\n\n\nIssues for the Writing and Use of Functions\nThere can be many functions. Choose their names and argument names carefully, so that they are meaningful, even if this means that they are longer than one would like.\nThere are mechanisms by which names and argument names can be abbreviated in actual use. As far as possible, make code self-documenting. Use meaningful names for R objects. Ensure that the names used reflect the hierarchies of files, data structures and code.\nR allows the use of names for elements of vectors and lists, and for rows and columns of arrays and dataframes. Consider the use of names rather than numbers when you pull out individual elements, columns etc. Thus dead.tot[,“dead”] is more meaningful and safer than dead.tot[,2]. Settings that may need to change in later use of the function should appear as default settings for parameters. Use lists, where this seems appropriate, to group together parameters that belong together conceptually.\nWhere appropriate, provide a demonstration mode for functions. Such a mode will print out summary information on the data and/or on the results of manipulations prior to analysis, with appropriate labeling. The code needed to implement this feature has the side-effect of showing by example what the function does, and may be useful for debugging.\nBreak functions up into a small number of sub-functions or “primitives”. Re-use existing functions wherever possible. Write any new “primitives” so that they can be re-used. This helps ensure that functions contain well-tested and well-understood components. Watch the r-help electronic mail list (section 13.3) for useful functions for routine tasks.\nWherever possible, give parameters sensible defaults. Often a good strategy is to use as defaults parameters that will serve for a demonstration run of the function.\nNULL is a useful default where the parameter mostly is not required, but where the parameter if it appears may be any one of several types of data structure. The test if(!is.null()) then determines whether one needs to investigate that parameter further.\nStructure computations so that it is easy to retrace them. For this reason substantial chunks of code should be incorporated into functions sooner rather than later.\nStructure code to avoid multiple entry of information."
  },
  {
    "objectID": "basics.html#making-tables",
    "href": "basics.html#making-tables",
    "title": "1  An overview of the R system",
    "section": "1.9 Making Tables",
    "text": "1.9 Making Tables\nThe function table() makes a table of counts. Specify one vector of values (often a factor) for each table margin that is required. For example:\n\nlibrary(lattice)    # The data frame `barley` is included in lattice\ntable(barley$year, barley$site)\n      \n       Grand Rapids Duluth University Farm Morris Crookston Waseca\n  1932           10     10              10     10        10     10\n  1931           10     10              10     10        10     10\n\nWARNING: NAs are by default ignored. The action needed to get NAs tabulated under a separate NA category depends, annoyingly, on whether or not the vector is a factor. If the vector is not a factor, specify exclude=NULL. If the vector is a factor then it is necessary to generate a new factor that includes NA as a level. Specify x <- factor(x,exclude=NULL).\n\nx <- c(1,5,NA,8)\nx <- factor(x)\nx\n[1] 1    5    <NA> 8   \nLevels: 1 5 8\nfactor(x,exclude=NULL)\n[1] 1    5    <NA> 8   \nLevels: 1 5 8 <NA>\n\n\nNumbers of NAs in subgroups of the data\nThe following gives information on the number of NAs in subgroups of the data:\n\nrainforest <- DAAG::rainforest\ntable(rainforest$species, !is.na(rainforest$branch))\n                 \n                  FALSE TRUE\n  Acacia mabellae     6   10\n  C. fraseri          0   12\n  Acmena smithii     15   11\n  B. myrtifolia       1   10\n\nThus for Acacia mabellae the variable branch (i.e. number of branches over 2cm in diameter) has 6 NAs, out of a total of 16 data values."
  },
  {
    "objectID": "basics.html#methods",
    "href": "basics.html#methods",
    "title": "1  An overview of the R system",
    "section": "1.10 Methods",
    "text": "1.10 Methods\nR is an object-oriented language. Objects may have a class. Functions that have generic implementations include print(), summary(), and plot() the class of the object supplied as argument determines what action will be taken. Thus in response to print(x), R determines the class attribute of x, if one exists. If for example the class attribute is \"factor\" then the function which finally handles the printing is print.factor(). The function print.default() is used to print objects that have not been assigned a class.\nMore generally, the class attribute of an object may be a character vector. If there are ancestor classes – parent, grandparent, . . ., these are specified in order in subsequent elements of the class vector. For example, ordered factors have the class \"ordered“, which inherits from the class \"factor\". Thus:\n\nfac<-ordered(1:3)\nclass(fac)\n[1] \"ordered\" \"factor\""
  },
  {
    "objectID": "basics.html#some-further-programming-niceties",
    "href": "basics.html#some-further-programming-niceties",
    "title": "1  An overview of the R system",
    "section": "1.11 *Some Further Programming Niceties",
    "text": "1.11 *Some Further Programming Niceties\n\n1.11.1 *Extracting Arguments to Functions {-}\nWhen an argument appears inside a function, it is taken to be another name for the object to which the argument refers. Other possibilities are that one might want the name itself, or the character vector that has the name. Thus, consider:\n\nfun1 <- function(x)x\nfun2 <- function(x)substitute(x)\nfun3 <- function(x)deparse(substitute(x))\n\nNow see what happens, in the three cases, when the argument is the linear model object created thus:\n\ncars.lm <- lm(dist ~ speed, data=cars) \nfun1(cars.lm)\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nCoefficients:\n(Intercept)        speed  \n     -17.58         3.93  \nfun2(cars.lm)\ncars.lm\nfun3(cars.lm)\n[1] \"cars.lm\"\n\n\n\n1.11.2 *Parsing and Evaluation of Expressions {-}\nWhen R encounters an expression such as mean(x+y) or cbind(x,y), there are two steps:\n* The text string is parsed and turned into an expression, i.e. the syntax is checked and it is turned into code that the R computing engine can more immediately evaluate. * The expression is evaluated.\nUpon typing in\n\nexpression(mean(x+y))\nexpression(mean(x + y))\n\nthe output is the unevaluated expression expression(mean(x+y)). Setting\n\nmy.exp <- expression(mean(x+y))\n\nstores this unevaluated expression in my.exp. The actual contents of my.exp are a little different from what is printed out. R gives as much information as it thinks helpful.\nNote that expression(mean(x+y)) is different from expression(“mean(x+y)”), as is obvious when the expression is evaluated.\nHere is an example:\n\nx <- 101:110\ny <- 21:30\nmy.exp <- expression(mean(x+y))\nmy.txt <- expression(\"mean(x+y)\")\neval(my.exp)\n[1] 131\neval(my.txt)\n[1] \"mean(x+y)\"\n\nThe function parse(), used with the argument text, takes code that is stored in a text string and turns it into an expression.\n\nparse(text=\"mean(x+y)\")\nexpression(mean(x + y))\nexpression(mean(x + y))\nexpression(mean(x + y))\n# We store the expression in my.exp2, and then evaluate it\nmy.exp2 <- parse(text=\"mean(x+y)\")\neval(my.exp2, list(x=1:5, y=13:17))\n[1] 18\nwith(list(x=1:5, y=13:17), eval(my.exp2))\n[1] 18"
  },
  {
    "objectID": "basics.html#next-steps",
    "href": "basics.html#next-steps",
    "title": "1  An overview of the R system",
    "section": "1.12 Next steps",
    "text": "1.12 Next steps\nIt may pay, at this point, to glance through chapters 6 and 7, which have a more detailed coverage of the topics in this chapter. Remember also to use R’s help pages and functions. Topics from chapter 6, additional to those covered above, that may be important for relatively elementary uses of R include:\n\nThe entry of patterned data (6.1.3)\nThe handling of missing values in subscripts when vectors are assigned (7.2)\nUnexpected consequences (e.g. conversion of columns of numeric data into factors) from errors in data (6.4.1)."
  },
  {
    "objectID": "basics.html#exercises",
    "href": "basics.html#exercises",
    "title": "1  An overview of the R system",
    "section": "1.13 Exercises",
    "text": "1.13 Exercises\n\nIn the data frame elasticband from section 1.3.1, plot distance against stretch.\nThe following creates a data frame that has October snow cover (millions of square kilometers)) in Eurasia for the years 1970-79:\n\n\n\nsnowCover <- data.frame(\n  year = 1970:1979,\n  snow.cover = c(6.5, 12, 14.9, 10, 10.7, 7.9, 21.9, 12.5, 14.5, 9.2))\n\n\nEnter the data into R.\nPlot snow.cover versus year.\n\nUse the hist() command to plot a histogram of the snow cover values.\n\nRepeat ii and iii after taking logarithms of snow cover.\n\n\n\nInput the following data, on damage that had occurred in space shuttle launches prior to the disastrous launch of Jan 28 1986. These are the data, for 6 launches out of 24, that were included in the pre-launch charts that were used in deciding whether to proceed with the launch. (Data for the 23 launches where information is available is in the data set orings, from the DAAG package.)\n\n\n\n\nTemperature\nErosion\nBlowby\nTotal\n\n\n\n\n53\n3\n2\n5\n\n\n57\n1\n0\n1\n\n\n63\n1\n0\n1\n\n\n70\n1\n0\n1\n\n\n70\n1\n0\n1\n\n\n75\n0\n2\n1\n\n\n\nEnter these data into a dataframe, with (for example) column names temperature, erosion, blowby and total. (Refer back to Section 1.6). Plot total incidents against temperature.\n\nFor each of the following code sequences, predict the result. Then do the computation:\n\n\n## a) \nanswer <- 0\nfor (j in 3:5){ answer <- j+answer }\n## b)\nanswer<- 10\nfor (j in 3:5){ answer <- j+answer }\n## c)\nanswer <- 10\nfor (j in 3:5){ answer <- j*answer }\n\n\nLook up the help for the function prod(), and use prod() to do the calculation in 1(c) above. Alternatively, how would you expect prod() to work? Try it!\nAdd up all the numbers from 1 to 100 in two different ways: using for(), and using sum(). Now apply the function to the sequence 1:100. What is its action?\nMultiply all the numbers from 1 to 50 in two different ways: using for() and using prod().\nThe volume of a sphere of radius \\(r\\) is \\(4r^3/3\\). For spheres having radii 3, 4, 5, …, 20 find the corresponding volumes and print the results out in a table. Construct a data frame with columns radius and volume.\nUse sapply() to apply the function is.factor() to each column of the data frame DAAG::tinting. For each of the columns that are identified as factors, determine the levels. Which columns are ordered factors? [Use is.ordered()].\nGenerate and save the following sets of numbers:\n\n\n\nGenerate the numbers 101, 102, …, 112, and store the result in the vector x.\n\nGenerate four repeats of the sequence of numbers (4, 6, 3).\n\nGenerate the sequence consisting of eight 4s, then seven 6s, and finally nine 3s. Store the numbers obtained , in order, in the columns of a 6 by 4 matrix.\n\nCreate a vector consisting of one 1, then two 2’s, three 3’s, etc., and ending with nine 9’s.\n\n\nFor each of the following calculations, what you would expect? Check to see if you were right!\n\n\n\n\n\n\nanswer <- c(2, 7, 1, 5, 12, 3, 4)  \nfor (j in 2:length(answer)){ answer[j] <- max(answer[j],answer[j-1])}\n\n\n\n\n\nanswer <- c(2, 7, 1, 5, 12, 3, 4)\nfor (j in 2:length(answer)){ answer[j] <- sum(answer[j],answer[j-1])}\n\n\nIn the data frame airquality (datasets package):\n\n\n\nDetermine, for each of the columns, the median, mean, upper and lower quartiles, and range;\n\nExtract the row or rows for which Ozone has its maximum value;\n\nextract the vector of values of Wind for values of Ozone that are above the upper quartile.\n\n\nRefer to the Eurasian snow data that is given in Exercise 1.6.\nFind the mean of the snow cover (a) for the odd-numbered years and (b) for the even-numbered years.\nDetermine which columns of the data frame MASS::Cars93 are factors. For each of these factor columns, print out the levels vector. Which of these are ordered factors?\nUse summary() to get information about data in the data frame attitude (in the datasets package), and MASS::cpus. Comment, for each of these data sets, on what this reveals.\nFrom the data frame MASS::mtcars extract a data frame mtcars6 that holds only the information for cars with 6 cylinders.\nFrom the data frame MASS::Cars93, extract a data frame which holds information for small and sporty cars.\nUse the function sample() function to generate 100 random integers between 0 and 19. Now look up the help for runif(), and use it for the same purpose. [Count a value between 0 and 0.05 as 0, where the range is from 0 to 1.]\nWrite a function that will take as its arguments a list of response variables, a list of factors, a data frame, and a function such as mean or median. It will return a data frame in which each value for each combination of factor levels is summarized in a single statistic, for example the mean or the median.\nDetermine the number of days, according to R, between the following dates:\n\n\n\nJanuary 1 in the year 1700, and January 1 in the year 1800\n\nJanuary 1 in the year 1998, and January 1 in the year 2000\n\n\nSeventeen people rated the sweetness of each of two samples of a milk product on a continuous scale from 1 to 7, one sample with four units of additive and the other with one unit of additive. The data frame DAAG::milk, with columns four and one, shows the ratings.\nHere is a function that plots, for each patient, the four result against the one result, with the same range for the x and y axes.\n\n\nplot.one <- function(){\n  xyrange <- range(milk)   # Calculates the range of all values in the data frame\n  par(pin=c(6.75, 6.75))      # Set plotting area = 6.75 in. by 6.75 in.\n  plot(four, one, data=milk, xlim=xyrange, ylim=xyrange, pch=16)\n  abline(0,1)                  # Line where four = one\n}\n\nRewrite this function so that, given the name of a data frame and of any two of its columns, it will plot the second named column against the first named column, showing also the line y = x.\n\nWrite a function that prints, with their row and column labels, only those elements of a correlation matrix for which abs(correlation) >= 0.9.\nWrite q wrapper function for one-way analysis of variance that provides a side by side boxplot of the distribution of values by groups. If no response variable is specified, the function will generate random normal data (no difference between groups) and provide the analysis of variance and show the boxplot.\nUse the function zoo::rollmean to compute the moving average of order 2 of the data for levels of Lake Erie, included in the dataset DAAG::greatLakes. Repeat for order 3.\nCreate a function to compute the average, variance and standard deviation of 1000 randomly generated uniform random numbers, on [0,1]. Compare your results with the theoretical results. The expected value of a uniform random variable on [0,1] is 0.5, and the variance of such a random variable is 0.0833.\nWrite a function that generates 100 independent observations on a uniformly distributed random variable on the interval [3.7, 5.8].\nFind the mean, variance and standard deviation of such a uniform random variable. Now modify the function so that you can specify an arbitrary interval.\nLook up the help for the sample() function. Use it to generate 50 random integers between 0 and 99, sampled without replacement. (This means that we do not allow any number to be sampled a second time.) Repeat several times. Now, generate 50 random integers between 0 and 9, with replacement. Determine the proportion of integers that occur more than once.\nWrite an R function that simulates a student guessing at a True-False test consisting of 40 questions. Find the mean and variance of the student’s answers. Compare with the theoretical values of .5 and .25.\nWrite an R function that simulates a student guessing at a multiple choice test consisting of 40 questions, where there is a chance of 1 in 5 of getting the right answer to each question.\nFind the mean and variance of the student’s answers. Compare with the theoretical values of .2 and .16.\nWrite an R function that simulates the number of working light bulbs out of 500, where each bulb has a probability .99 of working.\nUsing simulation, estimate the expected value and variance of the random variable X, which is 1 if the light bulb works and 0 if the light bulb does not work. What are the theoretical values?\nWrite a function that does an arbitrary number n of repeated simulations of the number of accidents in a year, plotting the result in a suitable way. Assume that the number of accidents in a year follows a Poisson distribution. Run the function assuming an average rate of 2.8 accidents per year.\nWrite a function that simulates the repeated calculation of the coefficient of variation (= the ratio of the mean to the standard deviation), for independent random samples from a normal distribution.\nWrite a function that, for any sample, calculates the median of the absolute values of the deviations from the sample median.\n*Generate random samples from normal, exponential, t (2 d. f.), and t (1 d. f.), thus:\n\n\n\nxn <- rnorm(100) (b) xe <- rexp(100)\n\nxt2 <- rt(100, df=2) (d) xt2 <- rt(100, df=1)\nApply the function from exercise 17 to each sample. Compare with the standard deviation in each case.\n\n\n*The vector x consists of the frequencies\n5, 3, 1, 4, 6\nThe first element is the number of occurrences of level 1 of a factor, the second is the number of occurrences of level 2, and so on. Write a function that takes any such vector x as its input, and outputs the vector of factor levels, here 1 1 1 1 1 2 2 2 3 . . .\n[You’ll need the information that is provided by cumsum(x). Form a vector in which 1’s appear whenever the factor level is incremented, and is otherwise zero. . . .]\n*Write a function that calculates the minimum of a quadratic expression, and the value of the function at the minimum.\n*A “between times” correlation matrix, has been calculated from data on heights of trees at times 1, 2, 3, 4, . . . Write a function that calculates the average of the correlations for any given lag.\n*Given data on trees at times 1, 2, 3, 4, . . ., write a function that calculates the matrix of “average” relative growth rates over the for each interval."
  },
  {
    "objectID": "basics.html#references-and-reading",
    "href": "basics.html#references-and-reading",
    "title": "1  An overview of the R system",
    "section": "1.14 References and reading",
    "text": "1.14 References and reading\nBraun and Murdoch (2021) . A first course in statistical programming with R.\nDalgaard (2008) . Introductory Statistics with R.\n[This introductory text has a biostatistical emphasis.]\nJ. Maindonald and Braun (2010) . Data Analysis and Graphics Using R –- An Example-Based Approach.\nJ. Maindonald, Braun, and Andrews (2024, forthcoming) . A Practical Guide to Data Analysis Using R. An Example-Based Approach. Cambridge University Press.\n[Appendix A gives a brief overview of the R language and system.]\nMatloff (2011) . The Art of R Programming.\nMurrell (2009) . Introduction to data technologies.\nMuenchen (2011) . R for SAS and SPSS Users.\nVenables and Ripley (2002) . Modern Applied Statistics with S.\n[This assumes a fair level of statistical sophistication. Explanation is careful, but often terse.]\nWickham (2016) . R for Data Science.\n\nReferences to packages\nDAAG: J. H. Maindonald and Braun (2022)\nlattice: Sarkar (2023)\nMASS: Ripley (2023)\n\n\n\n\nBraun, W. John, and Duncan J. Murdoch. 2021. A First Course in Statistical Programming with R. 3rd ed. Cambridge University Press.\n\n\nDalgaard, Peter. 2008. Introductory Statistics with R. 2nd ed. Springer.\n\n\nMaindonald, John H, and W. John Braun. 2022. DAAG: Data Analysis and Graphics Data and Functions. https://gitlab.com/daagur.\n\n\nMaindonald, John, and John Braun. 2010. Data Analysis and Graphics Using r: An Example-Based Approach. Cambridge University Press.\n\n\nMaindonald, John, W John Braun, and Jeffrey Andrews. 2024, forthcoming. A Practical Guide to Data Analysis Using r. An Example-Based Approach. Cambridge University Press. https://jhmaindonald.github.io/PGRcode.\n\n\nMatloff, Norman. 2011. The Art of r Programming. No Starch Inc.\n\n\nMuenchen, R. A. 2011. R for SAS and SPSS Users. 2nd ed. Springer. http://r4stats.com/books/r4sas-spss/.\n\n\nMurrell, Paul. 2009. Introduction to Data Technologies. 1st ed. CRC Press.\n\n\nRipley, Brian. 2023. MASS: Support Functions and Datasets for Venables and Ripley’s MASS. http://www.stats.ox.ac.uk/pub/MASS4/.\n\n\nSarkar, Deepayan. 2023. Lattice: Trellis Graphics for r. https://lattice.r-forge.r-project.org/.\n\n\nVenables, W. N., and B. D. Ripley. 2002. Modern Applied Statistics with S. 4th ed. Springer.\n\n\nWickham, H. 2016. R for Data Science. O’Reilly."
  },
  {
    "objectID": "graphs.html#the-base-graphics-plot-scatterplot-function",
    "href": "graphs.html#the-base-graphics-plot-scatterplot-function",
    "title": "2  Base R Graphics",
    "section": "2.1 The base graphics plot() scatterplot function",
    "text": "2.1 The base graphics plot() scatterplot function\nThe functions plot(), points(), lines(), text(), mtext(), axis(), and identify() are part of a suite that plots points, lines and text. To see some of the possibilities that R offers, enter\n\ndemo(graphics)\n\nPress the Enter key to move to each new graph."
  },
  {
    "objectID": "graphs.html#plot-and-allied-functions",
    "href": "graphs.html#plot-and-allied-functions",
    "title": "2  Base R Graphics",
    "section": "2.2 plot() and allied functions",
    "text": "2.2 plot() and allied functions\nThe following both plot y against x:\n\nplot(y ~ x)  # Use a formula to specify the graph\nplot(x, y)   # \n\nIt is usual for x and y to be numeric vectors of the same length. (Also possible, but rarely used, is for x to be a factor.) Try\n\nplot((0:20)*pi/10, sin((0:20)*pi/10))\nplot((1:30)*0.92, sin((1:30)*0.92))\n\nCompare the appearance that these graphs present. Is it obvious that these points lie on a sine curve? How can one make it obvious? (Place the cursor over the lower border of the graph sheet, until it becomes a double-sided arror. Drag the border in towards the top border, making the graph sheet short and wide.)\nTwo further examples are:\n\nplot(distance ~ stretch, data=DAAG::elasticband) \nplot(ACT ~ Year, data=DAAG::austpop, type=\"l\")\nplot(ACT ~ Year, data=DAAG::austpop, type=\"b\")\n\nThe points() function adds points to a plot. The lines() function adds lines to a plot . The text() function adds text at specified locations. The mtext() function places text in one of the margins. The axis() function gives fine control over the adding of axis ticks and labels, usually on a graph where one or both axes have been omitted (xaxt='n' or yaxt='n' or axes=false) from the initial plot.\nHere is a further possibility\n\nplot(spline(Year, ACT), data=DAAG::austpop, type=\"l\")  \n  # Fit smooth curve through points\n\n\n2.2.1 Plot methods for other classes of object\nThe plot function is a generic function that has special methods for “plotting” various different classes of object. For example, plotting a data frame gives, for each numeric variable, a normal probability plot. Plotting the lm object that is created by the use of the lm() linear model function gives diagnostic and other information that is designed to help in the interpretation of regression results. Try\n\nplot(hills)  # Has the same effect as `pairs(hills)`"
  },
  {
    "objectID": "graphs.html#fine-control-parameter-settings",
    "href": "graphs.html#fine-control-parameter-settings",
    "title": "2  Base R Graphics",
    "section": "2.3 Fine control – Parameter settings",
    "text": "2.3 Fine control – Parameter settings\nThe default settings of parameters, such as character size, are often adequate. In order to change parameter settings for a subsequent plot, use the par() function. For example\n\npar(cex=1.25)   # character expansion\nThis increases the text and plot symbol size 25% above the default. \n\nOn the first use of par() to make changes for the current device, it is often useful to store existing settings, so that they can be restored later. For this, specify, e.g. \n\noldpar <- par(cex=1.25, mex=1.25)  # mex=1.25 expands the margin by 25%\n\nThis stores the existing settings in oldpar, then changes parameters (here cex and mex) as requested. To restore the original parameter settings at some later time, enter par(oldpar). Here is an example:\n\noldpar <- par(cex=1.5)\nplot(distance ~ stretch, data=elasticband) \npar(oldpar)         # Restores the earlier settings\n\nInside a function one can specify, e.g. \n\noldpar <- par(cex=1.25)\non.exit(par(oldpar))\n\nType in help(par) to get details of available parameter settings.\n\n2.3.1 Multiple plots on the one page\nThe parameter mfrow can be used to configure the graphics sheet so that subsequent plots appear row by row, one after the other in a rectangular layout, on the one page.\nFor a column by column layout, use mfcol instead. Figure 2.1 uses a two by two layout to show plots of brain versus body with four different transformations, for data in the DAAG::MASS::Animals data frame:\n\n\n\n\n\n\n\n\nFigure 2.1: Plots of brain versus body for 26 different animals, with four different transformations.\n\n\n\n\n\n## Code\npar(mfrow=c(2,2))\nwith(MASS::Animals, {\n    plot(body, brain)\n    plot(sqrt(body), sqrt(brain))\n    plot(body^0.1, brain^0.1)\n    plot(log(body),log(brain))\n})\n\nNote the use of braces ({,}) to bracket together the four plot statements, as a way to ensure that in all four cases the variable names refer to columns of the MASS::Animals data frame.\n\n\nThe shape of the graph sheet\nOne may for example want the individual plots to be rectangular rather than square. The R for Windows functions win.graph() or x11() that set up the Windows screen take the parameters width (in inches), height (in inches) and pointsize (in 1/72 of an inch). The setting of pointsize (default =12) determines character heights. It is the relative sizes of these parameters that matter for screen display or for incorporation into Word and similar programs. Graphs can be enlarged or shrunk by pointing at one corner, holding down the left mouse button, and pulling."
  },
  {
    "objectID": "graphs.html#adding-points-lines-and-text",
    "href": "graphs.html#adding-points-lines-and-text",
    "title": "2  Base R Graphics",
    "section": "2.4 Adding points, lines and text",
    "text": "2.4 Adding points, lines and text\nHere is a simple example that uses the function text() to add text labels to the points on a plot. Data are\n\nprimates <- DAAG::primates\nprimates\n              Bodywt Brainwt\nPotar monkey    10.0     115\nGorilla        207.0     406\nHuman           62.0    1320\nRhesus monkey    6.8     179\nChimp           52.2     440\n## Observe that the row names store labels for each row\n\n\n\n\nFigure 2.2 (a) would be adequate for identifying points, but is not a presentation quality graph. Figure 2.2 (b) uses the xlab (x-axis) and ylab (y-axis) parameters to specify meaningful axis titles. It uses the parameter setting pos=4 to move the labeling to the right of the points. It sets pch=16 to make the plot character a heavy black dot. This helps make the points stand out against the labeling.\n\n\n\n\n\nFigure 2.2: Plots of the primates data, with labels on points. Panel B improves on Panel A\n\n\n\n\nTo place the text to the left of the points, specify\n\ntext(x=Bodywt, y=Brainwt, labels=row.names(primates), pos=2)\n\nHere is the R code for Figure 2.2:\n\n\n\n\n2.4.1 Size, colour, and choice of plotting symbol\nFor plot() and points() the parameter cex (“character expansion”) controls the size, while col (“colour”) controls the colour of the plotting symbol. The parameter pch controls the choice of plotting symbol. The parameters cex and col may be used in a similar way with text(). Try\n\noldpar <- par(mar=rep(0.15,4))\nplot(1, 1, xlim=c(1, 7.35), ylim=c(1.75,5), type=\"n\", axes=F, xlab=\"\",\n         ylab=\"\")   # Do not plot points or axes\nbox()               # Draw a box around the plot area\npoints(1:7, rep(4.65, 7), cex=2:8, col=1:7, pch=0:6, lwd=4)\ntext(1:7,rep(3.5, 7), labels=paste(0:6), cex=2:8, col=1:7)\n\nThe following, added to the plot that results from the above three statements, demonstrates other choices of pch.\n\npoints(1:7,rep(2.5,7), pch=(0:6)+7, cex=3)  # Plot symbols 7 to 13\ntext((1:7), rep(2.5,7), paste((0:6)+7), pos=4, cex=1)  # Add symbol number\npoints(1:7,rep(2,7), pch=(0:6)+14, cex=3)       # Plot symbols 14 to 20\ntext((1:7), rep(2,7), paste((0:6)+14), pos=4, cex=1)   # Add symbol number\npar(oldpar)\n\nNote the use of pos=4 to position the text to the right of the point (1=below, 2=left, 3=top, 4=right). Figure 2.3 shows the plots:\n\n\n\n\n\n\n\n\nFigure 2.3: Different plot symbols, colours and sizes\n\n\n\n\nA variety of color palettes are available. Figure 2.4 shows some of the possibilities:\n\n\n\n\nview.colors <- function(xlim=c(0.55,10.9), ylim=c(0,4)){\n  oldpar <- par(mar=rep(0.15,4))\n    plot(1, 1, xlim=xlim, ylim=ylim, type=\"n\", axes=F, \n         xlab=\"\",ylab=\"\", xaxs=\"i\", yaxs=\"i\")\n    text(1:8, rep(3.5,8), paste(1:8), col=palette()[1:8], cex=4)\n    text(9, 3.5, \"Default palette\", adj=0)\n    rainchars <- c(\"R\",\"O\",\"Y\",\"G\",\"B\",\"I\",\"V\")\n    text(1:7, rep(2.5,7), rainchars, col=rainbow(7), cex=4)\n    text(9, 2.5, \"rainbow(7)\", adj=0)\n  ## topo.colors()\n    rect((1:8)-0.5, rep(1.05,8), (1:8)+0.5, rep(1.95,8), col=topo.colors(8))\n    text(1:8, rep(1.5,8), paste(1:8), col=\"gray\", cex=2)\n    text(9, 1.5, \"topo.colors(8)\", adj=0)\n    ## heat.colors()\n    rect((1:8)-0.5, rep(0.05,8), (1:8)+0.5, rep(0.95,8), col=heat.colors(8))\n    text(1:8, rep(0.5,8), paste(1:8), col=\"gray\", cex=2)\n    text(9, 0.5, \"heat.colors(8)\", adj=0)\n    par(oldpar)\n}\n\n\n\n\n\n\nFigure 2.4: Some available colour palettes\n\n\n\n\nTo run the function, enter\n\n\n\n\n\n2.4.2 Adding Text in one of the margins\nUse mtext(side, line, text, ..) to add text in a margin (side) of the current plot. The sides are numbered 1(x-axis), 2(y-axis), 3(top) and 4(right)."
  },
  {
    "objectID": "graphs.html#identification-and-location-on-the-figure-region",
    "href": "graphs.html#identification-and-location-on-the-figure-region",
    "title": "2  Base R Graphics",
    "section": "2.5 Identification and Location on the Figure Region",
    "text": "2.5 Identification and Location on the Figure Region\nThe functions identify() and locator() can be used for this purpose. Draw the graph first, then call one or other of these functions.\nUse locator() to show the co-ordinates of points, and identify() to label points. Position the cursor near the point whose co-ordinates are required, and click the left mouse button. prints out the co-ordinates of points. One positions the cursor at the location for which coordinates are required, and click the left mouse button. Depending on the screen device, a click with a mouse button other than the first, or pressing the ESC key terminates the process and returns a list that has the co-ordinates of the points. Or, if the setting of the parameter n (by default 360) is reached first, that terminates the process and returns the list.\nThe function identify() requires specification of a vector x, a vector y, and a vector of text strings that are available for use a labels. The parameter n is set to the number of points. Click to the left or right, and slightly above or below a point, depending on the preferred positioning of the label. When labeling is terminated, the row numbers of the observations that have been labelled are printed on the screen, in order. Thus try:\n\nplot(brain ~ body, data=MASS::Animals)\nwith(primates, identify(Bodywt, Brainwt, rownames(primates)))\n\nThe function can be used to mark new points (specify type=\"p\") or lines (specify type=\"l\") or both points and lines (specify type=\"b\")."
  },
  {
    "objectID": "graphs.html#plots-that-show-the-distribution-of-data-values",
    "href": "graphs.html#plots-that-show-the-distribution-of-data-values",
    "title": "2  Base R Graphics",
    "section": "2.6 Plots that show the distribution of data values",
    "text": "2.6 Plots that show the distribution of data values\nWe discuss histograms, density plots, boxplots and normal probability plots.\n\n2.6.1 Histograms and density plots\nThe shapes of histograms depend on the placement of the breaks, as Figure 2.5 illustrates:\n\n\n\n\npar(mfrow=c(1,3))\npossum <- DAAG::possum\nwith(possum, {\nhere <- sex == \"f\"\nhist(totlngth[here], breaks = 72.5 + (0:5) * 5, ylim = c(0, 22),\n        xlab=\"Total length\", main =\"A: Breaks at 72.5, 77.5, ...\")\n\nplot(density(totlngth[here]),type=\"l\", main=\"B: Density curve\")\ndens <- density(totlngth[here])\nxlim <- range(dens$x)\nylim <- range(dens$y)\nhist(totlngth[here], breaks = 72.5 + (0:5) * 5, probability = TRUE,\n   xlim = xlim, ylim = ylim, xlab=\"Total length\", \n   main=\"C: Breaks at 73, 78, ...\")\nlines(dens)\n})\n\n\n\n\nFigure 2.5: Panel A shows a histogram with a frequency scale. Panel B is drawn with a density scale, so that a density curve can be readily superimposed. Panel C has a different choice of breakpoints, so that the histogram gives a rather different impression of the distribution of the data. The density curve is superimposed.\n\n\n\n\nHere is the code used to plot the histogram in Figure 2.5 A.\n\nwith(possum, {\n  here <- sex == \"f\"\n  hist(totlngth[here], breaks = 72.5 + (0:5) * 5, ylim = c(0, 22),\n          xlab=\"Total length\", main =\"A: Breaks at 72.5, 77.5, ...\")\n  })\n\nDensity plots, now that they are available, are often a preferred alternative to a histogram. A histogram may be viewed as a crude form of density plot. Density plots do not depend on a choice of breakpoints. The choice of width and type of window, controlling the nature and amount of smoothing, does affect the appearance of the plot, making it more or less smooth. The following will give a density plot:\n\nplot(density(totlngth[here]),type=\"l\")\ndetach(possum)\n\nIn Figure 2.5 B the y-axis for the histogram is labelled so that the area of a rectangle is the density for that rectangle, i.e., the frequency for the rectangle is divided by the width of the rectangle. This gives a scale that is appropriate for superimposing a density curve estimate. The only difference between Figure 2.5 C and Figure 2.5 B is that a different choice of breakpoints is used for the histogram, so that the histogram gives a rather different impression of the distribution of the data. Code for Figure 2.5 B is:\n\nwith(possum, {\n  here <- sex == \"f\"\n  dens <- density(totlngth[here])\n  xlim <- range(dens$x)\n  ylim <- range(dens$y)\n  hist(totlngth[here], breaks = 72.5 + (0:5) * 5, probability = TRUE,\n      xlim = xlim, ylim = ylim, xlab=\"Total length\", main=\"\")\n  lines(dens)\n})\n\n\n\n2.6.2 Boxplots\nFigure 2.6 shows a boxplot of the female possum lengths:\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.6: Boxplot of female possum lengths, with additional labeling information\n\n\n\n\nCode for the boxplot, without the annotation, is\n\ntotlngth <- subset(DAAG::possum, sex==\"f\")$totlngth\nboxplot(totlngth[here], horizontal=TRUE, xlab=\"Total length (cm)\")\nrug(totlngth, side=1)   # Add a 'rug' that shows the location of points\n\nFigure 2.6 adds interpretative information. Also a ‘rug’, showing the location of individual data points, has been added.\n\n\n2.6.3 Normal probability plots\nType qqnorm(y) to obtain a normal probability plot of the elements of y.\nThe points of a normal probability plot will on average lie on a straight line if the distribution is Normal. In order to calibrate the eye to recognize plots that indicate non-normal variation, it is helpful to do several normal probability plots for random samples of the relevant size from a normal distribution. Look ahead to Figure 2.12, which uses the lattice function qqmath(), with a much more effective graph than is readily obtained using base graphics functions."
  },
  {
    "objectID": "graphs.html#other-useful-plotting-functions",
    "href": "graphs.html#other-useful-plotting-functions",
    "title": "2  Base R Graphics",
    "section": "2.7 Other Useful Plotting Functions",
    "text": "2.7 Other Useful Plotting Functions\nFor the functions demonstrated here, we use data on the heights of 100 female athletes.\n\n2.7.1 Scatterplot smoothing\nThe function panel.smooth() plots points, then adds a smooth curve through the points. As an example, consider the MASS:forbes data frame that has 17 observations of barometric pressure (bp, inches of mercury) and boiling point of water (bp, degrees Fahrenheit), at various locations in the Alps. Figure 2.7 plots the data, and adds a smooth curve. A dashed line has been added for comparison.\n\n\n\n\n\n\n\n\nFigure 2.7: Boiling point of water, versus measured barometric pressure\n\n\n\n\nThe parameter span, specifying the proportion of points that influence the smoothed value at each point, can be used to control the smoothness. The default is \\(\\frac{2}{3}\\).\n\n\n2.7.2 Adding lines to plots\nUse the function abline() for this. The parameters may be an intercept and slope, or a vector that holds the intercept and slope, or an lm object. Alternatively it is possible to draw a horizontal line (h = ), or a vertical line (v = ).\n\n\n\n\n\n\n\n\nFigure 2.8: Plots of brain weight (gm) against body weight (kg).\nPanel A makes it clear why a logarithmic or other power transformation is needed. Panel B adds both a least squares regression line and a robust regression line.\n\n\n\n\nFigure 2.8 shows plots of brain weight (gm) against body weight (kg), for 28 species of land animals. Logarithmic scales on both axes result in a meaningful plot, shown in Panel B. Two lines have been added – a least squares regression line (in black), and a robust regression line (in red).\nThe robust regression line makes a more sense than the least squares line. There are a number of animals with large body weight that have a larger brain weight than the red line would suggest.\nCode for Panel B is:\n\nAnimals.lm <- lm(log10(brain) ~ log10(body), data=Animals)\nAnimals.rlm <- MASS::rlm(log10(brain) ~ log10(body), data=Animals)\nabline(Animals.rlm, col=2)\n\n\n\n2.7.3 Dotcharts\nThese can be a good alternative to barcharts. They have a much higher information to ink ratio! Try\n\ndotchart(islands)  # vector of named numeric values, in the datasets package\n\nUnfortunately there are many names, and there is substantial overlap. Either enlarge the graph to occupy a large display screen, or plot with perhaps cex=0.5, so that the names can be distinguished."
  },
  {
    "objectID": "graphs.html#sec-lattice",
    "href": "graphs.html#sec-lattice",
    "title": "2  Base R Graphics",
    "section": "2.8 Lattice graphics",
    "text": "2.8 Lattice graphics\nLattice plots allow the use of the layout on the page to reflect meaningful aspects of data structure. The lattice package sits on top of the grid package. To use lattice graphics, both these packages must be installed.\nProviding it is installed, the grid package will be attached automatically when lattice is attached.\nThe lattice package has now to an extent been overtaken by the ggplot2 package, with its richer and more complex language syntax. A number of packages have been built on top of ggplot2, aimed at providing specific types of graph, that allow users to work around much of the complexity. Functions in lattice and ggplot2 return a graphics object, i.e., instructions for creating a plot, rather than a plot. This can be successively updated using the update() function, and/or new layers added, before using the generic print() (or plot() has the same effect) function or to produce a plot. Explicit use of print() or plot() is not required – all that is needed is to return type the object name on the command line, effectively sending its instructions to the command line.\nThe function name print emphasizes that the main part of the work required to produce a graph has already been done before the print() function is called – all that is then required is take the instructions that have been embedded in the graphics object and use them to show a graph on the screen or on the printed page. Readers who wish to pursue use of ggplot2 and of packages built on ggplot2 are referred to the extensive tutorial resources available on the web.\n\nExamples that Present Panels of Scatterplots – Using xyplot()\nThe basic function for creating panels of scatterplots is xyplot(). As an example, consider data, in the data frame DAAG::tinting, that are from an experiment that investigated the effects of tinting of car windows on visual recognition performance, primarily for visual recognition tasks that would be performed through side windows. Variables are csoa (critical stimulus onset asynchrony, i.e. the time in milliseconds required to recognize an alphanumeric target), it (inspection time, i.e., the time required for a simple discrimination task) and age, with ordered factors tint (level of tinting: no, lo, hi), target (contrast: locon, hicon), and agegp (1 = young, in the early 20s; 2 = an older participant, in the early 70s), and the factor sex (1 = male, 2 = female). Figure 2.9 shows the style of graph that one can get from xyplot(). The different symbols are different contrasts.\n\n\n\n\nLoading required package: lattice\n\n\n\n\nFigure 2.9: cap16\n\n\n\n\n\n## Code\nlibrary(latticeExtra)\nxyplot(csoa ~ it | sex * agegp, data = DAAG::tinting, \n                panel = panel.superpose, \n       groups = target, auto.key = list(columns = 2))\n\nAddition of the argument groups=target has the result that two different colors of symbol are used to distinguish between low contrast and high contrast targets. To use different symbols, add an argument in the style of par.settings=simpleTheme(pch=c(1,16)). Additionally, or instead, it would make sense to use lighter and darker colors for the two levels of contrast. Code is easier to follow if one starts with the graphics object needed for Figure 2.9 and then updates it:\n\ngph <- xyplot(csoa ~ it | sex * agegp, data = DAAG::tinting, \n              panel = panel.superpose, groups = target)\nupdate(gph, auto.key = list(columns = 2),\n       par.settings=simpleTheme(pch=16, col=c(\"lightblue3\",\"blue\")))\n  # As the result goes to the command line, it is `printed`\nupdate(gph, groups = target, auto.key = list(columns = 2),\n       par.settings=simpleTheme(pch=16, col=c(\"lightblue3\",\"blue\")))\n\nA striking feature is that the very high values, for both csoa and it, occur only for elderly males. It is apparent that the long response times for some of the elderly males occur, as we might have expected, with the low contrast target.\nFigure 2.10 puts smooth curves through the data, separately for the two target types:\n\n\n\n\nxyplot(csoa~it|sex*agegp, data=DAAG::tinting, groups=target, \n      type=c(\"p\",\"smooth\"))\n\n\n\n\nFigure 2.10: Smooth curves have been added to the plots, separately for the two types of target\n\n\n\n\nThe relationship between csoa and it seems much the same for both levels of contrast. Finally, we do a plot (Figure 2.11) that uses different symbols (in black and white) for different levels of tinting. The longest times are for the high level of tinting.\n\n\n\n\n\n\n\n\nFigure 2.11: Panels show ``csoa versus it, for each combination of females/males and elderly/young. The different levels of tinting (no, +=low, >=high) are shown with different symbols.\n\n\n\n\n\n## Code\nlattice::xyplot(csoa ~ it | sex * agegp, data = DAAG::tinting,\n       groups = tint, auto.key = list(columns = 3))\n\n\n\nPlotting columns in parallel\nUse the parameter outer to control whether the columns appear on the same or separate panels. If on the same panel, it is desirable to use auto.key to give a simple key. The following use the dataset grog from the DAAGxtras package:\n\nlibrary(DAAG)\nxyplot(Beer+Spirit+Wine ~ Year | Country, outer=TRUE,  \n        data=grog) \nxyplot(Beer+Spirit+Wine ~ Year, groups=Country, outer=TRUE,  \n        data=grog) \nxyplot(Beer+Spirit+Wine ~ Year | Country, outer=FALSE,  \n        data=grog, auto.key=list(columns=3), \n        par.settings=simpleTheme(pch=16, cex=2) )\n\nIn the final plot, note the use of simpleTheme() as a simple mechanism for controlling common parameter settings. Use of the parameter par.settings makes the change for the current plot only. Use trellis.par.set() to make the changes for the duration of the current device, unless reset.\n\n### Fixed, sliced and free scales\njobs <- DAAG::jobs\n## scale=\"fixed\" \nxyplot(BC+Alberta ~ Date, data=jobs, outer=TRUE) \n## scale=\"sliced\" - different slices of same scale \nxyplot(BC+Alberta ~ Date, data=jobs, outer=TRUE, \n       scales=list(y=list(relation=\"sliced\")) ) \n## scale=\"free\" - independent scales \nxyplot(BC+Alberta ~ Date, data=jobs, outer=TRUE, \n       scales=list(y=list(relation=\"free\")) )\n\n\n\nData, compared with simulated normal data\n\nIs the sample data consistent with random normal data?\n\n\n\n\n\n\n\n\nFigure 2.12: Normal probability plots. If data are from a normal distribution then points should fall, on average, along a line. The plot in the top left shows the 43 lengths of female possums. The other plots are for independent normal random samples of size 43.\n\n\n\n\nThere is one unusually small value. Otherwise the points for the female possum lengths are as close to a straight line as in many of the plots for random normal data. Code is:\n\n\n\nThe idea is an important one. In order to judge whether data are normally distributed, examine a number of randomly generated samples of the same size from a normal distribution. It is a way to train the eye, to give an idea of the extent to which departures from linearity may be explained as random variation The mean and standard deviation is best matched to that of the data that are under investigation.\nFor comparison, for readers who are interested, here is code that plots the same data using base graphics:\n\npar(mfrow=c(2,4), mgp=c(1.8,0.5,0), mar=c(3.1,3.6,2.6,1.6))             # A 2 by 4 layout of plots\ngps <- unique(dat$gp)\nfor (val in gps){\n  x <- subset(dat, gp==val)$y\nqqnorm(x, xlab=\"\", ylab=\"Length\", main=levs)  \n}\n\n\n\n\nAdding new layers\nThe latticeExtra package provides wide-ranging abilities for overlaying or underlaying an existing graphics object. Or, given an initial graphics object, a separate graphics object can be converted to a layer that is then added to the initial object.\nIn each instance, conditioning variables can be added. In most cases, a groups parameter can be specified, i.e., the plot is repeated for the groupings within the one panel. The data on athletes in the Australian Institute of Sport, in the dataset DAAG::ais will be used for the example that now follows. The following show haemoglobin count versus red blood cell count, distinguished by sport within panel, with separate panels for females and males:\n\nlibrary(latticeExtra)\naisBS <- subset(DAAG::ais, sport %in% c(\"B_Ball\", \"Swim\"))\nbasic1 <-  xyplot(hc ~ rcc | sex, groups=sport[drop=TRUE],  data=aisBS)\nbasic2 <- update(basic1,\n      par.settings=simpleTheme(pch = c(1,3), lty=1:2, lwd=1.5),\n        # Plot characters 1 and 3 distinguish the groups\n      strip=strip.custom(factor.levels=c(\"Female\",\"Male\")),\n        # Label the panels \"Female\" and \"Male\", not \"f\" and \"m\"\n      scales=list(tck=0.5), auto.key=list(columns=2))\n\nPrinting the object basic1 would give a very basic plot. The object basic2 sets separate line types for males and females, creates thicker lines, specefies strip level names that are different from the group level names, reduces the length of the axis ticks, and adds a key.\nThe xyplot() function has provision for the addition of separate lines for the two sports, but not for the parallel lines that are preferred. The following creates a new layer. The values x and y will be taken from the data used for the object basic1, when the new layer is added. Code for the new layer is:\n\nlayer2 <- layer(parallel.fit <-\n                    fitted(lm(y ~ groups[subscripts] + x)),\n                panel.superpose(x, parallel.fit, type = \"r\", ...))\n\nThe following prints the graph:\n\n\n\n\nbasic2+layer2\n\n\n\n\nFigure 2.13: paste(cap20)\n\n\n\n\n### An incomplete list of _lattice_ Functions  {-}\nsplom( ~ data.frame)                    # Scatterplot matrix\nbwplot(factor ~ numeric , . .)          # Box and whisker plot\nqqnorm(numeric , . .)           # normal probability plots\ndotplot(factor ~ numeric , . .)         # 1-dim. Display\nstripplot(factor ~ numeric , . .)       # 1-dim. Display\nbarchart(character ~ numeric , . .)\nhistogram( ~ numeric , . .)\ndensityplot( ~ numeric , . .)           # Smoothed version of histogram\nqqmath(numeric ~ numeric , . .)         # QQ plot\nsplom( ~ dataframe, . .)            # Scatterplot matrix  \nparallel( ~ dataframe, . .)     # Parallel coordinate plots\ncloud(numeric ~ numeric * numeric, . .)           # 3-D plot\ncontourplot(numeric ~ numeric * numeric, . .)   # Contour plot\nlevelplot(numeric ~ numeric * numeric, . .)       # Contour plot variant"
  },
  {
    "objectID": "graphs.html#using-mathematical-expressions-in-plots",
    "href": "graphs.html#using-mathematical-expressions-in-plots",
    "title": "2  Base R Graphics",
    "section": "2.9 Using mathematical expressions in plots",
    "text": "2.9 Using mathematical expressions in plots\nThe following is a simple example. For this purpose, an expression has a much extended syntax, relative to that for a mathematical expression. See ?plotmath.\n`\n\n\n\n\n\n\n\n\nFigure 2.14: Examples where one or both axis labels are mathematical expressions,\n\n\n\n\n\npar(mgp=c(1.8,0.5,0), mfrow=c(1,2))\nr <- 1:15\ny <- pi*r^2\nplot(r, y, xlab=expression(\"Radius = \"*r), ylab=expression(Area == pi*~r^~2))\ntitle(main=expression(\"A: Plot of Area = \"*pi*~r^2 *\" vs Radius = \"*r))\ncurve(log(x/(1-x)), from=0.01, to=0.99, xlab=expression(pi),\n      ylab=expression(log(frac(pi,1-pi))))\ntitle(main=expression(\"B: Plot of \"*log(frac(pi,1-pi))*\" against \"*pi))\n\nFor lattice and ggplot2 graphics as well as base graphics, expressions of the type shown can appear anywhere in place of a character vector.\nNotice that in ylab = expression(Area == pi*r^2), there is a double equals sign (==), although what will appear on the plot has a single equals sign. See ?plotmath for detailed information on the plotting of mathematical expressions. Notice that * juxtaposes both text and mathematical symbols. Notice also that ~ been used to insert space before \\(r\\) and phantom(,) between \\(r\\) and the superscript 2. The final plot from demo(graphics) demonstrates some of the possibilities for plotting mathematical symbols."
  },
  {
    "objectID": "graphs.html#guidelines-for-graphs",
    "href": "graphs.html#guidelines-for-graphs",
    "title": "2  Base R Graphics",
    "section": "2.10 Guidelines for Graphs",
    "text": "2.10 Guidelines for Graphs\nDesign graphs to make their point tersely and clearly, with a minimum waste of ink. Label as necessary to identify important features. In scatterplots the graph should attract the eye’s attention to the points that are plotted, and to important grouping in the data. Use solid points, large enough to stand out relative to other features, when there is little or no overlap.\nWhen there is extensive overlap of plotting symbols, use open plotting symbols. Where points are dense, overlapping points will give a high ink density, which is exactly what one wants.\nUse scatterplots in preference to bar or related graphs whenever the horizontal axis represents a quantitative effect.\nUse graphs from which information can be read directly and easily in preference to those that rely on visual impression and perspective. Thus in scientific papers contour plots are much preferable to surface plots or two-dimensional bar graphs.\nDraw graphs so that reduction and reproduction will not interfere with visual clarity.\nExplain clearly how error bars should be interpreted — SE limits, 95% confidence interval, 2 SD limits, or whatever. Explain what source of error(s) is represented.\nIt is pointless to present information on a source of error that is of little or no interest, for example analytical error when the relevant source of `error’ for comparison of treatments is between fruit.\nUse colour or different plotting symbols to distinguish different groups. Take care to use colours that contrast."
  },
  {
    "objectID": "graphs.html#exercises",
    "href": "graphs.html#exercises",
    "title": "2  Base R Graphics",
    "section": "2.11 Exercises",
    "text": "2.11 Exercises\n\nThe data set huron that accompanies these notes has mean July average water surface elevations, in feet, IGLD (1955) for Harbor Beach, Michigan, on Lake Huron, Station 5014, for 1860-1986 . (Alternatively work with the vector LakeHuron from the datasets package, that has mean heights for 1875-1972 only.)\n\n\n\nPlot mean.height against year.\n\nUse the identify function to label points for the years that correspond to the lowest and highest mean levels. That is, type\n\n\n  identify(huron$year,huron$mean.height,labels=huron$year)\n\nand use the left mouse button to click on the lowest point and highest point on the plot. To quit, press (depending on the operating system) a mouse button other than the left, or press ESC.\nc) As in the case of many time series, the mean levels are correlated from year to year. To see how each year’s mean level is related to the previous year’s mean level, use\n\nlag.plot(huron$mean.height)\n\nThis plots the mean level at year i against the mean level at year i-1.\n\nPlot the graph of log(brain weight) versus log(body weight), for the data set Animals from the MASS package. Use the row labels to label the points corresponding to the three clear outliers.\nCheck the distributions of head lengths (hdlngth) in the possum data set that accompanies these notes. Compare the following forms of display:\n\n\na histogram (hist(possum$hdlngth));\n\na stem and leaf plot (stem(possum$hdlngth));\na normal probability plot (qqnorm(possum$hdlngth)); and\na density plot (plot(density(possum$hdlngth))). What are the advantages and disadvantages of these different forms of display?\n\n\nTry x <- rnorm(10). Print out the numbers that you get. Look up the help for rnorm. Now generate a sample of size 10 from a normal distribution with mean 170 and standard deviation 4.\nUse mfrow() to set up the layout for a 3 by 4 array of plots. In the top 4 rows, show normal probability plots for four separate `random’ samples of size 10, all from a normal distribution. In the middle 4 rows, display plots for samples of size 100. In the bottom four rows, display plots for samples of size 1000. Comment on how the appearance of the plots changes as the sample size changes.\nThe function runif() generates a sample from a uniform distribution, by default on the interval 0 to 1. Try x <- runif(10), and print out the numbers you get. Then repeat exercise 6 above, but taking samples from a uniform distribution rather than from a normal distribution. What shape do the points follow?\n\n*7. If you find exercise 6 interesting, you might like to try it for some further distributions. For example x <- rchisq(10,1) will generate 10 random values from a chi-squared distribution with degrees of freedom 1. The statement x <- rt(10,1) will generate 10 random values from a \\(t\\) distribution with degrees of freedom one. Make normal probability plots for samples of various sizes from these distributions.\n\nFor the first two columns of the data frame hills, examine the distribution using: (a) histogram; (b) density plots; (c) normal probability plots.\nRepeat (a), (b) and (c), now working with the logarithms of the data values.\nThis and remaining exercises ask for the use of lattice functions. The following data gives milk volume (g/day) for smoking and nonsmoking mothers :\n\n  Smoking Mothers:    621, 793,  593,  545, 753, 655, 895, 767, 714, 598, 693\n  Nonsmoking Mothers: 947, 945, 1086, 1202, 973, 981, 930, 745, 903, 899, 961\nPresent the data (i) in side by side boxplots (use bwplot()); (ii) using a dotplot form of display (use dotplot()).\n\nFor the possum data set, use lattice functions to generate the following plots:\n\n\n\nhistograms of hdlngth – use histogram();\n\nnormal probability plots of hdlngth – use qqmath();\n\ndensity plots of hdlngth – use densityplot(). Investigate the effect of varying the density bandwidth (bw).\n\n\nThe following exercises, all using lattice functions, relate to the data frame DAAG::possum:\n\n\n\nUsing xyplot(), explore the relation between hdlngth and totlngth, taking into account sex and Pop.\n\nConstruct a contour plot of chest versus belly and totlngth – use levelplot() or contourplot().\n\nConstruct box and whisker plots for hdlngth, using site as a factor.\n\nUse qqmath() to construct normal probability plots for hdlgth, for each separate level of sex and Pop. Does it appear that the distribution of hdlgth varies with the level of these other factors.\n\n\nThe dataframe airquality (datasets package) has columns Ozone, Solar.R, Wind, Temp, Month and Day. Use xyplot() to plot Ozone against Solar.R for each of the three temperature ranges, and for each of three wind ranges."
  },
  {
    "objectID": "graphs.html#references-and-reading",
    "href": "graphs.html#references-and-reading",
    "title": "2  Base R Graphics",
    "section": "2.12 References and reading",
    "text": "2.12 References and reading\nThe web page https://www.eecs.yorku.ca/~papaggel/courses/eecs6414/index.html\nhas links to many different collections of information on statistical graphics.\nChang (2013) . R graphics cookbook. O’Reilly.\nMurrell (2011) . R Graphics. Chapman and Hall/CRC.\n\n\n\n\nChang, Winston. 2013. R Graphics Cookbook. 1st ed. O’Reilly.\n\n\nMurrell, Paul. 2011. R Graphics. 2nd ed. Chapman; Hall/CRC. http://www.e-reading.org.ua/bookreader.php/137370/Murrell_-_R_Graphics.pdf."
  },
  {
    "objectID": "reg.html#linear-model-objects",
    "href": "reg.html#linear-model-objects",
    "title": "3  Multiple Linear Regression",
    "section": "3.1 Linear model objects",
    "text": "3.1 Linear model objects\nWe begin with the straight line regression example that appeared earlier, in Section 1.1. Look ahead to Figure 3.1, where a regression line has been added.\nThe code for the regression calculation is:\n\nelastic.lm <- lm(distance ~ stretch, data=DAAG::elasticband)\n\nThe object distance ~ stretch is a model formula, used to specify details of the model that is to be fitted. Other model formulae will appear in the course of this chapter.\n\n\n\n\nelasticband <- DAAG::elasticband\nplot(distance ~ stretch, data=DAAG::elasticband)\nabline(elastic.lm)\n\n\n\n\nFigure 3.1: Plot of distance versus stretch for the elastic band data, with fitted least squares line\n\n\n\n\nThe output from the regression is an lm object, which we have called elastic.lm. Now examine a summary of the regression results. Notice that the output documents the model formula that was used:\n\nsummary(elastic.lm, digits=4)\n\nCall:\nlm(formula = distance ~ stretch, data = DAAG::elasticband)\n\nResiduals:\n       1        2        3        4        5        6        7 \n  2.1071  -0.3214  18.0000   1.8929 -27.7857  13.3214  -7.2143 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)  -63.571     74.332  -0.855   0.4315\nstretch        4.554      1.543   2.951   0.0319\n\nResidual standard error: 16.33 on 5 degrees of freedom\nMultiple R-squared:  0.6352,    Adjusted R-squared:  0.5622 \nF-statistic: 8.706 on 1 and 5 DF,  p-value: 0.03186\n\nAn lm object is a list of named elements. Names of elastic.lm are:\n\nnames(elastic.lm)\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\nA number of functions are available for extracting information that might be required from the list. This is or most purposes preferable to extracting elements from the list directly. Examples are:\n\ncoef(elastic.lm)\n(Intercept)     stretch \n -63.571429    4.553571 \n\nThe function most often used to inspect regression output is summary(). It extracts the information that users are most likely to want. In section 5.1, we had\n\nsummary(elastic.lm)\n\nThere is a plot method for lm objects that gives the diagnostic information shown in Figure 3.2.\n\n\n\n\n\n\n\n\nFigure 3.2: Diagnostic plot of lm object, obtained by plot(elastic.lm).\n\n\n\n\nTo get Figure 3.2, type:\n\nopar <- par(mar=c(4.6,3.6,2.1,2.1), mgp=c(2,.5,0), mfrow=c(1,4)) \nplot(elastic.lm)\npar(opar)\n\nBy default the first, second and fourth plot use the row names to identify the three most extreme residuals. [If explicit row names are not given for the data frame, then the row numbers are used.]"
  },
  {
    "objectID": "reg.html#model-formulae-and-the-x-matrix",
    "href": "reg.html#model-formulae-and-the-x-matrix",
    "title": "3  Multiple Linear Regression",
    "section": "3.2 Model Formulae, and the X Matrix",
    "text": "3.2 Model Formulae, and the X Matrix\nThe model formula for the elastic band example was distance ~ stretch . The model formula is a recipe for setting up the calculations. All the calculations described in this chapter involve the use of an model matrix or X matrix, and a vector y of values of the dependent variable. For some of the examples we discuss later, it helps to know what the X matrix looks like. Details for the elastic band example follow. The X matrix, with the y-vector alongside, is:\n\ncbind(model.matrix(elastic.lm), distance=elasticband$distance)\n  (Intercept) stretch distance\n1           1      46      148\n2           1      54      182\n3           1      48      173\n4           1      50      166\n5           1      44      109\n6           1      42      141\n7           1      52      166\n\nThe model matrix relates to the part of the model that appears to the right of the equals sign. The straight line model is \\(y = a + b x + \\mbox{residual}\\), which we write as \\[ y = a \\times 1 + b \\times x + \\mbox{residual}\\] The following are the fitted values and residuals that we get with the estimates of a (= -63.6) and b ( = 4.55) from the least squares regression\n\n\n\n\n\n\n\n\n\n\n1\nStretch (mm)\n(Fitted)\n(Observed)\n(Residual)\n\n\n\n\n\\(-63.6 \\times\\)\n\\(4.55 \\times\\)\n\\(-63.6 + 4.55 \\times \\mbox{Stretch}\\)\n\\(\\mbox{Distance (mm)}\\)\n\\(\\mbox{Observed – Fitted}\\)\n\n\n1\n46\n\\(-63.6 + 4.55 \\times 46 = 145.7\\)\n148\n148-145.7 = 2.3\n\n\n1\n54\n\\(-63.6 + 4.55 \\times 54 = 182.1\\)\n182\n182-182.1 = -0.1\n\n\n1\n48\n\\(-63.6 + 4.55 \\times 48 = 154.8\\)\n173\n173-154.8 = 18.2\n\n\n1\n50\n\\(-63.6 + 4.55 \\times 50 = 163.9\\)\n166\n166-163.9 = 2.1\n\n\n…\n…\n…\n…\n…\n\n\n\nThe symbol \\(\\hat{y}\\) [pronounced y-hat] is commonly used fot fitted values of the variable \\(y\\).\nWe might alternatively fit the simpler (no intercept) model. For this we have \\[\ny = x \\times b + e\n\\] where \\(e\\) is a random variable with mean 0. The X matrix then consists of a single column, holding the values of x.\n\n3.2.1 Model formulae more generally\nModel formulae take forms such as:\ny ~ x+z             # Fit y as a linear combination of x and z\ny~x + fac + fac:x \n  # If fac is a factor and x is a variable, fac:x allows \n  # a different slope for each different level of fac.\nModel formulae are widely used to set up most of the model calculations in R.\nNotice the similarity between model formulae and the formulae that can be used for specifying plots. Thus, recall the graph formula for a coplot that gives a plot of y against x for each different combination of levels of fac1 (across the page) and fac2 (up the page) is:\ny ~ x | fac1+fac2\n\n\n3.2.2 *Manipulating Model Formulae\nModel formulae can be assigned, e.g. \nformyxz <- formula(y~x+z)\nor\nformyxz <- formula(“y~x+z”)\nThe argument to formula() can, as just demonstrated, be a text string. This makes it straightforward to paste the argument together from components that are stored in text strings. For example\n\nnames(elasticband)\n[1] \"stretch\"  \"distance\"\n\n\nnam <- names(elasticband)\nformds <- formula(paste(nam[1],\"~\",nam[2]))\nlm(formds, data=elasticband)\n\nCall:\nlm(formula = formds, data = elasticband)\n\nCoefficients:\n(Intercept)     distance  \n    26.3780       0.1395  \n\nNote that graphics formulae can be manipulated in exactly the same way as model formulae."
  },
  {
    "objectID": "reg.html#multiple-linear-regression-models",
    "href": "reg.html#multiple-linear-regression-models",
    "title": "3  Multiple Linear Regression",
    "section": "3.3 Multiple Linear Regression Models",
    "text": "3.3 Multiple Linear Regression Models\n\n3.3.1 The data frame Rubber\nThe data set MASS::Rubber is from the accelerated testing of tyre rubber. The variables are loss (the abrasion loss in gm/hr), hard (hardness in ‘Shore’ units), and tens (tensile strength in kg/sq m).1 Figure 3.3 shows a scatterplot matrix for the variables:\n\n\n\n\n\n\n\n\nFigure 3.3: Scatterplot matrix for the data frame MASS::Rubber\n\n\n\n\nCode is:\n\npairs(MASS::Rubber)\n\nThere is a negative correlation between loss and hard.\nWe proceed to regress loss on hard and tens.\n\nRubber.lm <- lm(loss~hard+tens, data=MASS::Rubber)\nsummary(Rubber.lm, digits=3)\n\nCall:\nlm(formula = loss ~ hard + tens, data = MASS::Rubber)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-79.385 -14.608   3.816  19.755  65.981 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept) 885.1611    61.7516  14.334 3.84e-14\nhard         -6.5708     0.5832 -11.267 1.03e-11\ntens         -1.3743     0.1943  -7.073 1.32e-07\n\nResidual standard error: 36.49 on 27 degrees of freedom\nMultiple R-squared:  0.8402,    Adjusted R-squared:  0.8284 \nF-statistic:    71 on 2 and 27 DF,  p-value: 1.767e-11\n\nIn addition to the use of plot() with lm objects, note the use of termplot().\n\n\n\n\npar(mfrow=c(1,2))\ntermplot(Rubber.lm, partial=TRUE, smooth=panel.smooth)\n\n\n\n\nFigure 3.4: Plot, obtained with termplot(), showing the contribution of each of the two terms in the model, at the mean of the contributions for the other term. A smooth curve has, in each panel, been fitted through the partial residuals. There is a clear suggestion that, at the upper end of the range, the response is not linear with tensile strength.\n\n\n\n\nFigure @ref(fig:fig24) used the following code:\n\npar(mfrow=c(1,2))\ntermplot(Rubber.lm, partial=TRUE, smooth=panel.smooth)\n\nThis plot raises interesting questions.\n\n\n3.3.2 Weights of Books\nThe books to which the data in the data set DAAG::oddbooks refer were chosen to cover a wide range of weight to height ratios. The use of the data to fit regression models illustrates how biases that affect the collection of observational data can skew results.\n\n\n\n\n\n\n\n\nFigure 3.5: Scatterplot matrix for the data frame DAAG::oddbooks\n\n\n\n\nCode is:\n\npairs(DAAG::oddbooks)\n\nThe correlations between thick, height and width are so strong that if one tries to use more than one of them as a explanatory variables, the coefficients are ill-determined. They contain very similar information, as is evident from the scatterplot matrix. The regressions on height and width give plausible results, while the coefficient of the regression on thick is entirely an artefact of the way that the books were selected.\nThe design of the data collection really is important for the interpretation of coefficients from a regression equation. Even where regression equations from observational data appear to work well for predictive purposes, the individual coefficients may be misleading. This is more than an academic issue, as the analyses in Lalonde (1986) demonstrate. They had data from experimental “treatment” and “control” groups, and also from two comparable non-experimental “controls”. The regression estimate of the treatment effect, when comparison was with one of the non-experimental controls, was statistically significant but with the wrong sign! The regression should be fitted only to that part of the data where values of the covariates overlap substantially. Dehejia and Wahba demonstrate the use of scores (“propensities”) to identify subsets that are defensibly comparable. The propensity is then the only covariate in the equation that estimates the treatment effect. It is impossible to be sure that any method is giving the right answer.\nAssuming a uniform density, the geometry suggests \\[\n\\mbox{weight} = \\mbox{density} \\times \\mbox{thick}  \\times \\mbox{height}  \\times \\mbox{breadth}\n\\] On a logarithmic scale, this transforms to \\[\n\\begin{aligned}\n\\log(\\mbox{weight}) &= \\log(\\mbox{density}) + \\log(\\mbox{thick})  + \\log(\\mbox{height})  + \\log(\\mbox{breadth})\\\\\n&= \\log(\\mbox{density}) + \\log(\\mbox{volume})\n\\end{aligned}\n\\]\nThe following ignores what the geometry suggests\n\nlogbooks <- log(DAAG::oddbooks) # We might expect weight to be\n                                # proportional to thick * height * width\n## Regress log(weight) on log(thick) + log(height) + log(breadth)\nlogbooks.lm3<-lm(weight~thick+height+breadth,data=logbooks)\n  # NB: `weight` is really `log(weight)`,\n  # and similarly for other variables\ncat(capture.output(summary(logbooks.lm3, digits=2))[-(1:8)], sep='\\n')\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)  -0.7191     3.2162  -0.224    0.829\nthick         0.4648     0.4344   1.070    0.316\nheight        0.1537     1.2734   0.121    0.907\nbreadth       1.8772     1.0696   1.755    0.117\n\nResidual standard error: 0.1611 on 8 degrees of freedom\nMultiple R-squared:  0.8978,    Adjusted R-squared:  0.8595 \nF-statistic: 23.43 on 3 and 8 DF,  p-value: 0.000257\n\nNotice that all the coefficients are at the level of statistical error, but even so the overall fit (\\(p\\)=0.000257) is clearly good.\nNow regress on \\(\\mbox{logVolume}\\) = \\(\\log(\\mbox{volume})\\)\n\n## Regress log(weight) on log(thick) + log(height) + log(breadth)\nlogVolume <- with(logbooks, thick+height+breadth)\nlogbooks.lm <- lm(weight~logVolume, data=logbooks)\ncat(capture.output(summary(logbooks.lm, digits=2))[-(1:8)], sep='\\n')\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   -8.942      2.731  -3.274  0.00837\nlogVolume      1.696      0.305   5.562  0.00024\n\nResidual standard error: 0.2228 on 10 degrees of freedom\nMultiple R-squared:  0.7557,    Adjusted R-squared:  0.7313 \nF-statistic: 30.94 on 1 and 10 DF,  p-value: 0.00024\n\nThe model still does relatively well on the data used. Note however that the coefficient of logVolume differs from the expected 1.0 by 2.28 standard errors.\nFigure @ref(fig:fig25) made it clear that all three variables are highly correlated. We now try adding each one in turn to the model regressed \\(\\log(\\mbox{weight})\\) on \\(\\mbox{logVolume}\\).\n\nprint(add1(logbooks.lm, scope=~.+thick+height+breadth, test='F'), digits=4)\nSingle term additions\n\nModel:\nweight ~ logVolume\n        Df Sum of Sq    RSS    AIC F value  Pr(>F)\n<none>               0.4966 -34.22                \nthick    1    0.2739 0.2227 -41.84  11.072 0.00884\nheight   1    0.2437 0.2528 -40.32   8.677 0.01634\nbreadth  1    0.2872 0.2094 -42.58  12.347 0.00658\nprint(add1(logbooks.lm, scope=~.+thick+height+breadth, test='F'), digits=3)\nSingle term additions\n\nModel:\nweight ~ logVolume\n        Df Sum of Sq   RSS   AIC F value Pr(>F)\n<none>               0.497 -34.2               \nthick    1     0.274 0.223 -41.8   11.07 0.0088\nheight   1     0.244 0.253 -40.3    8.68 0.0163\nbreadth  1     0.287 0.209 -42.6   12.35 0.0066\n\nThe smallest value of the AIC statistic is preferred, though given the small degrees of freedom for the residual, not too much can be made of the magnitude of the reduction in AIC. The preferred model (which is also the model that gives the smallest Pr(>F)) is then:\n\naddBreadth.lm <- update(logbooks.lm, formula=.~.+breadth)\ncat(capture.output(summary(addBreadth.lm, digits=2))[-(1:8)], sep='\\n')\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)  -0.9192     2.9508  -0.311  0.76252\nlogVolume     0.4564     0.4100   1.113  0.29452\nbreadth       1.1562     0.3291   3.514  0.00658\n\nResidual standard error: 0.1525 on 9 degrees of freedom\nMultiple R-squared:  0.897, Adjusted R-squared:  0.8741 \nF-statistic: 39.19 on 2 and 9 DF,  p-value: 3.611e-05\n\nOnce account has been taken of breadth, volume does not make any clearly useful contribution to predicting weight. This is a result that has no, or very limited, applicability outside of the circumstances that generated this dataset."
  },
  {
    "objectID": "reg.html#polynomial-and-spline-regression",
    "href": "reg.html#polynomial-and-spline-regression",
    "title": "3  Multiple Linear Regression",
    "section": "3.4 Polynomial and Spline Regression",
    "text": "3.4 Polynomial and Spline Regression\nLinear regression is linear in the explanatory terms that are supplied. These can include, for example polynomial terms. Note that polynomial curves of high degree are in general unsatisfactory. Spline curves, constructed by joining low order polynomial curves (typically cubics) in such a way that the slope changes smoothly, are in general preferable.\nThe data frame DAAG::seedrates gives, for each of a number of different seeding rates, the number of barley grain per head.\n\n\n\n\n\n\n\n\nFigure 3.6: Number of grain per head versus seeding rate, for the barley seeding rate data, with fitted quadratic curve.\n\n\n\n\nCode is:\n\nplot(grain ~ rate, data=DAAG::seedrates)   # Plot the data\nseedrates.lm2 <- lm(grain ~ rate+I(rate^2), data=DAAG::seedrates)\nwith(data=DAAG::seedrates, lines(lowess(rate, fitted(seedrates.lm2))))\n\n\n3.4.1 Spline Terms in Linear Models\nThe fitting of polynomial functions was a simple example of the use of linear models to fit terms that may be nonlinear functions of one or more of the variables. Spline functions variables extend this idea further. The splines considered here are constructed by joining together cubic curves, in such a way the joins are smooth. The places where the cubics join are known as `knots’. It turns out that, once the knots are fixed, and depending on the class of spline curves that are used, spline functions of a variable can be constructed as linear combinations of basis functions, where each basis function is a transformation of the variable.\nThe dataset cars, from the _datasets+ package, gives stopping distance (dist, in ft) versus speed (mph), for cars in the 1920s.\n\n\n\n\nLoading required package: nlme\nThis is mgcv 1.9-0. For overview type 'help(\"mgcv-package\")'.\n\n\n\n\nFigure 3.7: Stopping distance (dist) versus speed, for cars from the 1920s.\n\n\n\n\nCode is\n\npar(mfrow=c(1,2))\nlibrary(mgcv)\ncars3.gam <- gam(dist ~ s(speed, k=3, fx=T, bs=\"cr\"), data=cars)\n  # k=3 includes 1 degree of freedom for the intercept.\nplot(cars3.gam, residuals=T, pch=1, shift=mean(predict(cars3.gam)),\n     ylab=\"Stopping distance\")\ntitle(main=\"A: Regression spline smooth -- 3 df\")\n## Fit with automatic choice of smoothing parameter\ncars.gam <- gam(dist ~ s(speed, k=10), data=cars)  # k=10 is default\nplot(cars.gam, residuals=T, pch=1, shift=mean(predict(cars.gam)),\n     shade=T)\nlines(cars$speed, fitted(cars3.gam), col='red', lty=2)\ntitle(main=\"B: Penalized spline smooth\")\nlegend('topleft', lty=2, col='red', legend=\"Regression spline fit from Panel A\", bty=\"n\")\n\nThe Panel A choice of 3 degrees of freedom for a regression spline smooth is ad hoc. Better than such an ad hoc smooth is the penalized spline approach, which adds a penalty term that reflects model complexity to the residual sum of squares that is to be minimized. The name GAM (Generalized Additive Model) is used to refer to both types of model.\nThe Panel A regression spline smooth could alternatively be fitted as an lm style linear model. Fitting using the function mgcv::gam(), and specifying k=3 and fx=T to obtain a regression spline fixed degrees of freedom fit, has the advantage that the function plot.gam() can then be used to obtain a graph that shows 2 standard error bounds."
  },
  {
    "objectID": "reg.html#sec-lines2",
    "href": "reg.html#sec-lines2",
    "title": "3  Multiple Linear Regression",
    "section": "3.5 Using Factors in R Models",
    "text": "3.5 Using Factors in R Models\nFactors are crucial for specifying R models that include categorical or factor variables. Consider data from an experiment that compared houses with and without cavity insulation . While one would not usually handle these calculations using an lm model, it makes a simple example to illustrate the choi,ce of a baseline level, and a set of contrasts. Different choices, although they fit equivalent models, give output in which some of the numbers are different and must be interpreted differently.\nWe begin by entering the data from the command line:\n\ninsulation <- factor(c(rep(\"without\", 8), rep(\"with\", 7)))\n# 8 without, then 7 with\n        # `with’ precedes `without’ in alphanumeric order, & is the baseline\nkWh <- c(10225, 10689, 14683, 6584, 8541, 12086, 12467, \n    12669, 9708, 6700, 4307, 10315, 8017, 8162, 8022)\n\nTo formulate this as a regression model, we take kWh as the dependent variable, and the factor insulation as the explanatory variable.\n\ninsulation <- factor(c(rep(\"without\", 8), rep(\"with\", 7)))\n# 8 without, then 7 with\nkWh <- c(10225, 10689, 14683, 6584, 8541, 12086, 12467, \n         12669, 9708, 6700, 4307, 10315, 8017, 8162, 8022)\ninsulation.lm <- lm(kWh ~ insulation)\nsummary(insulation.lm, corr=F)\n\nCall:\nlm(formula = kWh ~ insulation)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4409.0  -979.1   131.9  1575.0  3690.0 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)\n(Intercept)         7890.1      873.8   9.030  5.8e-07\ninsulationwithout   3102.9     1196.4   2.593   0.0223\n\nResidual standard error: 2312 on 13 degrees of freedom\nMultiple R-squared:  0.341, Adjusted R-squared:  0.2903 \nF-statistic: 6.726 on 1 and 13 DF,  p-value: 0.02228\n\nThe \\(p\\)-value is 0.022, which may be taken as weak evidence that we can distinguish between the two types of houses. The factor levels are by default taken in alphabetical order, with the initial level as the baseline. Thus, with comes before without, and is the baseline. Hence:\nAverage for Insulated Houses = 7980\nEstimate for uninsulated houses = 7980 + 3103 = 10993\nStandard error of difference = 1196.\nIt often helps to keep in mind the model matrix or X matrix.\nHere are the X and the y that are used for the calculations.\nNote that the first eight data values were all withouts:\n\n\n\n7980\n3103\nAdd to get\nCompare with\nResidual\n\n\n\n\n1\n1\n7980+3103=10993\n10225\n10225-10993\n\n\n1\n1\n7980+3103=10993\n10689\n10689-10993\n\n\n. . .\n. . .\n. . .\n. . .\n. . . .\n\n\n1\n0\n7980+0\n9708\n9708-7980\n\n\n1\n0\n7980+0\n6700\n6700-7980\n\n\n\nType\n\nmodel.matrix(kWh ~ insulation)\n   (Intercept) insulationwithout\n1            1                 1\n2            1                 1\n3            1                 1\n4            1                 1\n5            1                 1\n6            1                 1\n7            1                 1\n8            1                 1\n9            1                 0\n10           1                 0\n11           1                 0\n12           1                 0\n13           1                 0\n14           1                 0\n15           1                 0\nattr(,\"assign\")\n[1] 0 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$insulation\n[1] \"contr.treatment\"\n\nand check that it gives the above model matrix.\n\n3.5.1 Other Choices of Contrasts\nThere are other ways to set up the X matrix that use other choices of contrasts. One obvious alternative is to make without the first factor level, so that it becomes the baseline. For this, specify:\n\ninsulation <- relevel(insulation, ref=\"without\")      \n  # Make `without’ the baseline\n\nAnother possibility is the sum contrasts.\nWith the sum contrasts the baseline is the mean over all factor levels. The effect for the first level is omitted; the user has to calculate it as minus the sum of the remaining effects. Here is the output from use of the `sum’ contrasts :\n\noptions(contrasts = c(\"contr.sum\", \"contr.poly\"), digits = 2)    \n   #  Try the `sum’ contrasts\ninsulation <- factor(insulation, levels=c(\"without\", \"with\")) \ninsulation.lm <- lm(kWh ~ insulation)\nsummary(insulation.lm, corr=F)\n\nCall:\nlm(formula = kWh ~ insulation)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -4409   -979    132   1575   3690 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)     9442        598   15.78  7.4e-10\ninsulation1     1551        598    2.59    0.022\n\nResidual standard error: 2310 on 13 degrees of freedom\nMultiple R-squared:  0.341, Adjusted R-squared:  0.29 \nF-statistic: 6.73 on 1 and 13 DF,  p-value: 0.0223\n\nHere is the interpretation:\nAverage of (mean for “without”, “mean for with”) = 9442\nEstimate for uninsulated houses (the first level) = 9442 + 1551 = 10993 As effects sum to one, the effect for the 2\\(^{nd}\\) level (`with’) is -1551.\nThus the estimate for insulated houses (1\\(^{st}\\) level) = 9442 - 1551 = 7980.\nThe sum contrasts are sometimes called “analysis of variance” contrasts. It is possible to set the choice of contrasts for each factor separately, with a statement such as:\ninsulation <- C(insulation, contr=treatment)\nAlso available are the helmert contrasts. These are not at all intuitive and rarely helpful, even though S-PLUS uses them as the default. Novices should avoid them ."
  },
  {
    "objectID": "reg.html#multiple-lines-different-regression-lines-for-different-species",
    "href": "reg.html#multiple-lines-different-regression-lines-for-different-species",
    "title": "3  Multiple Linear Regression",
    "section": "3.6 Multiple Lines – Different Regression Lines for Different Species",
    "text": "3.6 Multiple Lines – Different Regression Lines for Different Species\nThe terms that appear on the right of the model formula may be variables or factors, or interactions between variables and factors, or interactions between factors. Here we take advantage of this to fit different lines to different subsets of the data.\nIn the example that follows, we have weights for a porpoise species (Stellena styx) and for a dolphin species (Delphinus delphis).\n\ndolphins <- data.frame(\n  weight = c(35, 42, 71, 65, 63, 64, 45, 54, 59, 50, \n42, 55, 37, 47, 40, 52), \n  heart = c(245, 255, 525, 425, 425, 440, \n350, 300, 350, 320, 240, 305, 220, 310, 210, 350),\nspecies = rep(c(\"styx\", \"delph\"), c(7,9))\n)\n\n\n\n\nFigure 3.8 shows a plot of the data, with separate lines fitted for the two species:\n\nlibrary(lattice)\nxyplot(heart ~ weight, groups=species, auto.key=list(columns=2), data=dolphins,\n       par.settings=simpleTheme(pch=c(16,17)), type=c(\"p\",\"r\"), scales=list(log=T))\n\n\n\n\nFigure 3.8: Height weight versus body weight, with lobgarithmic scales on both axes. Separate lines are fitted for the two dolphin species.\n\n\n\n\nWe take x1 to be a variable that has the value 0 for Delphinus delphis, and 1 for Stellena styx. We take x2 to be body weight. Possibilities we may want to consider are:\n\nA single line: y = a + b x2\n\nTwo parallel lines: y = a1 + a2 x1 + b x2\n[For the first group (Stellena styx; x1 = 0) the constant term is a1, while for the second group (Delphinus delphis; x1 = 1) the constant term is a1 + a2.]\n\nTwo separate lines: y = a1 + a2 x1 + b1 x2  + b2 x1 x2\n[For the first group (Delphinus delphis; x1 = 0) the constant term is a1 and the slope is b1. For the second group (Stellena styx; x1 = 1) the constant term is a1 + a2, and the slope is b1 + b2.]\n\nNow compare these three models, both with the AIC statistics, and with AICc which adjusts for small sample size. AIC is one of several alternative ‘information’ statistics.\n\ncet.lm1 <- lm(log(heart) ~ log(weight), data = dolphins)\ncet.lm2 <- lm(log(heart) ~ factor(species) + log(weight), data = dolphins)\ncet.lm3 <- lm(log(heart) ~ factor(species) + factor(species)/log(weight), data = dolphins)\ncbind(AIC(cet.lm1, cet.lm2, cet.lm3), \n      AICc = sapply(list(cet.lm1, cet.lm2, cet.lm3), AICcmodavg::AICc))\n        df AIC AICc\ncet.lm1  3 -21  -19\ncet.lm2  4 -28  -25\ncet.lm3  5 -27  -21\n\nThe smallest value is best, in both cases. Both statistics favour the parallel lines model. The AICc statistic makes a much clearer case against fitting separate lines.\nSelected rows of the model matrix are:\n\nmodel.matrix(cet.lm2)[c(1,2,8,16), ]\n   (Intercept) factor(species)1 log(weight)\n1            1               -1         3.6\n2            1               -1         3.7\n8            1                1         4.0\n16           1                1         4.0\n\nNow try an analysis of variance comparison.\n\ncet.lm3 <- lm(log(heart) ~ factor(species) + log(weight) + \n    factor(species):log(weight), data=dolphins)\nanova(cet.lm1,cet.lm2,cet.lm3) \nAnalysis of Variance Table\n\nModel 1: log(heart) ~ log(weight)\nModel 2: log(heart) ~ factor(species) + log(weight)\nModel 3: log(heart) ~ factor(species) + log(weight) + factor(species):log(weight)\n  Res.Df    RSS Df Sum of Sq    F Pr(>F)\n1     14 0.1717                         \n2     13 0.0959  1    0.0758 9.59 0.0093\n3     12 0.0949  1    0.0010 0.12 0.7346"
  },
  {
    "objectID": "reg.html#aov-models-analysis-of-variance",
    "href": "reg.html#aov-models-analysis-of-variance",
    "title": "3  Multiple Linear Regression",
    "section": "3.7 aov models (Analysis of Variance)",
    "text": "3.7 aov models (Analysis of Variance)\nThe class of models that can be directly fitted as aov models is quite limited. In essence, aov() provides, for data where all combinations of factor levels have the same number of observations, another view of an lm model. One can however specify the error term that is to be used in testing for treatment effects. See Section 3.8 below.\nBy default, R uses the treatment contrasts for factors, i.e. the first level is taken as the baseline or reference level. A useful function is relevel(), using the parameter ref to set the level that is wanted as the reference level.\n\n3.7.1 Plant Growth Example\nFigure 3.9 shows boxplot comparisons of plant weight.\n\n\n\n\nopar <- par(mgp=c(2,0.5,0), mar=c(3.6,3.1,2.1,0.6))\nwith(PlantGrowth, boxplot(split(weight, group), horizontal=T))\npar(opar)\n\n\n\n\nFigure 3.9: Boxplots of plant weight, by group\n\n\n\n\nNow fit a model using aov()\n\nPlantGrowth.aov <- aov(weight~group, data=PlantGrowth)\nsummary(PlantGrowth.aov)\n            Df Sum Sq Mean Sq F value Pr(>F)\ngroup        2   3.77   1.883    4.85  0.016\nResiduals   27  10.49   0.389               \nsummary.lm(PlantGrowth.aov)   # As from `lm` model fit\n\nCall:\naov(formula = weight ~ group, data = PlantGrowth)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.071 -0.418 -0.006  0.263  1.369 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)    5.073      0.114   44.57   <2e-16\ngroup1        -0.041      0.161   -0.25    0.801\ngroup2        -0.412      0.161   -2.56    0.016\n\nResidual standard error: 0.62 on 27 degrees of freedom\nMultiple R-squared:  0.264, Adjusted R-squared:  0.21 \nF-statistic: 4.85 on 2 and 27 DF,  p-value: 0.0159\n\n\n\n3.7.2 Dataset MASS::cabbages (Run code to get output)\nType ?MASS::cabbages to get details of the data.\n\nhelp(cabbages)         # cabbages is from the MASS package\nnames(cabbages)\ncoplot(HeadWt~VitC|Cult+Date,data=cabbages)\n\nExamination of the plot suggests that cultivars differ greatly in the variability in head weight. Variation in the vitamin C levels seems relatively consistent between cultivars.\n\nVitC.aov<-aov(VitC~Cult+Date,data=cabbages)\nsummary(VitC.aov)"
  },
  {
    "objectID": "reg.html#sec-errorAOV",
    "href": "reg.html#sec-errorAOV",
    "title": "3  Multiple Linear Regression",
    "section": "3.8 Shading of Kiwifruit Vines",
    "text": "3.8 Shading of Kiwifruit Vines\nThese data (yields in kilograms) in the data frame DAAG::kiwishade, are from an experiment where there were four treatments - no shading, shading from August to December, shading from December to February, and shading from February to May. Each treatment appeared once in each of the three blocks. The northernmost plots were grouped in one block because they were similarly affected by shading from the sun. For the remaining two blocks shelter effects, in one case from the east and in the other case from the west, were thought more important. Results are given for each of the four vines in each plot. In experimental design parlance, the four vines within a plot constitute subplots.\nThe block:shade mean square (sum of squares divided by degrees of freedom) provides the error term. (If this is not specified, one still gets a correct analysis of variance breakdown. But the \\(F\\)-statistics and \\(p\\)-values will be wrong.)\n\nkiwishade <- DAAG::kiwishade\nkiwishade$shade <- relevel(kiwishade$shade, ref=\"none\")\n## Make sure that the level “none” (no shade) is used as reference\nkiwishade.aov<-aov(yield~block+shade+Error(block:shade),data=kiwishade) \nWarning in aov(yield ~ block + shade + Error(block:shade), data = kiwishade):\nError() model is singular\nsummary(kiwishade.aov)\n\nError: block:shade\n          Df Sum Sq Mean Sq F value Pr(>F)\nblock      2    172      86    4.12 0.0749\nshade      3   1395     465   22.21 0.0012\nResiduals  6    126      21               \n\nError: Within\n          Df Sum Sq Mean Sq F value Pr(>F)\nResiduals 36    439    12.2               \ncoef(kiwishade.aov)\n(Intercept) :\n(Intercept) \n         97 \n\nblock:shade :\nblock1 block2 shade1 shade2 shade3 \n  0.81   1.81   3.67   6.70  -6.61 \n\nWithin :\nnumeric(0)"
  },
  {
    "objectID": "reg.html#exercises",
    "href": "reg.html#exercises",
    "title": "3  Multiple Linear Regression",
    "section": "3.9 Exercises",
    "text": "3.9 Exercises\n\nThe datasets DAAG::elastic1 and DAAG::elastic2 were both obtained using the same apparatus, including the same rubber band, as the data frame DAAG::elasticband. The variable stretch is, in each case, the amount by which an elastic band was stretched over the end of a ruler, and distance the distance that the band traveled when released.\n\n\n\nUsing a different symbol and/or a different colour, plot the data from the two data frames elastic1 and elastic2 on the same graph.\nDo the two sets of results appear consistent?\n\nFor each of the data sets elastic1 and elastic2, determine the regression of stretch on distance. In each case determine\n\n\nfitted values and standard errors of fitted values and\nthe R\\(^2\\) statistic. Compare the two sets of results. What is the key difference?\n\n\nEnter the data frame beams, thus:\n\n\nbeams <- data.frame(\n  strength = c(11.14, 12.74, 13.13, 11.51, 12.38, \n               12.6, 11.13, 11.7, 11.02, 11.41), \n  SpecificGravity = c(0.499, 0.558, 0.604, 0.441, 0.55, \n                      0.528, 0.418, 0.48, 0.406, 0.467), \n  moisture = c(11.1, 8.9, 8.8, 8.9, 8.8, 9.9, 10.7, 10.5, \n               10.5, 10.7))\n\nRegress strength on SpecificGravity and Moisture. Carefully examine the regression diagnostic plot, obtained by supplying the name of the lm object as the first parameter to plot(). What does this indicate?\n\nUsing the data frame cars (in the datasets package), plot distance (i.e. stopping distance) versus speed. Fit a line to this relationship, and plot the line. Then try fitting and plotting a quadratic curve. Does the quadratic curve give a useful improvement to the fit? If you have studied the dynamics of particles, can you find a theory that would tell you how stopping distance might change with speed?\nUsing the data frame hills (in package MASS), regress time on distance and climb. What can you learn from the diagnostic plots that you get when you plot the lm object? Try also regressing log(time) on log(distance) and log(climb). Which of these regression equations would you prefer?\nUse the method of Section 3.5 to determine, formally, whether one needs different regression lines for the two data frames elastic1 and elastic2.\n\n6.In Section 3.5, check the form of the model matrix (i) for fitting two parallel lines and (ii) for fitting two arbitrary lines, using the sum contrasts.\n\nType\n\n\nhosp<-rep(c(”RNC”,”Hunter”,”Mater”),2)\nhosp\nfhosp<-factor(hosp)\nlevels(fhosp)\n\nNow repeat the steps involved in forming the factor fhosp, this time keeping the factor levels in the order \"RNC\", \"Hunter\", \"Mater\". Use contrasts(fhosp) to form and print out the matrix of contrasts. Do this using helmert contrasts, treatment contrasts, and sum contrasts. Using an outcome variable\n\ny <- c(2,5,8,10,3,9)\n\nfit the model lm(y~fhosp), repeating the fit for each of the three different choices of contrasts. Comment on what you get. For which choice(s) of contrasts do the parameter estimates change when you re-order the factor levels?\n\nIn the data set MASS::cement, examine the dependence of y (amount of heat produced) on x1, x2, x3 and x4 (which are proportions of four constituents). Begin by examining the scatterplot matrix. As the explanatory variables are proportions, do they require transformation, perhaps by taking log(x/(100-x))? What alternative strategies one might use to find an effective prediction equation?\nIn the dataset pressure (datasets), examine the dependence of pressure on temperature.\n[Transformation of temperature makes sense only if one first converts to degrees Kelvin. Consider transformation of pressure. A logarithmic transformation is too extreme; the direction of the curvature changes. What family of transformations might one try?\n*Repeat the analysis of the kiwishade data (section 5.8.2), but replacing Error(block:shade) with block:shade. Comment on the output that you get from summary(). To what extent is it potentially misleading? Also do the analysis where the block:shade term is omitted altogether. Comment on that analysis."
  },
  {
    "objectID": "reg.html#references-and-reading",
    "href": "reg.html#references-and-reading",
    "title": "3  Multiple Linear Regression",
    "section": "3.10 References and reading",
    "text": "3.10 References and reading\nCunningham (2021) . Causal inference. Yale University Press.\nFaraway (2014) . Linear Models with R. Taylor & Francis.\nFox and Weisberg (2018) . An R and S-PLUS Companion to Applied Regression. Sage Books.\nMaindonald and Braun (2010) . Data Analysis and Graphics Using R –- An Example-Based Approach. Cambridge University Press.\nMaindonald, Braun, and Andrews (2024, forthcoming) . A Practical Guide to Data Analysis Using R. An Example-Based Approach. Cambridge University Press.\nMaindonald (1992) . Statistical design, analysis and presentation issues.\nMuenchen (2011) . R for SAS and SPSS Users. Springer.\nTu and Gilthorpe (2011) . Statistical thinking in epidemiology. CRC Press.\nVenables and Ripley (2002) . Modern Applied Statistics with S. Springer, NY.\n[This assumes a fair level of statistical sophistication.\nExplanation is careful, but often terse.\nWood (2017) . Generalized Additive Models. An Introduction with R. Chapman and Hall/CRC.\n\n\n\n\nCunningham, Scott. 2021. Causal Inference. Yale University Press. https://mixtape.scunning.com/index.html.\n\n\nDavies, Owen L et al. 1947. “Statistical Methods in Research and Production.”\n\n\nFaraway, Julian J. 2014. Linear Models with R. 2nd ed. Taylor & Francis Ltd.\n\n\nFox, John, and Sanford Weisberg. 2018. An R Companion to Applied Regression. Sage publications. http://socserv.socsci.mcmaster.ca/jfox/Books/Companion.\n\n\nMaindonald, John. 1992. “Statistical Design, Analysis, and Presentation Issues.” New Zealand Journal of Agricultural Research 35 (2): 121–41.\n\n\nMaindonald, John, and John Braun. 2010. Data Analysis and Graphics Using r: An Example-Based Approach. Cambridge University Press.\n\n\nMaindonald, John, W John Braun, and Jeffrey Andrews. 2024, forthcoming. A Practical Guide to Data Analysis Using r. An Example-Based Approach. Cambridge University Press. https://jhmaindonald.github.io/PGRcode.\n\n\nMuenchen, R. A. 2011. R for SAS and SPSS Users. 2nd ed. Springer. http://r4stats.com/books/r4sas-spss/.\n\n\nTu, Y., and M. S. Gilthorpe. 2011. Statistical Thinking in Epidemiology. CRC Press.\n\n\nVenables, W. N., and B. D. Ripley. 2002. Modern Applied Statistics with S. 4th ed. Springer.\n\n\nWood, S. N. 2017. Generalized Additive Models. An Introduction with r. 2nd ed. Chapman; Hall/CRC."
  },
  {
    "objectID": "GLMandA.html#extending-the-linear-model",
    "href": "GLMandA.html#extending-the-linear-model",
    "title": "4  Generalized Linear and Additive Models",
    "section": "4.1 Extending the Linear Model",
    "text": "4.1 Extending the Linear Model\nThe basic model formulation is:\nObserved value = Model Prediction + Statistical Error  \nOften it is assumed that the statistical error values (values of \\(\\epsilon\\) in the discussion below) are independently and identically distributed as Normal. Generalized Linear Models, and the other extensions we describe, allow a variety of non-normal distributions. In the discussion of this section, the focus is on the form of the model prediction, leaving until later sections the discussion of different possibilities for the ‘error’ distribution.\n\n4.1.1 Multiple regression models\nThe various other models we describe are, in essence, generalizations of the multiple regression model, fitted in earlier chapters using lm(). \\[\n    y = b_0 + b_1 x_1 + b_2 x_2 + . . . + b_p x_p + \\epsilon\n\\] ### Generalized Linear Model (e.g. logit model) To simplify the discussion, a single explanatory variable will be assumed. \\[\n    y = g(b_0 + b_1 x_1) + \\epsilon,\\\\\n    \\mbox{so that } \\mbox{E}[y] = g(b_0 + b_1 x_1)\n\\] Here g() is selected from one of a small number of options. For logit models, with \\(\\mu\\) the expected number of ‘successes’. Then: \\[\n    \\mu = n \\dfrac{\\eta}{1+e^\\eta},\n    \\mbox{  where } \\eta = b_0 + b_1 x_1\n\\] Here \\(\\dfrac{\\mu}{n}\\) is an expected proportion.\nWe can turn this model around, and write \\[\n    \\eta = \\log(\\dfrac{\\mu}{n+\\mu}) = f(\\frac{\\pi}{1-\\pi}), \\mbox{ where } \\pi=\\frac{\\mu}{n}\n\\]\nHere f() is the logit link.\nGeneralized Additive Model\nGeneralized Additive Models are a generalization of Generalized Linear Models. For example, g(.) may be the function that undoes the logit transformation, as in a logistic regression model.\nSome terms may be smoothing functions, while others may be the usual linear model terms."
  },
  {
    "objectID": "GLMandA.html#logistic-regression",
    "href": "GLMandA.html#logistic-regression",
    "title": "4  Generalized Linear and Additive Models",
    "section": "4.2 Logistic Regression",
    "text": "4.2 Logistic Regression\nWe will use a logistic regression model as a starting point for discussing Generalized Linear Models. With proportions that range from less than 0.1 to 0.99, it is not reasonable to expect that the expected proportion will be a linear function of x. Some such transformation (link function) as the logit is required. Logit models transform to a log(odds) scale. If p is a probability (e.g. that horse A will win the race), then the corresponding odds are p/(1-p), and \\[\n    log(\\mbox{odds}) = log(\\frac{p}{1-p}) = log(p) - log(1-p)\n\\] The linear model predicts, not p, but \\(log(\\frac{p}{1-p})\\).\nThe logit or log(odds) function turns expected proportions into values that may range from \\(-\\infty\\) to \\(\\infty\\). The values from the linear model may in principle vary across the whole real line. One needs a transformation, such as the logit, such that transformed values may extend outside the range from 0 to 1.\nAmong other link functions that are used with proportions, one of the commonest is the complementary log-log link. See ?make.link for details of those that are available for use with GLM models, as fitted using glm().\n\n4.2.1 Anesthetic Depth Example\nThirty patients were given an anesthetic agent that was maintained at a pre-determined [alveolar] concentration for 15 minutes before making an incision . It was then noted whether the patient moved, i.e. jerked or twisted. The interest is in estimating how the probability of jerking or twisting varies with increasing concentration of the anesthetic agent.\nThe response is best taken as nomove, for reasons that will emerge later. There are a small number of concentrations; so we begin by tabulating proportion that have the nomove outcome against concentration.\n\n\n\nPatients moving (0) and not moving (1), for each of six different alveolar concentrations.\n\n\n\n0.8\n1\n1.2\n1.4\n1.6\n2.5\n\n\n\n\nnomove=0\n6\n4\n2\n2\n0\n0\n\n\nnomove=1\n1\n1\n4\n4\n4\n2\n\n\n\n\n\nWe fit two models, the logit model and the complementary log-log model. We can fit the models either directly to the 0/1 data, or to the proportions in Table 1. To understand the output, you need to know about “deviances”. A deviance has a role very similar to a sum of squares in regression. Thus we have:\n\n\n\nRegression\nLogistic regression\n\n\n\n\ndegrees of freedom\ndegrees of freedom\n\n\nsum of squares\ndeviance\n\n\nmean sum of squares (divide by d.f.)\nmean deviance (divide by d.f.)\n\n\nMinimize residual sum of squares\nMinimize deviance\n\n\n\nIf individuals respond independently, with the same probability, then we have Bernoulli trials. Justification for assuming the same probability will arise from the way in which individuals are sampled. While individuals will certainly be different in their response the notion is that, each time a new individual is taken, they are drawn at random from some larger population.\nHere is the R code:\n\nanesthetic <- DAAG::anesthetic\nanaes.logit <- glm(nomove ~ conc, family = binomial(link = logit), \n                   data = anesthetic)\nsummary(anaes.logit)\n\nCall:\nglm(formula = nomove ~ conc, family = binomial(link = logit), \n    data = anesthetic)\n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)\n(Intercept)   -6.469      2.418  -2.675  0.00748\nconc           5.567      2.044   2.724  0.00645\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 41.455  on 29  degrees of freedom\nResidual deviance: 27.754  on 28  degrees of freedom\nAIC: 31.754\n\nNumber of Fisher Scoring iterations: 5\n\nFigure 4.1 is a graphical summary of the results. The labeling on the \\(y\\)-axis is on the scale of the linear predictor (\\(\\eta\\)),\n\n\n\n\ntermplot(anaes.logit, partial=T, pch=16, ylab=\"Probability\", yaxt='n')\nprobVal <- c(c(0.02, seq(from=.1, to=.9, by=.2), 0.98, 0.998)) \naxis(2, at=log(probVal/(1-probVal)), labels=paste(probVal), las=1)\n\n\n\n\nFigure 4.1: Plot, versus concentration, of log(odds) [= logit(proportion)] of patients not moving. The line is the estimate of the proportion of\nmoves that is based on the fitted logit model.\n\n\n\n\nCode is:\n\ntermplot(anaes.logit, partial=T, pch=16, ylab=\"Probability\", yaxt='n')\nprobVal <- c(c(0.02, seq(from=.1, to=.9, by=.2), 0.98, 0.998)) \naxis(2, at=log(probVal/(1-probVal)), labels=paste(probVal), las=1)\n\nWith such a small sample size it is impossible to do much that is useful to check the adequacy of the model. Try also plot(anaes.logit)."
  },
  {
    "objectID": "GLMandA.html#glm-models-generalized-linear-regression-modelling",
    "href": "GLMandA.html#glm-models-generalized-linear-regression-modelling",
    "title": "4  Generalized Linear and Additive Models",
    "section": "4.3 GLM models (Generalized Linear Regression Modelling)",
    "text": "4.3 GLM models (Generalized Linear Regression Modelling)\nIn the above we had\n\nanaes.logit <- glm(nomove ~ conc, family = binomial(link = logit),\n                   data=anesthetic)\n\nThe family parameter specifies the distribution for the dependent variable. An optional argument allows specification of the link function. Below we give further examples.\n\n4.3.1 Data in the form of counts\nData that are in the form of counts can often be analysed quite effectively assuming the poisson family. The link that is commonly used is log. The log link transforms from positive numbers to numbers in the range \\(-\\infty\\) to \\(\\infty\\).\n\n\n4.3.2 The gaussian family\nIf no family is specified, then the family is taken to be gaussian.\nThe default link is then the identity, effectively giving an lm model. This way of formulating a linear model has the advantage that one is not restricted to what is effectively the identity link.\n\n# Dataset airquality, from datasets package\nair.glm<-glm(Ozone^(1/3) ~ Solar.R + Wind + Temp, data = airquality)\n    # Assumes gaussian family, i.e. normal errors model\nsummary(air.glm)"
  },
  {
    "objectID": "GLMandA.html#models-that-include-spline-terms",
    "href": "GLMandA.html#models-that-include-spline-terms",
    "title": "4  Generalized Linear and Additive Models",
    "section": "4.4 Models that include spline terms",
    "text": "4.4 Models that include spline terms\nIn most circumstances, rather than adding regression spline terms to lm style linear models, it is better to move directly to using the penalized spline approach, with automatic choice of smoothing parameter, that is implemented in the mgcv package. Note however the risk of over-fitting is there is a correlation structure in the data, such as is likely to be present for time sereis.\n\n4.4.1 Dewpoint Data\nThe data set dewpoint has columns mintemp, maxtemp and dewpoint. The dewpoint values are averages, for each combination of mintemp and maxtemp, of monthly data from a number of different times and locations. We fit the model:\ndewpoint = mean of dewpoint + smooth(mintemp) + smooth(maxtemp)\nTaking out the mean is a computational convenience. Also it provides a more helpful form of output. Here are details of the calculations for a generalized additive model:\n\ndewpoint <- DAAG::dewpoint\nlibrary(mgcv)\nLoading required package: nlme\nThis is mgcv 1.9-0. For overview type 'help(\"mgcv-package\")'.\ndewpoint.gam <- gam(dewpt ~ s(mintemp) + s(maxtemp),\n                      data = dewpoint)\nsummary(dewpoint.gam, digits=3)\n\nFamily: gaussian \nLink function: identity \n\nFormula:\ndewpt ~ s(mintemp) + s(maxtemp)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept) 13.12500    0.04215   311.4   <2e-16\n\nApproximate significance of smooth terms:\n             edf Ref.df      F p-value\ns(mintemp) 3.220  4.042 3992.2  <2e-16\ns(maxtemp) 6.252  7.420  794.3  <2e-16\n\nR-sq.(adj) =  0.996   Deviance explained = 99.7%\nGCV = 0.14971  Scale est. = 0.12794   n = 72\n\n\n\n4.4.2 Model Summaries\nType in\n\nmethods(summary)\n\nto get a list of the summary methods that are available. You may want to mix and match, e.g. summary.lm() on an aov object. The output may not be what you might expect. So be careful!"
  },
  {
    "objectID": "GLMandA.html#further-types-of-model",
    "href": "GLMandA.html#further-types-of-model",
    "title": "4  Generalized Linear and Additive Models",
    "section": "4.5 Further types of model",
    "text": "4.5 Further types of model\n\n4.5.1 Survival Analysis\nFor example times at which subjects were either lost to the study or died (‘failed’) may be recorded for individuals in each of several treatment groups. Engineering or business failures can be modelled using this same methodology. The R survival package has state of the art abilities for survival analysis.\n\n\n4.5.2 Nonlinear Models\nThe function nls() (non-linear least squares) can be used to obtain a least squares fit to a non-linear function."
  },
  {
    "objectID": "GLMandA.html#further-elaborations",
    "href": "GLMandA.html#further-elaborations",
    "title": "4  Generalized Linear and Additive Models",
    "section": "4.6 Further Elaborations",
    "text": "4.6 Further Elaborations\nGeneralised Linear Models were developed in the 1970s. They unified a wide range of what had earlier been treated as distinct methods, and have now become a stock-in-trade of statistical analysts. Their practical implementation took advantage of new computational abilities that had been developed for handling linear model calculations.\nPractical data analysis demands further elaborations. An important elaboration is to the incorporation of more than one term in the error structure. The R nlme and lme4 packages implement such extensions, for a wide class of nonlinear models as well for linear models. Each such new development builds on the theoretical and computational tools that have arisen from earlier developments. Powerful new analysis tools will continue to appear for a long time yet. This is fortunate. Most professional users of R will regularly encounter data where the methodology that the data ideally demands is not yet available."
  },
  {
    "objectID": "GLMandA.html#exercises",
    "href": "GLMandA.html#exercises",
    "title": "4  Generalized Linear and Additive Models",
    "section": "4.7 Exercises",
    "text": "4.7 Exercises\n\nFit a Poisson regression model to the data in the data frame DAAG::moths, Allow different intercepts for different habitats. Use log(meters) as a covariate. Why log(meters)?"
  },
  {
    "objectID": "GLMandA.html#references-and-reading",
    "href": "GLMandA.html#references-and-reading",
    "title": "4  Generalized Linear and Additive Models",
    "section": "4.8 References and reading",
    "text": "4.8 References and reading\nFaraway (2016) . Extending the Linear Model with R. Taylor & Francis.\nMaindonald and Braun (2010) . Data Analysis and Graphics Using R –- An Example-Based Approach. Cambridge University Press.\nMaindonald, Braun, and Andrews (2024, forthcoming) . A Practical Guide to Data Analysis Using R. An Example-Based Approach. Cambridge University Press.\nVenables and Ripley (2002) . Modern Applied Statistics with S. Springer, NY.\nWood (2017) . Generalized Additive Models. An Introduction with R. Chapman and Hall/CRC.\n\n\n\n\nFaraway, Julian J. 2016. Extending the Linear Model with r. 2nd ed. Taylor & Francis Inc.\n\n\nMaindonald, John, and John Braun. 2010. Data Analysis and Graphics Using r: An Example-Based Approach. Cambridge University Press.\n\n\nMaindonald, John, W John Braun, and Jeffrey Andrews. 2024, forthcoming. A Practical Guide to Data Analysis Using r. An Example-Based Approach. Cambridge University Press. https://jhmaindonald.github.io/PGRcode.\n\n\nVenables, W. N., and B. D. Ripley. 2002. Modern Applied Statistics with S. 4th ed. Springer.\n\n\nWood, S. N. 2017. Generalized Additive Models. An Introduction with r. 2nd ed. Chapman; Hall/CRC."
  },
  {
    "objectID": "ts.html#smooth-with-automatic-choice-of-smoothing-parameter",
    "href": "ts.html#smooth-with-automatic-choice-of-smoothing-parameter",
    "title": "5  Regular Time Series in R",
    "section": "5.1 Smooth, with automatic choice of smoothing parameter",
    "text": "5.1 Smooth, with automatic choice of smoothing parameter\nFigure 5.3 uses the abilities of the mgcv package, assuming independently and identically distributed data (hence, no serial correlation!) to make an automatic choice of the smoothing parameter. As the curve is conditional on a particular realization of the process that generated it, its usefulness is limited. It does not separate systematic effects from effects due to processes that evolve in time. The curve is not repeatable.\n\n\n\n\n\n\n\n\nFigure 5.3: GAM smoothing term, fitted to the Lake Erie Data. Most of the autocorrelation structure has been removed, leaving residuals that are very nearly independent.\n\n\n\n\nCode is:\n\nsuppressPackageStartupMessages(library(mgcv))\ndf <-  data.frame(height=as.vector(Erie),  year=time(Erie))\nobj <- gam(height ~ s(year), data=df)\nplot(obj, fg=\"gray\", shift=mean(df$height), residuals=TRUE, pch=1,\n     xlab=\"\", ylab=\"Height of lake\")\n\nThe pointwise confidence limits are similarly conditioned, relevant perhaps for interpolation given this particular realization. All that is repeatable, given another realization, is the process that generated the curve, not the curve itself."
  },
  {
    "objectID": "ts.html#fitting-and-use-of-an-autoregressive-model",
    "href": "ts.html#fitting-and-use-of-an-autoregressive-model",
    "title": "5  Regular Time Series in R",
    "section": "5.2 Fitting and use of an autoregressive model",
    "text": "5.2 Fitting and use of an autoregressive model\nSeveral different types of time series models may be used to model the correlation structure, allowing realistic estimates of the lake level a short time ahead, with realistic confidence bounds around those estimates. For the Lake Erie data, an autoregressive correlation structure does a good job of accounting for the pattern of change around a mean that stays constant.\nFigure 5.2 suggested that a correlation between each year and the previous year accounted for the main part of the autocorrelation structure in Figure 2. An AR1 model (autoregressive with a correlation at lag 1 only), which we now fit, formalizes this.\n\nar(Erie, order.max=1)\n\nCall:\nar(x = Erie, order.max = 1)\n\nCoefficients:\n     1  \n0.8512  \n\nOrder selected 1  sigma^2 estimated as  0.02906\n\nThe one coefficient that is now given is the lag 1 correlation, equaling 0.851.\n\ninvestigates how repeated simulations of this process, with a lag 1 correlation of 0.0.85, compare with\n\n\nThis illustrates the point that a GAM smooth will extract, from an autoregressive process with mean 0, a pattern that is not repeatable when the process is re-run.\n\n\n\n\n\n\n\n\n\nFigure 5.4: The plots are from repeated simulations of an AR1 process with a lag 1 correlation of 0.85. Smooth curves, assuming independent errors, have been fitted.\n\n\n\n\nCode is:\n\npar(mfrow=c(2,3))\nfor (i in 1:6){\nysim <- arima.sim(list(ar=0.85), n=200)\ndf <- data.frame(x=1:200, y=ysim)\ndf.gam <- gam(y ~ s(x), data=df)\nplot(df.gam, fg=\"gray\", ylab=paste(\"Sim\", i), residuals=TRUE)\n}\n\nThe curves are different on each occasion. For generalization beyond the particular realization that generated them, they serve no useful purpose.\nOnce an autoregressive model has been fitted, the function forecast() in the forecast package can be used to predict future levels, albeit with very wide confidence bounds. For this, it is necessary to refit the model using the function arima(). An arima model with order (1,0,0) is an autoregressive model with order 1.\n\n\n\n\n\n\n\n\nFigure 5.5: Predictions, 15 years into the future, of lake levels (m). The shaded areas give 80% and 95% confidence bounds.\n\n\n\n\nCode is:\n\nerie.ar <- arima(Erie, order=c(1,0,0))\nsuppressPackageStartupMessages(library(forecast))\nfc <- forecast(erie.ar, h=15)\nplot(fc, main=\"\", fg=\"gray\",  ylab=\"Lake level (m)\")\n  # 15 time points ahead\n\nThis brief excursion into a simple form of time series model is designed only to indicate the limitations of automatic smooths, and to give a sense of the broad style of time series modeling. The list of references at the end of the chapter has details of several books on time series."
  },
  {
    "objectID": "ts.html#regression-with-time-series-errors",
    "href": "ts.html#regression-with-time-series-errors",
    "title": "5  Regular Time Series in R",
    "section": "5.3 Regression with time series errors",
    "text": "5.3 Regression with time series errors\nFigure 5.6 shows the estimated contributions of the two model terms, in a fit of annual rainfall in the Murray-Darling basin of Australia as a sum of smooth functions of Year and SOI.\n\n\n\n\n\n\n\n\nFigure 5.6: Estimated contributions of model terms to mdbRain, in a GAM model that adds smooth terms in Year and Rain. The dashed curves show pointwise 2-SE limits, for the fitted curve.\n\n\n\n\nCode is:\n\npar(mfrow=c(1,2))\nmdbRain.gam <- gam(mdbRain ~ s(Year) + s(SOI), data=DAAG::bomregions)\nplot(mdbRain.gam, residuals=TRUE, se=2, fg=\"gray\",\n     pch=1, select=1, cex=1.35, ylab=\"Partial, Year\")\nmtext(side=3, line=0.75, \"A: Effect of Year\", adj=0)\nplot(mdbRain.gam, residuals=TRUE, se=2, fg=\"gray\",\n     pch=1, select=2, cex=1.35, ylab=\"Partial, SOI\")\nmtext(side=3, line=0.75, \"B: Effect of SOI\", adj=0)\n\nThe left panel indicates a consistent pattern of increase of rainfall with succeeding years, given an adjustment for the effect of SOI. Errors from the fitted model are consistent with the independent errors assumption. The model has then identified a pattern of increase of rainfall with time, given SOI, that does seem real. It is necessary to warn against reliance on extrapolation more than a few time points into the future. While the result is consistent with expected effects from global warming, those effects are known to play out very differently in different parts of the globe.\nSequential correlation structures are often effective, with data collected over time, for use in modeling departure from iid errors. Where there is such structure structure in the data, the methodology will if possible use a smooth curve to account for it.\nThe residuals can be checked to determine whether the fitted curve has removed most of the correlation structure in the data. Figure 5.7 shows the autocorrelation function of the residuals, followed by autocorrelation functions for several series of independent random normal numbers. Apart from the weakly attested correlation at a lag of 12 years, which is a commonplace of weather data, the pattern of sequential correlation is not much different from what can be expected in a sequence of independent random normal numbers.\n\n\n\n\n\n\n\n\nFigure 5.7: The top left panel shows the autocorrelations of the residuals from the model mdbRain.gam. The five remaining panels are the equivalent plots for sequences of independent random normal numbers.\n\n\n\n\nCode is:\n\nmdbRain.gam <- gam(mdbRain ~ s(Year) + s(SOI), data=DAAG::bomregions)\nn <-  dim(DAAG::bomregions)[1]\nacf(resid(mdbRain.gam), ylab=\"MDB series\")\nfor(i in 1:5)acf(rnorm(n), ylab=paste(\"Sim\",i),\n                 fg=\"gray\", col=\"gray40\")"
  },
  {
    "objectID": "ts.html#box-jenkins-arima-time-series-modeling",
    "href": "ts.html#box-jenkins-arima-time-series-modeling",
    "title": "5  Regular Time Series in R",
    "section": "5.4 \\(^*\\)Box-Jenkins ARIMA Time Series Modeling",
    "text": "5.4 \\(^*\\)Box-Jenkins ARIMA Time Series Modeling\nModels that are closely analogous to ARIMA models had been used earlier in control theory. ARIMA models are feedback systems! From the perspective of the Box-Jenkins ARIMA (Autoregressive Integrated Moving Average) approach to time series models, autoregressive models are a special case. Many standard types of time series can be modeled very satisfactorily as ARIMA processes.\nThe simulations in Figure 5.7 show a pattern of variation that seems not too different from that in the actual series. Modeling of the process as an ARMA or ARIMA process (i.e., allow for a moving average term) may do even better. Use the auto.arima() function in the forecast package to fit an ARIMA process:"
  },
  {
    "objectID": "ts.html#count-data-with-poisson-errors",
    "href": "ts.html#count-data-with-poisson-errors",
    "title": "5  Regular Time Series in R",
    "section": "5.5 Count Data with Poisson Errors",
    "text": "5.5 Count Data with Poisson Errors\nData are for aircraft accidents, from the website https://www.planecrashinfo.com/. The 1920 file has accidents starting from 1908. The full data are in the dataset gamclass::airAccs. Data are a time series.\nSerious accidents are sufficiently uncommon that joint occurrences, or cases where one event changes the probability of the next, are likely to be uncommon.\nSuch issues as there are with sequential correlation can be ameliorated by working with weekly, rather than daily, counts.\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.8: Estimated number of events (aircraft crashes) per week, versus time. The yearly tick marks are for January 1 of the stated year.\n\n\n\n\nFigure 5.8 shows a fitted smooth curve, with pointwise confidence bounds, from a GAM smoothing model that was fitted to the weekly counts.\nThe function gamclass::eventCounts() was used to create weekly counts of accidents from January 1, 2006:\n\n## Code\nairAccs <- gamclass::airAccs\nfromDate <- as.Date(\"2006-01-01\")\ndfWeek06 <- gamclass::eventCounts(airAccs, dateCol=\"Date\",\n                  from=fromDate, by=\"1 week\", prefix=\"num\")\ndfWeek06$day <- julian(dfWeek06$Date, origin=fromDate)\n\nCode for Figure 5.8 is then.\n\n## Code\nsuppressPackageStartupMessages(library(mgcv))\nyear <- seq(from=fromDate, to=max(dfWeek06$Date), by=\"1 year\")\nat6 <- julian(seq(from=fromDate, to=max(dfWeek06$Date), \n                  by=\"6 months\"), origin=fromDate)\natyear <- julian(year, origin=fromDate)\ndfWeek06.gam <- gam(num~s(day, k=200), data=dfWeek06, family=quasipoisson)\navWk <- mean(predict(dfWeek06.gam))\nplot(dfWeek06.gam, xaxt=\"n\", shift=avWk, trans=exp, rug=FALSE,\n     xlab=\"\", ylab=\"Estimated rate per week\", fg=\"gray\")\naxis(1, at=atyear, labels=format(year, \"%Y\"), lwd=0, lwd.ticks=1)\nabline(h=0.5+(1:4)*0.5, v=at6, col=\"gray\", lty=3, lwd=0.5)\n# mtext(side=3, line=0.75, \"A: Events per week, vs date\", adj=0)\n\nThe argument k to the function s() that sets up the smooth controls the temporal resolution. A large k allows, if the data seem to justify it, for fine resolution. A penalty is applied that discriminates against curves that are overly ‘wiggly’.\nNot all count data is suitable for modeling assuming a Poisson type rare event distribution. For example, the dataset DAAG::hurricNamed has details, for the years 1950-2012, of US deaths from Atlantic hurricanes. For any given hurricane, deaths are not at all independent rare events."
  },
  {
    "objectID": "ts.html#exercises",
    "href": "ts.html#exercises",
    "title": "5  Regular Time Series in R",
    "section": "5.6 Exercises",
    "text": "5.6 Exercises\n\nUse the function acf() to plot the autocorrelation function of lake levels in successive years in the data set LakeHuron (in datasets). Do the plots both with type=\"correlation\" and with type=\"partial\"."
  },
  {
    "objectID": "ts.html#references-and-reading",
    "href": "ts.html#references-and-reading",
    "title": "5  Regular Time Series in R",
    "section": "5.7 References and reading",
    "text": "5.7 References and reading\nSee the vignette that accompanies the forecast package.\nHyndman and Athanasopoulos (2021) . Forecasting: principles and practice. OTexts.\n\n\n\n\nHyndman, Rob J, and George Athanasopoulos. 2021. Forecasting: Principles and Practice. 3rd ed. OTexts. https://otexts.com/fpp2/."
  },
  {
    "objectID": "trees.html#decision-tree-models-tree-based-models",
    "href": "trees.html#decision-tree-models-tree-based-models",
    "title": "6  Tree-based models",
    "section": "6.1 Decision Tree models (Tree-based models)",
    "text": "6.1 Decision Tree models (Tree-based models)\nTree-based classification, as for example implemented in the rpart (recursive partitioning) package, can be used for multivariate supervised classification (discrimination) or for tree-based regression. Tree-based methods are in general more suited to binary regression and classification than to regression with an ordinal or continuous dependent variable.\nSuch “Classification and Regression Trees” (CART), may be suitable for regression and classification problems when there are extensive data. An advantage of such methods is that they automatically handle non-linearity and interactions. Output includes a “decision tree” that is immediately useful for prediction. The MASS:fgl glass fragment data will be used for an example. If high levels of accuracy are important and obtaining a single decision tree is not a priority, the ‘’random forests’’ approach that will be described below is usually to be preferred.\nFigure 6.1 shows an initial tree, before pruning.\n\n\n\n\n\n\n\n\nFigure 6.1: Initial tree for predicting type for the forensic glass data.\n\n\n\n\nCode is:\n\nlibrary(rpart)\nfgl <- MASS::fgl\nset.seed(31)    ## Use to reproduce output shown.\n# Use fgl: Forensic glass fragment data; from MASS package\nglass.tree <- rpart(type ~ RI+Na+Mg+Al+Si+K+Ca+Ba+Fe, data=fgl)\nplot(glass.tree);  text(glass.tree)\n\nNow check how cross-validated predictive accuracy changes with the number of splits. The column xerror is the one to check. Error values must be multiplied by the root node error to get an absolute error value.\n\nprintcp(glass.tree, digits=3)\n\nClassification tree:\nrpart(formula = type ~ RI + Na + Mg + Al + Si + K + Ca + Ba + \n    Fe, data = fgl)\n\nVariables actually used in tree construction:\n[1] Al Ba Ca Fe Mg Na RI\n\nRoot node error: 138/214 = 0.645\n\nn= 214 \n\n      CP nsplit rel error xerror   xstd\n1 0.2065      0     1.000  1.043 0.0497\n2 0.0725      2     0.587  0.601 0.0517\n3 0.0580      3     0.514  0.572 0.0512\n4 0.0362      4     0.457  0.493 0.0494\n5 0.0326      5     0.420  0.507 0.0497\n6 0.0109      7     0.355  0.464 0.0485\n7 0.0100      9     0.333  0.471 0.0487\n\nThe optimum number of splits, as indicated by this output (this may change from one run to the next) is 7. The function prune() should be used to prune the splits back to this number. For this purpose, set cp to a value between that for nsplit=7 and that for nsplit=5.\n\nprintcp(prune(glass.tree, cp = 0.011))\n\nClassification tree:\nrpart(formula = type ~ RI + Na + Mg + Al + Si + K + Ca + Ba + \n    Fe, data = fgl)\n\nVariables actually used in tree construction:\n[1] Al Ba Ca Mg Na RI\n\nRoot node error: 138/214 = 0.64486\n\nn= 214 \n\n        CP nsplit rel error  xerror     xstd\n1 0.206522      0   1.00000 1.04348 0.049733\n2 0.072464      2   0.58696 0.60145 0.051652\n3 0.057971      3   0.51449 0.57246 0.051156\n4 0.036232      4   0.45652 0.49275 0.049357\n5 0.032609      5   0.42029 0.50725 0.049733\n6 0.011000      7   0.35507 0.46377 0.048534\n\nTo use single tree methods effectively, one needs to be familiar with approaches for such pruning, involving the use of cross-validation to obtain error estimates. Methods for reduction of tree complexity that are based on significance tests at each individual node (i.e. branching point) typically choose trees that over-predict.\n\n6.1.1 The random forests approach\nThe random forests approach, implemented in the randomForests package, involves generating trees for repeated bootstrap samples (by default, 500) from the data, with data that is excluded from the bootstrap sample used to make, in each case, a prediction for that tree. The final prediction is based on a vote over all trees This is simplest for classification trees. The stopping criterion for individual trees is, unlike the case for single tree methods, not of great importance. Predictive accuracy is typically much better than for single tree methods."
  },
  {
    "objectID": "trees.html#exercises",
    "href": "trees.html#exercises",
    "title": "6  Tree-based models",
    "section": "6.2 Exercises",
    "text": "6.2 Exercises\n\nThe MASS::Aids2 dataset contains de-identified data on the survival status of patients diagnosed with AIDS before July 1 1991. Use tree-based classification (rpart()) to identify major influences on survival.\nCompare the effectiveness of rpart() with that of randomForest(), for discriminating between plagiotropic and orthotropic species in the data set DAAG::leafshape."
  },
  {
    "objectID": "trees.html#references-and-reading",
    "href": "trees.html#references-and-reading",
    "title": "6  Tree-based models",
    "section": "6.3 References and reading",
    "text": "6.3 References and reading\nSee the vignettes that accompany the rpart package.\nLiaw and Wiener (2002) . Classification and Regression by randomForest. R News.\nMaindonald and Braun (2010) . Data Analysis and Graphics Using R –- An Example-Based Approach. Cambridge University Press.\nMaindonald, Braun, and Andrews (2024, forthcoming) . A Practical Guide to Data Analysis Using R. An Example-Based Approach. Cambridge University Press.\n\n\n\n\nLiaw, A., and M. Wiener. 2002. “Classification and Regression by randomForest.” R News 2 (3): 18–22.\n\n\nMaindonald, John, and John Braun. 2010. Data Analysis and Graphics Using r: An Example-Based Approach. Cambridge University Press.\n\n\nMaindonald, John, W John Braun, and Jeffrey Andrews. 2024, forthcoming. A Practical Guide to Data Analysis Using r. An Example-Based Approach. Cambridge University Press. https://jhmaindonald.github.io/PGRcode."
  },
  {
    "objectID": "mva.html#multivariate-eda-and-principal-components-analysis",
    "href": "mva.html#multivariate-eda-and-principal-components-analysis",
    "title": "7  Multivariate Methods",
    "section": "7.1 Multivariate EDA, and Principal Components Analysis",
    "text": "7.1 Multivariate EDA, and Principal Components Analysis\nPrincipal components analysis is often a useful exploratory tool for multivariate data. The idea is to replace the initial set of variables by a small number of “principal components” that together may explain most of the variation in the data. The first principal component is the component (linear combination of the initial variables) that explains the greatest part of the variation. The second principal component is the component that, among linear combinations of the variables that are uncorrelated with the first principal component, explains the greatest part of the remaining variation, and so on.\nThe measure of variation used is the sum of the variances of variables, perhaps after scaling so that they each have variance one. An analysis that works with the unscaled variables, and hence with the variance-covariance matrix, gives a greater weight to variables that have a large variance. The common alternative –- scaling variables so that they each have variance equal to one – is equivalent to working with the correlation matrix.\nWith biological measurement data, it is usually desirable to begin by taking logarithms. The standard deviations then measure the logarithm of relative change. Because all variables measure much the same quantity (i.e. relative variability), and because the standard deviations are typically fairly comparable, scaling to give equal variances is unnecessary.\nThe data set DAAG::possum has nine morphometric measurements on each of 102 mountain brushtail possums, trapped at seven sites from southern Victoria to central Queensland . It is good practice to begin by examining relevant scatterplot matrices. This may draw attention to gross errors in the data. A plot in which the sites and/or the sexes are identified will draw attention to any very strong structure in the data. For example one site may be quite different from the others, for some or all of the variables. Taking logarithms of these data does not make much difference to the appearance of the plots. This is because the ratio of largest to smallest value is relatively small, never more than 1.6, for all variables.\nScatterplot matrix possibilities include:\n\npairs(possum[,6:14], col=palette()[as.integer(possum$sex)])\npairs(possum[,6:14], col=palette()[as.integer(possum$site)])\nhere<-!is.na(possum$footlgth)    # We need to exclude missing values\nprint(sum(!here))                # Check how many values are missing\n\nWe now look (Figure 7.1) at the view of the data that comes from plotting the second principal component against the first:\n\n\n\n\n\n\n\n\nFigure 7.1: Second principal component versus first principal component, by population and by sex, for the possum data.\n\n\n\n\nCode is:\n\npossum <- DAAG::possum\nhere <- !is.na(possum$footlgth)    # We need to exclude missing values\npossum.prc <- princomp(log(possum[here,6:14]))  # Principal components\n# Print scores on second pc versus scores on first pc,\n# by populations and sex, identified by site\nlibrary(lattice)\nxyplot(possum.prc$scores[,2] ~  possum.prc$scores[,1]|\n       possum$Pop[here]+possum$sex[here], groups=possum$site,\n      par.settings=simpleTheme(pch=0:6), auto.key=list(columns=7))"
  },
  {
    "objectID": "mva.html#cluster-analysis",
    "href": "mva.html#cluster-analysis",
    "title": "7  Multivariate Methods",
    "section": "7.2 Cluster Analysis",
    "text": "7.2 Cluster Analysis\nCluster analysis is a form of unsupervised classification, ‘unsupervised’ because the clusters are not known in advance. There are two common types of algorithms – algorithms based on hierachical agglomeration, and algorithms based on iterative relocation.\nIn hierarchical agglomeration each observation starts as a separate group.\nGroups that are ‘close’ to one another are then successively merged. The output yields a hierarchical clustering tree that shows the relationships between observations and between the clusters into which they are successively merged. A judgment is then needed on the point at which further merging is unwarranted.\nIn iterative relocation, the algorithm starts with an initial classification, typically from a prior use of a hierarchical agglomeration algorithm, that it then tries to improve.\nThe mva package has the function dist() that calculates distances, the function hclust() that does hierarchical agglomerative clustering with with one of several choices of methods, and the function kmeans() (k-means clustering) that implements iterative relocation."
  },
  {
    "objectID": "mva.html#discriminant-analysis",
    "href": "mva.html#discriminant-analysis",
    "title": "7  Multivariate Methods",
    "section": "7.3 Discriminant Analysis",
    "text": "7.3 Discriminant Analysis\nWe start with data that are classified into several groups, and want a rule that will allow us to predict the group to which a new data value will belong. This is a form of supervised classification – thr groups are known in advance. For example, we may wish to predict, based on prognostic measurements and outcome information for previous patients, which future patients are likely to remain free of disease symptoms for twelve months or more following treatment. Or we may wish to check, based on morphometric measurements, the extent of differences between animals from different sites. Can they be clearly distinguished, to an extent that they appear different species?\nCalculations now follow for the possum data frame, using the lda() function from the MASS package. Our interest is in whether it is possible, on the basis of morphometric measurements, to distinguish animals from different sites. A cruder distinction is between populations, i.e. sites in Victoria (an Australian state) as opposed to sites in other states (New South Wales or Queensland). Because there is little effect on the distribution of variable values, it appears unnecessary to take logarithms. This will be discussed further below.\n\nlibrary(MASS)                 # Only if not already attached.\nhere<- !is.na(possum$footlgth)\npossum.lda <- lda(site ~ hdlngth+skullw+totlngth+\n   totlngth+footlgth+earconch+eye+chest+belly,data=possum, subset=here)\noptions(digits=4)\npossum.lda$svd   # Examine the singular values\n[1] 13.8327  3.9224  2.7004  1.4731  1.0494  0.3572\n\nFigure 7.2 shows the scatterplot matrix of first three canonical variates.\n\n\n\n\n\n\n\n\nFigure 7.2: Scatterplot matrix of first three canonical variates.\n\n\n\n\nCode is:\n\nplot(possum.lda, dimen=3)    \n# Scatterplot matrix for scores on 1st 3 canonical variates\n\nThe singular values are the ratio of between to within group sums of squares, for the canonical variates in turn. Clearly canonical variates after the third will have little if any discriminatory power. One can use predict.lda() to get (among other information) scores on the first few canonical variates.\nNote that there may be interpretative advantages in taking logarithms of biological measurement data. The standard against which patterns of measurement are commonly compared is that of allometric growth, which implies a linear relationship between the logarithms of the measurements. Differences between different sites are then indicative of different patterns of allometric growth. The reader may wish to repeat the above analysis, but working with the logarithms of measurements."
  },
  {
    "objectID": "mva.html#exercises",
    "href": "mva.html#exercises",
    "title": "7  Multivariate Methods",
    "section": "7.4 Exercises",
    "text": "7.4 Exercises\n\nUsing the data set MASS::painters, apply principal components analysis to the scores for Composition, Drawing, Colour, and Expression. Examine the loadings on the first three principal components. Plot a scatterplot matrix of the first three principal components, using different colors or symbols to identify the different schools.\nUsing the columns of continuous or ordinal data in the MASS::Cars93 dataset, determine scores on the first and second principal components. Investigate the comparison between (i) USA and non-USA cars, and\n\n\nthe six different types (Type) of car. Now create a new data set in which binary factors become columns of 0/1 data, and include these in the principal components analysis.\n\n\nRepeat the calculations of exercises 1 and 2, but this time using the function MASS::lda() to derive canonical discriminant scores, as in section 6.3.\nInvestigate discrimination between plagiotropic and orthotropic species in the data set DAAG::leafshape."
  },
  {
    "objectID": "mva.html#references-and-reading",
    "href": "mva.html#references-and-reading",
    "title": "7  Multivariate Methods",
    "section": "7.5 References and reading",
    "text": "7.5 References and reading\nMaindonald and Braun (2010) . Data Analysis and Graphics Using R –- An Example-Based Approach. Cambridge University Press.\nMaindonald, Braun, and Andrews (2024, forthcoming) . A Practical Guide to Data Analysis Using R. An Example-Based Approach. Cambridge University Press.\nVenables and Ripley (2002) . Modern Applied Statistics with S. Springer, NY.\n\n\n\n\nMaindonald, John, and John Braun. 2010. Data Analysis and Graphics Using r: An Example-Based Approach. Cambridge University Press.\n\n\nMaindonald, John, W John Braun, and Jeffrey Andrews. 2024, forthcoming. A Practical Guide to Data Analysis Using r. An Example-Based Approach. Cambridge University Press. https://jhmaindonald.github.io/PGRcode.\n\n\nVenables, W. N., and B. D. Ripley. 2002. Modern Applied Statistics with S. 4th ed. Springer."
  },
  {
    "objectID": "mlm.html#multi-level-models-examples",
    "href": "mlm.html#multi-level-models-examples",
    "title": "8  Multi-Level Models and Repeated Measures Models",
    "section": "8.1 Multi-level models – examples",
    "text": "8.1 Multi-level models – examples\n\nThe Kiwifruit Shading Data, Again\nRefer back to Section 3.8 for details of the data. The fixed effects are block and treatment (shade). The random effects are block (though making block a random effect is optional, for purposes of comparing treatments), plot within block, and units within each block/plot combination. Here is the analysis:\n\nlibrary(nlme)\nkiwishade <- DAAG::kiwishade\nkiwishade$plot <- factor(paste(kiwishade$block, kiwishade$shade, \n    sep=\".\"))\nkiwishade.lme <- lme(yield~shade,random=~1|block/plot, data=kiwishade)\nsummary(kiwishade.lme)\nLinear mixed-effects model fit by REML\n  Data: kiwishade \n       AIC      BIC    logLik\n  265.9663 278.4556 -125.9831\n\nRandom effects:\n Formula: ~1 | block\n        (Intercept)\nStdDev:    2.019373\n\n Formula: ~1 | plot %in% block\n        (Intercept) Residual\nStdDev:    1.478623 3.490381\n\nFixed effects:  yield ~ shade \n                 Value Std.Error DF  t-value p-value\n(Intercept)  100.20250  1.761617 36 56.88098  0.0000\nshadeAug2Dec   3.03083  1.867621  6  1.62283  0.1558\nshadeDec2Feb -10.28167  1.867621  6 -5.50522  0.0015\nshadeFeb2May  -7.42833  1.867621  6 -3.97743  0.0073\n Correlation: \n             (Intr) shdA2D shdD2F\nshadeAug2Dec -0.53               \nshadeDec2Feb -0.53   0.50        \nshadeFeb2May -0.53   0.50   0.50 \n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-2.41538976 -0.59814252 -0.06899575  0.78046182  1.58909233 \n\nNumber of Observations: 48\nNumber of Groups: \n          block plot %in% block \n              3              12 \nanova(kiwishade.lme)\n            numDF denDF  F-value p-value\n(Intercept)     1    36 5190.560  <.0001\nshade           3     6   22.211  0.0012\nintervals(kiwishade.lme)\nApproximate 95% confidence intervals\n\n Fixed effects:\n                  lower       est.      upper\n(Intercept)   96.629775 100.202500 103.775225\nshadeAug2Dec  -1.539072   3.030833   7.600738\nshadeDec2Feb -14.851572 -10.281667  -5.711762\nshadeFeb2May -11.998238  -7.428333  -2.858428\n\n Random Effects:\n  Level: block \n                    lower     est.    upper\nsd((Intercept)) 0.5475859 2.019373 7.446993\n  Level: plot \n                    lower     est.    upper\nsd((Intercept)) 0.3700762 1.478623 5.907772\n\n Within-group standard error:\n   lower     est.    upper \n2.770652 3.490381 4.397072 \n\nWe are interested in the three sd estimates. By squaring the standard deviations and converting them to variances we get the information in the following table:\n\n\n\n\n\n\n\n\n\nVariance component\nNotes\n\n\n\n\nblock\n2.0192\\(^2\\) = 4.076\nThree blocks\n\n\nplot\n1.4792\\(^2\\) = 2.186\n4 plots per block\n\n\nresidual (within group)\n3.4902\\(^2\\)=12.180\n4 vines (subplots) per plot\n\n\n\nThe above gives the information for an analysis of variance table. We have:\n\n\n\n\n\n\n\n\n\n\nVariance component\nMean square for anova table\nd.f.\n\n\n\n\nblock\n4.076\n12.180 + 4 \\(\\times\\) 2.186 + 16 \\(\\times\\) 4.076 = 86.14\n2 (3-1)\n\n\nplot\n2.186\n12.180 + 4 \\(\\times\\) 2.186 = 20.92\n6 (3-1)\\(\\times\\)(4-1)\n\n\nresidual (within gp)\n12.180\n12.18\n3 \\(\\times\\) 4 \\(\\times\\) (4-1)\n\n\n\nNow see where these same pieces of information appeared in the analysis of variance table of Section 3.8:\n\nkiwishade.aov<-aov(yield~block+shade+Error(block:shade),data=kiwishade)\nWarning in aov(yield ~ block + shade + Error(block:shade), data = kiwishade):\nError() model is singular\nsummary(kiwishade.aov)\n\nError: block:shade\n          Df Sum Sq Mean Sq F value  Pr(>F)\nblock      2  172.3    86.2   4.118 0.07488\nshade      3 1394.5   464.8  22.211 0.00119\nResiduals  6  125.6    20.9                \n\nError: Within\n          Df Sum Sq Mean Sq F value Pr(>F)\nResiduals 36  438.6   12.18               \n\n\n\nThe Tinting of Car Windows\nIn Section 2.8 we encountered data from an experiment that aimed to model the effects of the tinting of car windows on visual performance . The authors are mainly interested in effects on side window vision, and hence in visual recognition tasks that would be performed when looking through side windows. Data are in the data frame tinting. In this data frame, csoa (critical stimulus onset asynchrony, i.e. the time in milliseconds required to recognise an alphanumeric target), it (inspection time, i.e. the time required for a simple discrimination task) and age are variables, while tint (3 levels) and target (2 levels) are ordered factors. The variable sex is coded 1 for males and 2 for females, while the variable agegp is coded 1 for young people (all in their early 20s) and 2 for older participants (all in the early 70s).\nWe have two levels of variation – within individuals (who were each tested on each combination of tint and target), and between individuals. So we need to specify id (identifying the individual) as a random effect. Plots such as we examined in Section 2.8 make it clear that, to get variances that are approximately homogeneous, we need to work with log(csoa) and log(it). Here we examine the analysis for log(it). We start with a model that is likely to be more complex than we need (it has all possible interactions):\n\ntinting <- DAAG::tinting\nitstar.lme<-lme(log(it)~tint*target*agegp*sex,\n  random=~1|id, data=tinting,method=\"ML\")\n\nA reasonable guess is that first order interactions may be all we need, i.e. \n\nit2.lme<-lme(log(it)~(tint+target+agegp+sex)^2,\n  random=~1|id, data=tinting,method=\"ML\")\n\nFinally, there is the very simple model, allowing only for main effects:\n\nit1.lme<-lme(log(it)~(tint+target+agegp+sex),\n  random=~1|id, data=tinting,method=\"ML\")\n\nNote that all these models have been fitted by maximum likelihood. This allows the equivalent of an analysis of variance comparison.\nHere is what we get:\n\nanova(itstar.lme,it2.lme,it1.lme)\n           Model df       AIC      BIC    logLik   Test  L.Ratio p-value\nitstar.lme     1 26  8.146187 91.45036 21.926906                        \nit2.lme        2 17 -3.742883 50.72523 18.871441 1 vs 2  6.11093  0.7288\nit1.lme        3  8  1.138171 26.77022  7.430915 2 vs 3 22.88105  0.0065\n\nThe model that limits attention to first order interactions appears adequate. As a preliminary to examining the first order interactions individually, we re-fit the model used for it2.lme, now with method=\"REML\".\n\nit2.reml<-update(it2.lme,method=\"REML\")\n\nWe now examine the estimated effects:\n\noptions(digits=3)\nsummary(it2.reml)$tTable\n                          Value Std.Error  DF t-value  p-value\n(Intercept)             3.61907    0.1301 145  27.817 5.30e-60\ntint.L                  0.16095    0.0442 145   3.638 3.81e-04\ntint.Q                  0.02096    0.0452 145   0.464 6.44e-01\ntargethicon            -0.11807    0.0423 145  -2.789 5.99e-03\nagegpolder              0.47121    0.2329  22   2.023 5.54e-02\nsexm                    0.08213    0.2329  22   0.353 7.28e-01\ntint.L:targethicon     -0.09193    0.0461 145  -1.996 4.78e-02\ntint.Q:targethicon     -0.00722    0.0482 145  -0.150 8.81e-01\ntint.L:agegpolder       0.13075    0.0492 145   2.658 8.74e-03\ntint.Q:agegpolder       0.06972    0.0520 145   1.341 1.82e-01\ntint.L:sexm            -0.09794    0.0492 145  -1.991 4.83e-02\ntint.Q:sexm             0.00542    0.0520 145   0.104 9.17e-01\ntargethicon:agegpolder -0.13887    0.0584 145  -2.376 1.88e-02\ntargethicon:sexm        0.07785    0.0584 145   1.332 1.85e-01\nagegpolder:sexm         0.33164    0.3261  22   1.017 3.20e-01\n\nBecause tint is an ordered factor, polynomial contrasts are used.\n\n\nThe Michelson Speed of Light Data\nThe MASS::michelson dataframe has columns Speed, Run, and Expt, for five experiments of 20 runs each. A plot of the data seems consistent with sequential dependence within runs, possibly with random variation between runs.\n\n\n\n\n\n\n\n\nFigure 8.1: Plots show speed of light estimates against run number, for each of five experiments.\n\n\n\n\nCode is:\n\nmichelson <- MASS::michelson\nlattice::xyplot(Speed~Run|factor(Expt), layout=c(5,1),\n                data=michelson, type=c('p','r'),\n                scales=list(x=list(at=seq(from=1,to=19, by=3))))\n\nWe try a model that allows the estimates to vary linearly with Run (from 1 to 20), with the slope varying randomly between experiments. We assume an autoregressive dependence structure of order 1. We allow the variance to change from one experiment to another.\nTo test whether this level of model complexity is justified statistically, one needs to compare models with and without these effects, setting method=\"ML\" in each case, and compare the likelihoods.\n\nmichelson <- MASS::michelson\nlibrary(nlme)\nmichelson$Run <- as.numeric(michelson$Run)   # Ensure Run is a variable\nmich.lme1 <- lme(fixed = Speed ~ Run, data = michelson, \n      random =  ~ Run| Expt, correlation = corAR1(form =  ~ 1 | Expt), \n      weights = varIdent(form =  ~ 1 | Expt), method='ML')\nmich.lme0 <- lme(fixed = Speed ~ Run, data = michelson, \n      random =  ~ 1| Expt, correlation = corAR1(form =  ~ 1 | Expt), \n      weights = varIdent(form =  ~ 1 | Expt), method='ML')\nanova(mich.lme0, mich.lme1)\n          Model df  AIC  BIC logLik   Test  L.Ratio p-value\nmich.lme0     1  9 1121 1144   -551                        \nmich.lme1     2 11 1125 1153   -551 1 vs 2 2.63e-08       1\n\nThe simpler model is preferred. Can it be simplified further?"
  },
  {
    "objectID": "mlm.html#references-and-reading",
    "href": "mlm.html#references-and-reading",
    "title": "8  Multi-Level Models and Repeated Measures Models",
    "section": "8.2 References and reading",
    "text": "8.2 References and reading\nSee the vignettes that accompany the lme4 package.\nMaindonald and Braun (2010) . Data Analysis and Graphics Using R –- An Example-Based Approach. Cambridge University Press.\nMaindonald, Braun, and Andrews (2024, forthcoming) . A Practical Guide to Data Analysis Using R. An Example-Based Approach. Cambridge University Press.\nPinheiro and Bates (2000) . Mixed effects models in S and S-PLUS. Springer.\n\n\n\n\nMaindonald, John, and John Braun. 2010. Data Analysis and Graphics Using r: An Example-Based Approach. Cambridge University Press.\n\n\nMaindonald, John, W John Braun, and Jeffrey Andrews. 2024, forthcoming. A Practical Guide to Data Analysis Using r. An Example-Based Approach. Cambridge University Press. https://jhmaindonald.github.io/PGRcode.\n\n\nPinheiro, J. C., and D. M. Bates. 2000. Mixed Effects Models in S and S-PLUS. Springer."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Braun, W. John, and Duncan J. Murdoch. 2021. A First Course in\nStatistical Programming with R. 3rd ed. Cambridge\nUniversity Press.\n\n\nChang, Winston. 2013. R Graphics Cookbook. 1st ed. O’Reilly.\n\n\nCunningham, Scott. 2021. Causal Inference. Yale University\nPress. https://mixtape.scunning.com/index.html.\n\n\nDalgaard, Peter. 2008. Introductory Statistics with\nR. 2nd ed. Springer.\n\n\nDavies, Owen L et al. 1947. “Statistical Methods in Research and\nProduction.”\n\n\nFaraway, Julian J. 2014. Linear Models with R. 2nd\ned. Taylor & Francis Ltd.\n\n\n———. 2016. Extending the Linear Model with r. 2nd ed. Taylor\n& Francis Inc.\n\n\nFox, John, and Sanford Weisberg. 2018. An R Companion\nto Applied Regression. Sage publications. http://socserv.socsci.mcmaster.ca/jfox/Books/Companion.\n\n\nHyndman, Rob J, and George Athanasopoulos. 2021. Forecasting:\nPrinciples and Practice. 3rd ed. OTexts. https://otexts.com/fpp2/.\n\n\nLiaw, A., and M. Wiener. 2002. “Classification and Regression by\nrandomForest.” R News 2 (3): 18–22.\n\n\nMaindonald, John. 1992. “Statistical Design, Analysis, and\nPresentation Issues.” New Zealand Journal of Agricultural\nResearch 35 (2): 121–41.\n\n\nMaindonald, John H, and W. John Braun. 2022. DAAG: Data Analysis and\nGraphics Data and Functions. https://gitlab.com/daagur.\n\n\nMaindonald, John, and John Braun. 2010. Data Analysis and Graphics\nUsing r: An Example-Based Approach. Cambridge University Press.\n\n\nMaindonald, John, W John Braun, and Jeffrey Andrews. 2024, forthcoming.\nA Practical Guide to Data Analysis Using r. An Example-Based\nApproach. Cambridge University Press. https://jhmaindonald.github.io/PGRcode.\n\n\nMatloff, Norman. 2011. The Art of r Programming. No Starch Inc.\n\n\nMuenchen, R. A. 2011. R for SAS and SPSS Users. 2nd ed.\nSpringer. http://r4stats.com/books/r4sas-spss/.\n\n\nMurrell, Paul. 2009. Introduction to Data Technologies. 1st ed.\nCRC Press.\n\n\n———. 2011. R Graphics. 2nd ed. Chapman; Hall/CRC. http://www.e-reading.org.ua/bookreader.php/137370/Murrell_-_R_Graphics.pdf.\n\n\nPinheiro, J. C., and D. M. Bates. 2000. Mixed Effects Models in\nS and S-PLUS. Springer.\n\n\nRipley, Brian. 2023. MASS: Support Functions and Datasets for\nVenables and Ripley’s MASS. http://www.stats.ox.ac.uk/pub/MASS4/.\n\n\nSarkar, Deepayan. 2023. Lattice: Trellis Graphics for r. https://lattice.r-forge.r-project.org/.\n\n\nTu, Y., and M. S. Gilthorpe. 2011. Statistical Thinking in\nEpidemiology. CRC Press.\n\n\nVenables, W. N., and B. D. Ripley. 2002. Modern Applied Statistics\nwith S. 4th ed. Springer.\n\n\nWickham, H. 2016. R for Data Science. O’Reilly.\n\n\nWood, S. N. 2017. Generalized Additive Models. An Introduction with\nr. 2nd ed. Chapman; Hall/CRC."
  }
]